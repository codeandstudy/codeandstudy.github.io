<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script></script><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="爬虫,搜索引擎,Scrapy,Python,"><link rel="alternate" href="/atom.xml" title="mtianyan's blog" type="application/atom+xml"><meta name="description" content="伯乐在线爬取所有文章   使用scrapy对于伯乐在线的文章内容评论数，收藏数等进行爬取。包含从环境配置，软件安装，项目初始化，xpath，图片下载，数据保存到本地文件以及mysql（同步异步）等一系列内容。"><meta name="keywords" content="爬虫,搜索引擎,Scrapy,Python"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy分布式爬虫打造搜索引擎- (二)伯乐在线爬取所有文章"><meta property="og:url" content="http://blog.mtianyan.cn/post/1cc4531e.html"><meta property="og:site_name" content="mtianyan&#39;s blog"><meta property="og:description" content="伯乐在线爬取所有文章   使用scrapy对于伯乐在线的文章内容评论数，收藏数等进行爬取。包含从环境配置，软件安装，项目初始化，xpath，图片下载，数据保存到本地文件以及mysql（同步异步）等一系列内容。"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-d0f10707f0b49b08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-b46e91f577c5bf6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-e3b33dc61bc8ab3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-909b623357abad96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-d34dc1f60a28f9f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:updated_time" content="2018-02-02T12:40:13.956Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Scrapy分布式爬虫打造搜索引擎- (二)伯乐在线爬取所有文章"><meta name="twitter:description" content="伯乐在线爬取所有文章   使用scrapy对于伯乐在线的文章内容评论数，收藏数等进行爬取。包含从环境配置，软件安装，项目初始化，xpath，图片下载，数据保存到本地文件以及mysql（同步异步）等一系列内容。"><meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/1779926-d0f10707f0b49b08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://blog.mtianyan.cn/post/1cc4531e.html"><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement("script"),n=t.getElementsByTagName("script")[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,0,("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"e28768be"}),daovoice("update")</script><title>Scrapy分布式爬虫打造搜索引擎- (二)伯乐在线爬取所有文章 | mtianyan's blog</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?415372bd35fec36f7558dd96b48ec03f";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div> <a href="https://github.com/mtianyan" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">mtianyan's blog</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">天涯明月笙的博客小站(Github托管)</p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br> 公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://blog.mtianyan.cn/post/1cc4531e.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="mtianyan"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="mtianyan's blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Scrapy分布式爬虫打造搜索引擎- (二)伯乐在线爬取所有文章</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-27T14:58:57+08:00">2017-06-27</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Scrapy分布式爬虫打造搜索引擎/" itemprop="url" rel="index"><span itemprop="name">Scrapy分布式爬虫打造搜索引擎</span></a></span></span> <span id="/post/1cc4531e.html" class="leancloud_visitors" data-flag-title="Scrapy分布式爬虫打造搜索引擎- (二)伯乐在线爬取所有文章"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">热度&#58;</span><span class="leancloud-visitors-count"></span> <span>℃</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">4,778</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">22</span></div></div></header><div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center"><p>伯乐在线爬取所有文章</p></blockquote><div class="note primary"><p> 使用scrapy对于伯乐在线的文章内容评论数，收藏数等进行爬取。<br>包含从环境配置，软件安装，项目初始化，xpath，图片下载，数据保存到本地文件以及mysql（同步异步）等一系列内容。</p></div><a id="more"></a><h2 id="二、伯乐在线爬取所有文章"><a href="#二、伯乐在线爬取所有文章" class="headerlink" title="二、伯乐在线爬取所有文章"></a>二、伯乐在线爬取所有文章</h2><h3 id="1-初始化文件目录"><a href="#1-初始化文件目录" class="headerlink" title="1. 初始化文件目录"></a>1. 初始化文件目录</h3><p>基础环境</p><blockquote><ol><li>python 3.5.1</li><li>JetBrains PyCharm 2016.3.2</li><li>mysql+navicat</li></ol></blockquote><p>为了便于日后的部署：我们开发使用了虚拟环境。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenv</span><br><span class="line">pip install virtualenvwrapper-win</span><br><span class="line">安装虚拟环境管理</span><br><span class="line">mkvirtualenv articlespider3</span><br><span class="line">创建虚拟环境</span><br><span class="line">workon articlespider3</span><br><span class="line">直接进入虚拟环境</span><br><span class="line">deactivate</span><br><span class="line">退出激活状态</span><br><span class="line">workon</span><br><span class="line">知道有哪些虚拟环境</span><br></pre></td></tr></table></figure><h4 id="scrapy项目初始化介绍"><a href="#scrapy项目初始化介绍" class="headerlink" title="scrapy项目初始化介绍"></a>scrapy项目初始化介绍</h4><blockquote><p>自行官网下载py35对应得whl文件进行pip离线安装<br>Scrapy 1.3.3</p></blockquote><p><strong>命令行创建scrapy项目</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd desktop</span><br><span class="line"></span><br><span class="line">scrapy startproject ArticleSpider</span><br></pre></td></tr></table></figure><p></p><p><strong>scrapy目录结构</strong></p><p>scrapy借鉴了django的项目思想</p><blockquote><ul><li><code>scrapy.cfg</code>：配置文件。</li><li><code>setings.py</code>：设置</li></ul></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPIDER_MODULES = [&apos;ArticleSpider.spiders&apos;] #存放spider的路径</span><br><span class="line">NEWSPIDER_MODULE = &apos;ArticleSpider.spiders&apos;</span><br></pre></td></tr></table></figure><p>pipelines.py:</p><blockquote><p>做跟数据存储相关的东西</p></blockquote><p>middilewares.py:</p><blockquote><p>自己定义的middlewares 定义方法，处理响应的IO操作</p></blockquote><p><strong>init</strong>.py:</p><blockquote><p>项目的初始化文件。</p></blockquote><p>items.py：</p><blockquote><p>定义我们所要爬取的信息的相关属性。Item对象是种类似于表单，用来保存获取到的数据</p></blockquote><p><strong>创建我们的spider</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ArticleSpider</span><br><span class="line">scrapy genspider jobbole blog.jobbole.com</span><br></pre></td></tr></table></figure><p></p><p>可以看到直接为我们创建好的空项目里已经有了模板代码。如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    <span class="comment"># start_urls是一个带爬的列表，</span></span><br><span class="line">    <span class="comment">#spider会为我们把请求下载网页做到，直接到parse阶段</span></span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>scray在命令行启动某一个Spyder的命令:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl jobbole</span><br></pre></td></tr></table></figure><p></p><p><strong>在windows报出错误</strong></p><p><code>ImportError: No module named &#39;win32api&#39;</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pypiwin32#解决</span><br></pre></td></tr></table></figure><p><strong>创建我们的调试工具类*</strong></p><p>在项目根目录里创建main.py<br>作为调试工具文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></span><br><span class="line">__author__ = <span class="string">'mtianyan'</span></span><br><span class="line">__date__ = <span class="string">'2017/3/28 12:06'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#将系统当前目录设置为项目根目录</span></span><br><span class="line"><span class="comment">#os.path.abspath(__file__)为当前文件所在绝对路径</span></span><br><span class="line"><span class="comment">#os.path.dirname为文件所在目录</span></span><br><span class="line"><span class="comment">#H:\CodePath\spider\ArticleSpider\main.py</span></span><br><span class="line"><span class="comment">#H:\CodePath\spider\ArticleSpider</span></span><br><span class="line">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line"><span class="comment">#执行命令，相当于在控制台cmd输入改名了</span></span><br><span class="line">execute([<span class="string">"scrapy"</span>, <span class="string">"crawl"</span> , <span class="string">"jobbole"</span>])</span><br></pre></td></tr></table></figure><p></p><p><strong>settings.py的设置不遵守reboots协议</strong></p><p><code>ROBOTSTXT_OBEY = False</code></p><p>在jobble.py打上断点:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure><p></p><p>可以看到他返回的htmlresponse对象:<br>对象内部：</p><blockquote><ul><li>body:网页内容</li><li>_DEFAULT_ENCODING= ‘ascii’</li><li>encoding= ‘utf-8’</li></ul></blockquote><p>可以看出scrapy已经为我们做到了将网页下载下来。而且编码也进行了转换.</p><h3 id="2-提取伯乐在线内容"><a href="#2-提取伯乐在线内容" class="headerlink" title="2. 提取伯乐在线内容"></a>2. 提取伯乐在线内容</h3><h4 id="xpath的使用"><a href="#xpath的使用" class="headerlink" title="xpath的使用"></a>xpath的使用</h4><p>xpath让你可以不懂前端html，不看html的详细结构，只需要会右键查看就能获取网页上任何内容。速度远超beautifulsoup。<br>目录:</p><pre><code>1. xpath简介
2. xpath术语与语法
3. xpath抓取误区：javasrcipt生成html与html源文件的区别
4. xpath抓取实例
</code></pre><p>为什么要使用xpath？</p><ul><li>xpath使用路径表达式在xml和html中进行导航</li><li>xpath包含有一个标准函数库</li><li>xpath是一个w3c的标准</li><li>xpath速度要远远超beautifulsoup。</li></ul><p><strong>xpath节点关系</strong></p><ol><li>父节点<code>*上一层节点*</code></li><li>子节点</li><li>兄弟节点<code>*同胞节点*</code></li><li>先辈节点<code>*父节点，爷爷节点*</code></li><li>后代节点<code>*儿子，孙子*</code><br>xpath语法:</li></ol><table><thead><tr><th>表达式</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td>article</td><td style="text-align:center">选取所有article元素的所有子节点</td></tr><tr><td>/article</td><td style="text-align:center">选取根元素article</td></tr><tr><td>article/a</td><td style="text-align:center">选取所有属于article的子元素的a元素</td></tr><tr><td>//div</td><td style="text-align:center">选取所有div元素（不管出现在文档里的任何地方）</td></tr><tr><td>article//div</td><td style="text-align:center">选取所有属于article元素的后代的div元素，不管它出现在article之下的任何位置</td></tr><tr><td>//@class</td><td style="text-align:center">选取所有名为class的属性</td></tr></tbody></table><p><strong>xpath语法-谓语:</strong></p><table><thead><tr><th>表达式</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td>/article/div[1</td><td style="text-align:center">选取属于article子元素的第一个div元素</td></tr><tr><td>/article/div[last()]</td><td style="text-align:center">选取属于article子元素的最后一个div元素</td></tr><tr><td>/article/div[last()-1]</td><td style="text-align:center">选取属于article子元素的倒数第二个div元素</td></tr><tr><td>//div[@color]</td><td style="text-align:center">选取所有拥有color属性的div元素</td></tr><tr><td>//div[@color=’red’]</td><td style="text-align:center">选取所有color属性值为red的div元素</td></tr></tbody></table><p><strong>xpath语法:</strong></p><table><thead><tr><th>表达式</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td>/div/*</td><td style="text-align:center">选取属于div元素的所有子节点</td></tr><tr><td>//*</td><td style="text-align:center">选取所有元素</td></tr><tr><td>//div[@*]</td><td style="text-align:center">选取所有带属性的div 元素</td></tr><tr><td>//div/a 丨//div/p</td><td style="text-align:center">选取所有div元素的a和p元素</td></tr><tr><td>//span丨//ul</td><td style="text-align:center">选取文档中的span和ul元素</td></tr><tr><td>article/div/p丨//span</td><td style="text-align:center">选取所有属于article元素的div元素的p元素以及文档中所有的 span元素</td></tr></tbody></table><p><strong>xpath抓取误区</strong></p><p><a href="https://addons.mozilla.org/en-us/firefox/addon/firebug/?src=dp-dl-dependencies" target="_blank" rel="noopener">firebugs插件</a></p><p>取某一个网页上元素的xpath地址</p><blockquote><p>如:<a href="http://blog.jobbole.com/110287/" target="_blank" rel="noopener">http://blog.jobbole.com/110287/</a></p></blockquote><p>在标题处右键使用firebugs查看元素。<br>然后在<code>&lt;h1&gt;2016 腾讯软件开发面试题（部分）&lt;/h1&gt;</code>右键查看xpath</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        re_selector = response.xpath(<span class="string">"/html/body/div[3]/div[3]/div[1]/div[1]/h1"</span>)</span><br><span class="line">        <span class="comment"># print(re_selector)</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>调试debug可以看到<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re_selector =(selectorlist)[]</span><br></pre></td></tr></table></figure><p></p><p>可以看到返回的是一个空列表，<br>列表是为了如果我们当前的xpath路径下还有层级目录时可以进行选取<br>空说明没取到值：</p><p><strong>我们可以来chorme里观察一下</strong></p><blockquote><p>chorme取到的值<br><code>//*[@id=&quot;post-110287&quot;]/div[1]/h1</code></p></blockquote><p>chormexpath代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        re_selector = response.xpath(<span class="string">'//*[@id="post-110287"]/div[1]/h1'</span>)</span><br><span class="line">        <span class="comment"># print(re_selector)</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>可以看出此时可以取到值</p><blockquote><p>分析页面，可以发现页面内有一部html是通过JavaScript ajax交互来生成的，因此在f12检查元素时的页面结构里有，而xpath不对<br>xpath是基于html源代码文件结构来找的</p></blockquote><p>xpath可以有多种多样的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">re_selector = response.xpath(<span class="string">"/html/body/div[1]/div[3]/div[1]/div[1]/h1/text()"</span>)</span><br><span class="line">re2_selector = response.xpath(<span class="string">'//*[@id="post-110287"]/div[1]/h1/text()'</span>)</span><br><span class="line">re3_selector = response.xpath(<span class="string">'//div[@class="entry-header“]/h1/text()'</span>)</span><br></pre></td></tr></table></figure><p><del>推荐使用id型。因为页面id唯一。</del></p><p>推荐使用class型，因为后期循环爬取可扩展通用性强。</p><p>通过了解了这些此时我们已经可以抓取到页面的标题，此时可以使用xpath利器照猫画虎抓取任何内容。只需要点击右键查看xpath。</p><p><strong>开启控制台调试</strong></p><p><code>scrapy shell http://blog.jobbole.com/110287/</code></p><p><strong>完整的xpath提取伯乐在线字段代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment">#提取文章的具体字段</span></span><br><span class="line">        title = response.xpath(<span class="string">'//div[@class="entry-header"]/h1/text()'</span>).extract_first(<span class="string">""</span>)</span><br><span class="line">        create_date = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/text()"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>,<span class="string">""</span>).strip()</span><br><span class="line">        praise_nums = response.xpath(<span class="string">"//span[contains(@class, 'vote-post-up')]/h10/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        fav_nums = response.xpath(<span class="string">"//span[contains(@class, 'bookmark-btn')]/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        match_re = re.match(<span class="string">".*?(\d+).*"</span>, fav_nums)</span><br><span class="line">        <span class="keyword">if</span> match_re:</span><br><span class="line">            fav_nums = match_re.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        comment_nums = response.xpath(<span class="string">"//a[@href='#article-comment']/span/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        match_re = re.match(<span class="string">".*?(\d+).*"</span>, comment_nums)</span><br><span class="line">        <span class="keyword">if</span> match_re:</span><br><span class="line">            comment_nums = match_re.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        content = response.xpath(<span class="string">"//div[@class='entry']"</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        tag_list = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/a/text()"</span>).extract()</span><br><span class="line">        tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</span><br><span class="line">        tags = <span class="string">","</span>.join(tag_list)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="css选择器的使用："><a href="#css选择器的使用：" class="headerlink" title="css选择器的使用："></a>css选择器的使用：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过css选择器提取字段</span></span><br><span class="line">        <span class="comment"># front_image_url = response.meta.get("front_image_url", "")  #文章封面图</span></span><br><span class="line">        title = response.css(<span class="string">".entry-header h1::text"</span>).extract_first()</span><br><span class="line">        create_date = response.css(<span class="string">"p.entry-meta-hide-on-mobile::text"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>,<span class="string">""</span>).strip()</span><br><span class="line">        praise_nums = response.css(<span class="string">".vote-post-up h10::text"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        match_re = re.match(<span class="string">".*?(\d+).*"</span>, fav_nums)</span><br><span class="line">        <span class="keyword">if</span> match_re:</span><br><span class="line">            fav_nums = int(match_re.group(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fav_nums = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        match_re = re.match(<span class="string">".*?(\d+).*"</span>, comment_nums)</span><br><span class="line">        <span class="keyword">if</span> match_re:</span><br><span class="line">            comment_nums = int(match_re.group(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            comment_nums = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        content = response.css(<span class="string">"div.entry"</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text"</span>).extract()</span><br><span class="line">        tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</span><br><span class="line">        tags = <span class="string">","</span>.join(tag_list)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="3-爬取所有文章"><a href="#3-爬取所有文章" class="headerlink" title="3. 爬取所有文章"></a>3. 爬取所有文章</h3><p><strong>yield关键字</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用request下载详情页面，下载完成后回调方法parse_detail()提取文章内容中的字段</span></span><br><span class="line"><span class="keyword">yield</span> Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail)</span><br></pre></td></tr></table></figure><p><strong>scrapy.http import Request下载网页</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request</span><br><span class="line">Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail)</span><br></pre></td></tr></table></figure><p><strong>parse拼接网址应对herf内有可能网址不全</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line">url=parse.urljoin(response.url,post_url)</span><br><span class="line">parse.urljoin(<span class="string">"http://blog.jobbole.com/all-posts/"</span>,<span class="string">"http://blog.jobbole.com/111535/"</span>)</span><br><span class="line"><span class="comment">#结果为http://blog.jobbole.com/111535/</span></span><br></pre></td></tr></table></figure><p><strong>class层级关系</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">next_url = response.css(<span class="string">".next.page-numbers::attr(href)"</span>).extract_first(<span class="string">""</span>)</span><br><span class="line"><span class="comment">#如果.next .pagenumber 是指两个class为层级关系。而不加空格为同一个标签</span></span><br></pre></td></tr></table></figure><p><strong>twist异步机制</strong></p><p>Scrapy使用了Twisted作为框架，Twisted有些特殊的地方是它是事件驱动的，并且比较适合异步的代码。在任何情况下，都不要写阻塞的代码。阻塞的代码包括:</p><ul><li>访问文件、数据库或者Web</li><li>产生新的进程并需要处理新进程的输出，如运行shell命令</li><li>执行系统层次操作的代码，如等待系统队列</li></ul><p><strong>实现全部文章字段下载的代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">            1. 获取文章列表页中的文章url并交给scrapy下载后并进行解析</span></span><br><span class="line"><span class="string">            2. 获取下一页的url并交给scrapy进行下载， 下载完成后交给parse</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">    <span class="comment"># 解析列表页中的所有文章url并交给scrapy下载后并进行解析</span></span><br><span class="line">    post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</span><br><span class="line">    <span class="keyword">for</span> post_url <span class="keyword">in</span> post_urls:</span><br><span class="line">        <span class="comment">#request下载完成之后，回调parse_detail进行文章详情页的解析</span></span><br><span class="line">        <span class="comment"># Request(url=post_url,callback=self.parse_detail)</span></span><br><span class="line">        print(response.url)</span><br><span class="line">        print(post_url)</span><br><span class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail)</span><br><span class="line">        <span class="comment">#遇到href没有域名的解决方案</span></span><br><span class="line">        <span class="comment">#response.url + post_url</span></span><br><span class="line">        print(post_url)</span><br><span class="line">    <span class="comment"># 提取下一页并交给scrapy进行下载</span></span><br><span class="line">    next_url = response.css(<span class="string">".next.page-numbers::attr(href)"</span>).extract_first(<span class="string">""</span>)</span><br><span class="line">    <span class="keyword">if</span> next_url:</span><br><span class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), callback=self.parse)</span><br></pre></td></tr></table></figure><p><strong>全部文章的逻辑流程图</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-d0f10707f0b49b08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="所有文章流程图"></p><h3 id="4-scrapy的items整合字段"><a href="#4-scrapy的items整合字段" class="headerlink" title="4. scrapy的items整合字段"></a>4. scrapy的items整合字段</h3><p>数据爬取的任务就是从非结构的数据中提取出结构性的数据。<br>items 可以让我们自定义自己的字段（类似于字典，但比字典的功能更齐全）</p><p><strong>在当前页，需要提取多个url</strong></p><p>原始写法,extract之后则生成list列表，无法进行二次筛选：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</span><br></pre></td></tr></table></figure><p>改进写法:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">post_nodes = response.css(<span class="string">"#archive .floated-thumb .post-thumb a"</span>)</span><br><span class="line">        <span class="keyword">for</span> post_node <span class="keyword">in</span> post_nodes:</span><br><span class="line">            <span class="comment">#获取封面图的url</span></span><br><span class="line">            image_url = post_node.css(<span class="string">"img::attr(src)"</span>).extract_first(<span class="string">""</span>)</span><br><span class="line">            post_url = post_node.css(<span class="string">"::attr(href)"</span>).extract_first(<span class="string">""</span>)</span><br></pre></td></tr></table></figure><p><strong>在下载网页的时候把获取到的封面图的url传给parse_detail的response</strong><br>在下载网页时将这个封面url获取到，并通过meta将他发送出去。在callback的回调函数中接收该值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> Request(url=parse.urljoin(response.url,post_url),meta=&#123;<span class="string">"front_image_url"</span>:image_url&#125;,callback=self.parse_detail)</span><br><span class="line"></span><br><span class="line">front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)</span><br></pre></td></tr></table></figure><p><strong>urljoin的好处</strong><br>如果你没有域名，我就从response里取出来，如果你有域名则我对你起不了作用了</p><p><strong>编写我们自定义的item并在jobboled.py中填充。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_object_id = scrapy.Field()</span><br><span class="line">    front_image_url = scrapy.Field()</span><br><span class="line">    front_image_path = scrapy.Field()</span><br><span class="line">    praise_nums = scrapy.Field()</span><br><span class="line">    comment_nums = scrapy.Field()</span><br><span class="line">    fav_nums = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field()</span><br></pre></td></tr></table></figure><p>import之后实例化，实例化之后填充：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="keyword">from</span> ArticleSpider.items <span class="keyword">import</span> JobBoleArticleItem</span><br><span class="line"><span class="number">2.</span> article_item = JobBoleArticleItem()</span><br><span class="line"><span class="number">3.</span> article_item[<span class="string">"title"</span>] = title</span><br><span class="line">        article_item[<span class="string">"url"</span>] = response.url</span><br><span class="line">        article_item[<span class="string">"create_date"</span>] = create_date</span><br><span class="line">        article_item[<span class="string">"front_image_url"</span>] = [front_image_url]</span><br><span class="line">        article_item[<span class="string">"praise_nums"</span>] = praise_nums</span><br><span class="line">        article_item[<span class="string">"comment_nums"</span>] = comment_nums</span><br><span class="line">        article_item[<span class="string">"fav_nums"</span>] = fav_nums</span><br><span class="line">        article_item[<span class="string">"tags"</span>] = tags</span><br><span class="line">        article_item[<span class="string">"content"</span>] = content</span><br></pre></td></tr></table></figure><p><strong>yield article_item将这个item传送到pipelines中</strong><br>pipelines可以接收到传送过来的item<br>将setting.py中的pipeline配置取消注释<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Configure item pipelines</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &apos;ArticleSpider.pipelines.ArticlespiderPipeline&apos;: 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>当我们的item被传输到pipeline我们可以将其进行存储到数据库等工作</p><p><strong>setting设置下载图片pipeline</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;</span><br><span class="line">&apos;scrapy.pipelines.images.ImagesPipeline&apos;: 1,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>H:\CodePath\pyEnvs\articlespider3\Lib\site-packages\scrapy\pipelines<br>里面有三个scrapy默认提供的pipeline<br>提供了文件，图片，媒体。</p><p>ITEM_PIPELINES是一个数据管道的登记表，每一项具体的数字代表它的优先级，数字越小，越早进入。</p><p><strong>setting设置下载图片的地址</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># IMAGES_MIN_HEIGHT = 100</span><br><span class="line"># IMAGES_MIN_WIDTH = 100</span><br></pre></td></tr></table></figure><p></p><p>设置下载图片的最小高度，宽度。</p><p>新建文件夹images在</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_URLS_FIELD = <span class="string">"front_image_url"</span></span><br><span class="line">project_dir = os.path.abspath(os.path.dirname(__file__))</span><br><span class="line">IMAGES_STORE = os.path.join(project_dir, <span class="string">'images'</span>)</span><br></pre></td></tr></table></figure><p><strong>安装PIL</strong><br><code>pip install pillow</code></p><p><strong>定制自己的pipeline使其下载图片后能保存下它的本地路径</strong><br>get_media_requests()接收一个迭代器对象下载图片<br>item_completed获取到图片的下载地址</p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-b46e91f577c5bf6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="自定义图片pipeline的调试信息"></p><p>继承并重写item_completed()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="comment">#重写该方法可从result中获取到图片的实际下载地址</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</span><br><span class="line">            image_file_path = value[<span class="string">"path"</span>]</span><br><span class="line">        item[<span class="string">"front_image_path"</span>] = image_file_path</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>setting中设置使用我们自定义的pipeline，而不是系统自带的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'ArticleSpider.pipelines.ArticlespiderPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="comment"># 'scrapy.pipelines.images.ImagesPipeline': 1,</span></span><br><span class="line">    <span class="string">'ArticleSpider.pipelines.ArticleImagePipeline'</span>:<span class="number">1</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/1779926-e3b33dc61bc8ab3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="保存下来的本地地址"></p><p><strong>图片url的md5处理</strong><br>新建package utils</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></span><br><span class="line">    m = hashlib.md5()</span><br><span class="line">    m.update(url)</span><br><span class="line">    <span class="keyword">return</span> m.hexdigest()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    print(get_md5(<span class="string">"http://jobbole.com"</span>.encode(<span class="string">"utf-8"</span>)))</span><br></pre></td></tr></table></figure><p>不确定用户传入的是不是:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment">#str就是unicode了</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(url, str):</span><br><span class="line">        url = url.encode(<span class="string">"utf-8"</span>)</span><br><span class="line">    m = hashlib.md5()</span><br><span class="line">    m.update(url)</span><br><span class="line">    <span class="keyword">return</span> m.hexdigest()</span><br></pre></td></tr></table></figure><p>在jobbole.py中将url的md5保存下来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ArticleSpider.utils.common <span class="keyword">import</span> get_md5</span><br><span class="line">article_item[<span class="string">"url_object_id"</span>] = get_md5(response.url)</span><br></pre></td></tr></table></figure><h3 id="5-数据保存到本地文件以及mysql中"><a href="#5-数据保存到本地文件以及mysql中" class="headerlink" title="5. 数据保存到本地文件以及mysql中"></a>5. 数据保存到本地文件以及mysql中</h3><h4 id="保存到本地json文件"><a href="#保存到本地json文件" class="headerlink" title="保存到本地json文件"></a>保存到本地json文件</h4><p><strong>import codecs打开文件避免一些编码问题，自定义JsonWithEncodingPipeline实现json本地保存</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWithEncodingPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#自定义json文件的导出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = codecs.open(<span class="string">'article.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment">#将item转换为dict，然后生成json对象，false避免中文出错</span></span><br><span class="line">        lines = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(lines)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="comment">#当spider关闭的时候</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure><p>setting.py注册pipeline</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'ArticleSpider.pipelines.JsonWithEncodingPipeline'</span>: <span class="number">2</span>,</span><br><span class="line">   <span class="comment"># 'scrapy.pipelines.images.ImagesPipeline': 1,</span></span><br><span class="line">    <span class="string">'ArticleSpider.pipelines.ArticleImagePipeline'</span>:<span class="number">1</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>scrapy exporters JsonItemExporter导出</strong></p><p>scrapy自带的导出：</p><pre><code>- &apos;CsvItemExporter&apos;, 
- &apos;XmlItemExporter&apos;,
- &apos;JsonItemExporter&apos;
</code></pre><p><code>from scrapy.exporters import JsonItemExporter</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonExporterPipleline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#调用scrapy提供的json export导出json文件</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'articleexport.json'</span>, <span class="string">'wb'</span>)</span><br><span class="line">        self.exporter = JsonItemExporter(self.file, encoding=<span class="string">"utf-8"</span>, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>设置setting.py注册该pipeline</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'ArticleSpider.pipelines.JsonExporterPipleline '</span>: <span class="number">2</span></span><br></pre></td></tr></table></figure><h4 id="保存到数据库-mysql"><a href="#保存到数据库-mysql" class="headerlink" title="保存到数据库(mysql)"></a>保存到数据库(mysql)</h4><p>数据库设计数据表，表的内容字段是和item一致的。数据库与item的关系。类似于django中model与form的关系。<br><strong>日期的转换，将字符串转换为datetime</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"> <span class="keyword">try</span>:</span><br><span class="line">            create_date = datetime.datetime.strptime(create_date, <span class="string">"%Y/%m/%d"</span>).date()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            create_date = datetime.datetime.now().date()</span><br></pre></td></tr></table></figure><p><strong>数据库表设计</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-909b623357abad96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="jobbole数据表设计"></p><ul><li>三个num字段均设置不能为空，然后默认0.</li><li>content设置为longtext</li><li>主键设置为url_object_id</li></ul><p><strong>数据库驱动安装</strong><br><code>pip install mysqlclient</code></p><p>Linux报错解决方案:<br>ubuntu:<br><code>sudo apt-get install libmysqlclient-dev</code><br>centos:<br><code>sudo yum install python-devel mysql-devel</code></p><p><strong>保存到数据库pipeline(同步）编写</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#采用同步的机制写入mysql</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.conn = MySQLdb.connect(<span class="string">'127.0.0.1'</span>, <span class="string">'root'</span>, <span class="string">'mima'</span>, <span class="string">'article_spider'</span>, charset=<span class="string">"utf8"</span>, use_unicode=<span class="keyword">True</span>)</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">            insert into jobbole_article(title, url, create_date, fav_nums)</span></span><br><span class="line"><span class="string">            VALUES (%s, %s, %s, %s)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</span><br><span class="line">        self.conn.commit()</span><br></pre></td></tr></table></figure><p><strong>保存到数据库的(异步Twisted)编写</strong><br>因为我们的爬取速度可能大于数据库存储的速度。异步操作。<br>设置可配置参数<br>seeting.py设置<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MYSQL_HOST = &quot;127.0.0.1&quot;</span><br><span class="line">MYSQL_DBNAME = &quot;article_spider&quot;</span><br><span class="line">MYSQL_USER = &quot;root&quot;</span><br><span class="line">MYSQL_PASSWORD = &quot;123456&quot;</span><br></pre></td></tr></table></figure><p></p><p>代码中获取到设置的可配置参数<br>twisted异步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> MySQLdb.cursors</span><br><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"></span><br><span class="line"><span class="comment">#连接池ConnectionPool</span></span><br><span class="line"><span class="comment">#    def __init__(self, dbapiName, *connargs, **connkw):</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dbpool)</span>:</span></span><br><span class="line">        self.dbpool = dbpool</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">        dbparms = dict(</span><br><span class="line">            host = settings[<span class="string">"MYSQL_HOST"</span>],</span><br><span class="line">            db = settings[<span class="string">"MYSQL_DBNAME"</span>],</span><br><span class="line">            user = settings[<span class="string">"MYSQL_USER"</span>],</span><br><span class="line">            passwd = settings[<span class="string">"MYSQL_PASSWORD"</span>],</span><br><span class="line">            charset=<span class="string">'utf8'</span>,</span><br><span class="line">            cursorclass=MySQLdb.cursors.DictCursor,</span><br><span class="line">            use_unicode=<span class="keyword">True</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#**dbparms--&gt;("MySQLdb",host=settings['MYSQL_HOST']</span></span><br><span class="line">        dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparms)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(dbpool)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment">#使用twisted将mysql插入变成异步执行</span></span><br><span class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</span><br><span class="line">        query.addErrback(self.handle_error, item, spider) <span class="comment">#处理异常</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure, item, spider)</span>:</span></span><br><span class="line">        <span class="comment">#处理异步插入的异常</span></span><br><span class="line">        <span class="keyword">print</span> (failure)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></span><br><span class="line">        <span class="comment">#执行具体的插入</span></span><br><span class="line">        <span class="comment">#根据不同的item 构建不同的sql语句并插入到mysql中</span></span><br><span class="line">        insert_sql, params = item.get_insert_sql()</span><br><span class="line">        cursor.execute(insert_sql, params)</span><br><span class="line"></span><br><span class="line">``` </span><br><span class="line">可选django.items</span><br><span class="line"></span><br><span class="line">https://github.com/scrapy-plugins/scrapy-djangoitem</span><br><span class="line"></span><br><span class="line">可以让我们保存的item直接变成django的models.</span><br><span class="line"></span><br><span class="line"><span class="comment">#### scrapy的itemloader来维护提取代码</span></span><br><span class="line"></span><br><span class="line">itemloadr提供了一个容器，让我们配置某一个字段该使用哪种规则。</span><br><span class="line">add_css add_value add_xpath</span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="comment"># 通过item loader加载item</span></span><br><span class="line">        front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)  <span class="comment"># 文章封面图</span></span><br><span class="line">        item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)</span><br><span class="line">        item_loader.add_css(<span class="string">"title"</span>, <span class="string">".entry-header h1::text"</span>)</span><br><span class="line">        item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line">        item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</span><br><span class="line">        item_loader.add_css(<span class="string">"create_date"</span>, <span class="string">"p.entry-meta-hide-on-mobile::text"</span>)</span><br><span class="line">        item_loader.add_value(<span class="string">"front_image_url"</span>, [front_image_url])</span><br><span class="line">        item_loader.add_css(<span class="string">"praise_nums"</span>, <span class="string">".vote-post-up h10::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"comment_nums"</span>, <span class="string">"a[href='#article-comment'] span::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"fav_nums"</span>, <span class="string">".bookmark-btn::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"tags"</span>, <span class="string">"p.entry-meta-hide-on-mobile a::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"content"</span>, <span class="string">"div.entry"</span>)</span><br><span class="line">        <span class="comment">#调用这个方法来对规则进行解析生成item对象</span></span><br><span class="line">        article_item = item_loader.load_item()</span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/1779926-d34dc1f60a28f9f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="直接使用itemloader的问题"></p><ol><li>所有值变成了list</li><li>对于这些值做一些处理函数<br><strong>item.py中对于item process处理函数</strong><br>MapCompose可以传入函数对于该字段进行处理，而且可以传入多个</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> MapCompose</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_mtianyan</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value+<span class="string">"-mtianyan"</span></span><br><span class="line"></span><br><span class="line"> title = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"mtianyan"</span>,add_mtianyan),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p><em>注意：此处的自定义方法一定要写在代码前面。</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create_date = scrapy.Field(</span><br><span class="line">    input_processor=MapCompose(date_convert),</span><br><span class="line">    output_processor=TakeFirst()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>只取list中的第一个值。</p><p><strong>自定义itemloader实现默认提取第一个</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    <span class="comment">#自定义itemloader实现默认提取第一个</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br></pre></td></tr></table></figure><p><strong>list保存原值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">return_value</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line">front_image_url = scrapy.Field(</span><br><span class="line">        output_processor=MapCompose(return_value)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p><strong>下载图片pipeline增加if增强通用性</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="comment">#重写该方法可从result中获取到图片的实际下载地址</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"front_image_url"</span> <span class="keyword">in</span> item:</span><br><span class="line">            <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</span><br><span class="line">                image_file_path = value[<span class="string">"path"</span>]</span><br><span class="line">            item[<span class="string">"front_image_path"</span>] = image_file_path</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p><strong>自定义的item带处理函数的完整代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(date_convert),</span><br><span class="line">    )</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_object_id = scrapy.Field()</span><br><span class="line">    front_image_url = scrapy.Field(</span><br><span class="line">        output_processor=MapCompose(return_value)</span><br><span class="line">    )</span><br><span class="line">    front_image_path = scrapy.Field()</span><br><span class="line">    praise_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    comment_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    fav_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#因为tag本身是list，所以要重写</span></span><br><span class="line">    tags = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_comment_tags),</span><br><span class="line">        output_processor=Join(<span class="string">","</span>)</span><br><span class="line">    )</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/post/1cc4531e.html">Scrapy分布式爬虫打造搜索引擎- (二)伯乐在线爬取所有文章</a></p><p><span>文章作者:</span><a href="/" title="访问 mtianyan 的个人博客">mtianyan</a></p><p><span>发布时间:</span>2017年06月27日 - 14:06</p><p><span>最后更新:</span>2018年02月02日 - 20:02</p><p><span>原始链接:</span><a href="/post/1cc4531e.html" title="Scrapy分布式爬虫打造搜索引擎- (二)伯乐在线爬取所有文章">http://blog.mtianyan.cn/post/1cc4531e.html</a><span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://blog.mtianyan.cn/post/1cc4531e.html" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"",text:"复制成功",icon:"success",showConfirmButton:!0})})})</script></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>请博主吃包辣条</div> <button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'> <span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"> <img id="wechat_qr" src="/images/wechatpay.png" alt="mtianyan 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"> <img id="alipay_qr" src="/images/alipay.jpg" alt="mtianyan 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/爬虫/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a><a href="/tags/搜索引擎/" rel="tag"><i class="fa fa-tag"></i> 搜索引擎</a><a href="/tags/Scrapy/" rel="tag"><i class="fa fa-tag"></i> Scrapy</a><a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/post/9e947300.html" rel="next" title="Scrapy分布式爬虫打造搜索引擎 - (一)基础知识"><i class="fa fa-chevron-left"></i> Scrapy分布式爬虫打造搜索引擎 - (一)基础知识</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/post/b9bf70b2.html" rel="prev" title="Scrapy分布式爬虫打造搜索引擎- (三)知乎网问题和答案爬取">Scrapy分布式爬虫打造搜索引擎- (三)知乎网问题和答案爬取<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div class="jiathis_style"> <span class="jiathis_txt">分享到：</span> <a class="jiathis_button_fav">收藏夹</a> <a class="jiathis_button_copy">复制网址</a> <a class="jiathis_button_email">邮件</a> <a class="jiathis_button_weixin">微信</a> <a class="jiathis_button_qzone">QQ空间</a> <a class="jiathis_button_tqq">腾讯微博</a> <a class="jiathis_button_douban">豆瓣</a> <a class="jiathis_button_share">一键分享</a> <a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a><a class="jiathis_counter_style"></a></div><script type="text/javascript">var jiathis_config={data_track_clickback:!0,summary:"",shortUrl:!1,hideMore:!1}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=2154292" charset="utf-8"></script></div></div></div><div class="comments" id="comments"><div id="SOHUCS"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> 站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="mtianyan"><p class="site-author-name" itemprop="name">mtianyan</p><p class="site-description motion-element" itemprop="description">爱分享，爱技术，爱生活。</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">37</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">5</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">23</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/mtianyan" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://www.jianshu.com/u/db9a7a0daa1f" target="_blank" title="简书"><i class="fa fa-fw fa-book"></i> 简书</a></span><span class="links-of-author-item"><a href="https://plus.google.com/u/0/114963812195952881148" target="_blank" title="Google"><i class="fa fa-fw fa-google"></i> Google</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="http://mtianyan.gitee.io/" title="本站孪生站(国内码云托管)" target="_blank">本站孪生站(国内码云托管)</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#二、伯乐在线爬取所有文章"><span class="nav-number">1.</span> <span class="nav-text">二、伯乐在线爬取所有文章</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-初始化文件目录"><span class="nav-number">1.1.</span> <span class="nav-text">1. 初始化文件目录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#scrapy项目初始化介绍"><span class="nav-number">1.1.1.</span> <span class="nav-text">scrapy项目初始化介绍</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-提取伯乐在线内容"><span class="nav-number">1.2.</span> <span class="nav-text">2. 提取伯乐在线内容</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#xpath的使用"><span class="nav-number">1.2.1.</span> <span class="nav-text">xpath的使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#css选择器的使用："><span class="nav-number">1.2.2.</span> <span class="nav-text">css选择器的使用：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-爬取所有文章"><span class="nav-number">1.3.</span> <span class="nav-text">3. 爬取所有文章</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-scrapy的items整合字段"><span class="nav-number">1.4.</span> <span class="nav-text">4. scrapy的items整合字段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-数据保存到本地文件以及mysql中"><span class="nav-number">1.5.</span> <span class="nav-text">5. 数据保存到本地文件以及mysql中</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#保存到本地json文件"><span class="nav-number">1.5.1.</span> <span class="nav-text">保存到本地json文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#保存到数据库-mysql"><span class="nav-number">1.5.2.</span> <span class="nav-text">保存到数据库(mysql)</span></a></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">mtianyan</span><div class="theme-info"><div class="powered-by"></div> <span class="post-count">博客全站共182.8k字</span></div></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script> <span class="site-uv">本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人次</span> <span class="site-pv">本站总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><script type="text/javascript">!function(){var t="1eb79150519fd9b791c77e8eef6f3632";if((window.innerWidth||document.documentElement.clientWidth)<960)window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=cyrJmJ3rL&conf='+t+'"><\/script>');else{!function(t,e){var n=document.getElementsByTagName("head")[0]||document.head||document.documentElement,a=document.createElement("script");a.setAttribute("type","text/javascript"),a.setAttribute("charset","UTF-8"),a.setAttribute("src",t),"function"==typeof e&&(window.attachEvent?a.onreadystatechange=function(){var t=a.readyState;"loaded"!==t&&"complete"!==t||(a.onreadystatechange=null,e())}:a.onload=e),n.appendChild(a)}("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:"cyrJmJ3rL",conf:t})})}}()</script><script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script><script type="text/javascript">var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")};function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){var r=!1,s=0,a=0,i=n.title.trim(),c=i.toLowerCase(),l=n.content.trim().replace(/<[^>]+>/g,""),h=l.toLowerCase(),p=decodeURIComponent(n.url),u=[],f=[];if(""!=i&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}u=u.concat(e(t,c,!1)),f=f.concat(e(t,h,!1))}),(u.length>0||f.length>0)&&(r=!0,s=u.length+f.length)),r){[u,f].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});function d(e,o,n,r){for(var s=r[r.length-1],i=s.position,c=s.word,l=[],h=0;i+c.length<=n&&0!=r.length;){c===t&&h++,l.push({position:i,length:c.length});var p=i+c.length;for(r.pop();0!=r.length&&(i=(s=r[r.length-1]).position,c=s.word,p>i);)r.pop()}return a+=h,{hits:l,start:o,end:n,searchTextCount:h}}var g=[];0!=u.length&&g.push(d(0,0,i.length,u));for(var v=[];0!=f.length;){var $=f[f.length-1],C=$.position,m=$.word,x=C-20,w=C+80;x<0&&(x=0),w<C+m.length&&(w=C+m.length),w>l.length&&(w=l.length),v.push(d(0,x,w,f))}v.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var y=parseInt("1");y>=0&&(v=v.slice(0,y));function T(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var b="";0!=g.length?b+="<li><a href='"+p+"' class='search-result-title'>"+T(i,g[0])+"</a>":b+="<li><a href='"+p+"' class='search-result-title'>"+i+"</a>",v.forEach(function(t){b+="<a href='"+p+'\'><p class="search-result">'+T(l,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:a,hitCount:s,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),!1===isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){27===t.which&&$(".search-popup").is(":visible")&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("2uqmIjYredIvFy6tEXbKG4Fj-gzGzoHsz","CWl1rE8cQlIseOg2Cq3hzxYi")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var i=0;i<e.length;i++){var s=e[i],r=s.get("url"),l=s.get("time"),c=document.getElementById(r);$(c).find(t).text(l)}for(i=0;i<n.length;i++){r=n[i],c=document.getElementById(r);var u=$(c).find(t);""==u.text()&&u.text(0)}}else o.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var s=new e,r=new AV.ACL;r.setPublicReadAccess(!0),r.setPublicWriteAccess(!0),s.setACL(r),s.set("title",o),s.set("url",n),s.set("time",1),s.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={},pbOptions.iconStyle="default",pbOptions.boxForm="horizontal",pbOptions.position="bottomCenter",pbOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-postbottom",pbOptions),flOptions={},flOptions.iconStyle="default",flOptions.boxForm="horizontal",flOptions.position="middleRight",flOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-float",flOptions)</script><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><div id="hexo-helper-live2d"><canvas id="live2dcanvas" width="150" height="300"></canvas></div><style>#live2dcanvas{position:fixed;width:150px;height:300px;opacity:.7;right:-30px;z-index:999;pointer-events:none;bottom:40px}</style><script type="text/javascript" src="/live2d/device.min.js"></script><script type="text/javascript">
const loadScript = function loadScript(c,b){var a=document.createElement("script");a.type="text/javascript";"undefined"!=typeof b&&(a.readyState?a.onreadystatechange=function(){if("loaded"==a.readyState||"complete"==a.readyState)a.onreadystatechange=null,b()}:a.onload=function(){b()});a.src=c;document.body.appendChild(a)};
(function(){
  if((typeof(device) != 'undefined') && (device.mobile())){
    var trElement = document.getElementById('hexo-helper-live2d');
    trElement.parentNode.removeChild(trElement);
    return;
  }else
    if (typeof(device) === 'undefined') console.error('Cannot find current-device script.');
  loadScript("/live2d/script.js", function(){loadlive2d("live2dcanvas", "/live2d/assets/hijiki.model.json", 0.5);});
})();
</script></body></html><script type="text/javascript" src="/js/src/love.js"></script>
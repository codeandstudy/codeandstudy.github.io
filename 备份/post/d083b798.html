<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script></script><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="爬虫,搜索引擎,Scrapy,Python,"><link rel="alternate" href="/atom.xml" title="mtianyan's blog" type="application/atom+xml"><meta name="description" content="四、通过CrawlSpider对招聘网站拉钩网进行整站爬取   使用CrawlSpider对于拉勾网进行整站爬取。附带源码解读，数据库建表与爬取后将数据存入数据库等内容。"><meta name="keywords" content="爬虫,搜索引擎,Scrapy,Python"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取"><meta property="og:url" content="http://blog.mtianyan.cn/post/d083b798.html"><meta property="og:site_name" content="mtianyan&#39;s blog"><meta property="og:description" content="四、通过CrawlSpider对招聘网站拉钩网进行整站爬取   使用CrawlSpider对于拉勾网进行整站爬取。附带源码解读，数据库建表与爬取后将数据存入数据库等内容。"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-c22f6fb27848ef5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-1247e16b04708ea3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:updated_time" content="2018-02-02T12:40:13.956Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取"><meta name="twitter:description" content="四、通过CrawlSpider对招聘网站拉钩网进行整站爬取   使用CrawlSpider对于拉勾网进行整站爬取。附带源码解读，数据库建表与爬取后将数据存入数据库等内容。"><meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/1779926-c22f6fb27848ef5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://blog.mtianyan.cn/post/d083b798.html"><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement("script"),n=t.getElementsByTagName("script")[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,0,("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"e28768be"}),daovoice("update")</script><title>Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取 | mtianyan's blog</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?415372bd35fec36f7558dd96b48ec03f";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div> <a href="https://github.com/mtianyan" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">mtianyan's blog</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">天涯明月笙的博客小站(Github托管)</p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br> 公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://blog.mtianyan.cn/post/d083b798.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="mtianyan"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="mtianyan's blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-01T00:56:12+08:00">2017-07-01</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Scrapy分布式爬虫打造搜索引擎/" itemprop="url" rel="index"><span itemprop="name">Scrapy分布式爬虫打造搜索引擎</span></a></span></span> <span id="/post/d083b798.html" class="leancloud_visitors" data-flag-title="Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">热度&#58;</span><span class="leancloud-visitors-count"></span> <span>℃</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">1,959</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">10</span></div></div></header><div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center"><p>四、通过CrawlSpider对招聘网站拉钩网进行整站爬取</p></blockquote><div class="note danger"><p> 使用CrawlSpider对于拉勾网进行整站爬取。<br>附带源码解读，数据库建表与爬取后将数据存入数据库等内容。</p></div><a id="more"></a><p><strong>推荐工具cmder</strong><br><a href="http://cmder.net/" target="_blank" rel="noopener">http://cmder.net/</a><br>下载full版本，使我们在windows环境下也可以使用linux部分命令。<br>配置path环境变量</p><h3 id="1-设计拉勾网的数据表结构"><a href="#1-设计拉勾网的数据表结构" class="headerlink" title="1. 设计拉勾网的数据表结构"></a>1. 设计拉勾网的数据表结构</h3><p><img src="http://upload-images.jianshu.io/upload_images/1779926-c22f6fb27848ef5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="拉勾网数据库表设计"></p><h3 id="2-初始化拉钩网项目并解读crawl源码"><a href="#2-初始化拉钩网项目并解读crawl源码" class="headerlink" title="2. 初始化拉钩网项目并解读crawl源码"></a>2. 初始化拉钩网项目并解读crawl源码</h3><p><code>scrapy genspider --list</code><br>查看可使用的初始化模板<br>ailable templates:</p><ul><li>basic</li><li>crawl</li><li>csvfeed</li><li>xmlfeed</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t crawl lagou www.lagou.com</span><br></pre></td></tr></table></figure><p><strong>cmd与pycharm不同，mark root</strong><br>setting.py 设置目录<br><strong>crawl模板</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'lagou'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.lagou.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.lagou.com/'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r'Items/'</span>), callback=<span class="string">'parse_item'</span>, follow=<span class="keyword">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        <span class="comment">#i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()</span></span><br><span class="line">        <span class="comment">#i['name'] = response.xpath('//div[@id="name"]').extract()</span></span><br><span class="line">        <span class="comment">#i['description'] = response.xpath('//div[@id="description"]').extract()</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre></td></tr></table></figure><p><strong>源码阅读剖析</strong><br><a href="https://doc.scrapy.org/en/1.3/topics/spiders.html#crawlspider" target="_blank" rel="noopener">https://doc.scrapy.org/en/1.3/topics/spiders.html#crawlspider</a></p><p>提供了一些可以让我们进行简单的follow的规则，link，迭代爬取</p><p>rules：</p><blockquote><p>规则，crawel spider读取并执行</p></blockquote><p>parse_start_url(response)：</p><p>example：</p><p>rules是一个可迭代对象，里面有Rule实例-&gt;LinkExtractor的分析<br><code>allow=(&#39;category\.php&#39;, ), callback=&#39;parse_item&#39;,</code><br>allow允许的url模式。callback，要回调的函数名。<br>因为rules里面没有self，无法获取到方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># Extract links matching 'category.php' (but not matching 'subsection.php')</span></span><br><span class="line">        <span class="comment"># and follow links from them (since no callback means follow=True by default).</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Extract links matching 'item.php' and parse them with the spider's method parse_item</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'Hi, this is an item page! %s'</span>, response.url)</span><br><span class="line">        item = scrapy.Item()</span><br><span class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</span><br><span class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</span><br><span class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p><strong>分析拉勾网模板代码</strong></p><ol><li>将http加上s</li><li>重命名parse_item为我们自定义的parse_job</li><li>点击<code>class LagouSpider(CrawlSpider):</code>的CrawlSpider，进入crawl源码</li><li><code>class CrawlSpider(Spider):</code>可以看出它继承于spider</li><li><em>入口：<code>def start_requests(self):</code></em></li><li>alt+左右方向键，不同代码跳转</li><li><em>5-&gt;之后默认parse CrawlSpider里面有parse函数。但是这次我们不能向以前一样覆盖</em></li></ol><p>Crawl.py核心函数parse。</p><blockquote><p>parse函数调用_parse_response</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">       <span class="keyword">return</span> self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><code>_parse_response</code></p><ol><li>判断是否有callback即有没有self.parse_start_url</li><li>我们可以重载parse_start_url加入自己的处理</li><li>把参数传递给函数，并调用process_results函数</li></ol><p><code>_parse_response函数</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(self, response, callback, cb_kwargs, follow=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> callback:</span><br><span class="line">        cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</span><br><span class="line">        cb_res = self.process_results(response, cb_res)</span><br><span class="line">        <span class="keyword">for</span> requests_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</span><br><span class="line">            <span class="keyword">yield</span> requests_or_item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> follow <span class="keyword">and</span> self._follow_links:</span><br><span class="line">        <span class="keyword">for</span> request_or_item <span class="keyword">in</span> self._requests_to_follow(response):</span><br><span class="line">            <span class="keyword">yield</span> request_or_item</span><br></pre></td></tr></table></figure><p>parse_start_url的return值将会被process_results方法接收处理<br>如果不重写，因为返回为空，然后就相当于什么都没做</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_results</span><span class="params">(self, response, results)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><p>点击followlink</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_crawler</span><span class="params">(self, crawler)</span>:</span></span><br><span class="line">    super(CrawlSpider, self).set_crawler(crawler)</span><br><span class="line">    self._follow_links = crawler.settings.getbool(<span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>如果setting中有这个参数，则可以进一步执行到parse</p><p><code>_requests_to_follow</code></p><ol><li>判断传入的是不是response，如果不是直接returns</li><li>针对当前response设置一个空set，去重</li><li>把self的rules通过enumerate变成一个可迭代对象</li><li>跳转rules详情</li><li>拿到link通过link_extractor.extract_links抽取出具体的link</li><li>执行我们的process_links</li><li>link制作完成发起Request,回调_response_downloaded函数</li><li>然后执行parse_respose</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    seen = set()</span><br><span class="line">    <span class="keyword">for</span> n, rule <span class="keyword">in</span> enumerate(self._rules):</span><br><span class="line">        links = [lnk <span class="keyword">for</span> lnk <span class="keyword">in</span> rule.link_extractor.extract_links(response)</span><br><span class="line">                 <span class="keyword">if</span> lnk <span class="keyword">not</span> <span class="keyword">in</span> seen]</span><br><span class="line">        <span class="keyword">if</span> links <span class="keyword">and</span> rule.process_links:</span><br><span class="line">            links = rule.process_links(links)</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">            seen.add(link)</span><br><span class="line">            r = Request(url=link.url, callback=self._response_downloaded)</span><br><span class="line">            r.meta.update(rule=n, link_text=link.text)</span><br><span class="line">            <span class="keyword">yield</span> rule.process_request(r)</span><br></pre></td></tr></table></figure><p><code>_compile_rules</code></p><ol><li>在我们初始化时会调用_compile_rules</li><li><code>copy.copy(r) for r in self.rules]</code>将我们的rules进行一个copy</li><li>调用回调函数get_method。</li><li>调用rules里面我们定义的process_links</li><li>调用rules里面我们定义的process_request</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_method</span><span class="params">(method)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> callable(method):</span><br><span class="line">            <span class="keyword">return</span> method</span><br><span class="line">        <span class="keyword">elif</span> isinstance(method, six.string_types):</span><br><span class="line">            <span class="keyword">return</span> getattr(self, method, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    self._rules = [copy.copy(r) <span class="keyword">for</span> r <span class="keyword">in</span> self.rules]</span><br><span class="line">    <span class="keyword">for</span> rule <span class="keyword">in</span> self._rules:</span><br><span class="line">        rule.callback = get_method(rule.callback)</span><br><span class="line">        rule.process_links = get_method(rule.process_links)</span><br><span class="line">        rule.process_request = get_method(rule.process_request)</span><br></pre></td></tr></table></figure><pre><code>self.process_links = process_links
self.process_request = process_request
</code></pre><p>可以通过在rules里面传入我们自己的处理函数，实现对url的自定义。<br>达到负载均衡，多地不同ip访问。</p><p><code>_response_downloaded</code><br>通过rule取到具体的rule<br>调用我们自己的回调函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_response_downloaded</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    rule = self._rules[response.meta[<span class="string">'rule'</span>]]</span><br><span class="line">    <span class="keyword">return</span> self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)</span><br></pre></td></tr></table></figure><ul><li>allow ：符合这个url我就爬取</li><li>deny : 符合这个url规则我就放弃</li><li>allow_domin : 这个域名下的我才处理</li><li>allow_domin : 这个域名下的我不处理</li><li>restrict_xpaths：进一步限定xpath</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),</span><br><span class="line">                 tags=(<span class="string">'a'</span>, <span class="string">'area'</span>), attrs=(<span class="string">'href'</span>,), canonicalize=<span class="keyword">True</span>,</span><br><span class="line">                 unique=<span class="keyword">True</span>, process_value=<span class="keyword">None</span>, deny_extensions=<span class="keyword">None</span>, restrict_css=()</span><br></pre></td></tr></table></figure><p>extract_links<br>如果有restrict_xpaths，他会进行读取执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_links</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    base_url = get_base_url(response)</span><br><span class="line">    <span class="keyword">if</span> self.restrict_xpaths:</span><br><span class="line">        docs = [subdoc</span><br><span class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> self.restrict_xpaths</span><br><span class="line">                <span class="keyword">for</span> subdoc <span class="keyword">in</span> response.xpath(x)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        docs = [response.selector]</span><br><span class="line">    all_links = []</span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">        links = self._extract_links(doc, response.url, response.encoding, base_url)</span><br><span class="line">        all_links.extend(self._process_links(links))</span><br><span class="line">    <span class="keyword">return</span> unique_list(all_links)</span><br></pre></td></tr></table></figure><p>get_base_url:</p><p>urllib.parse.urljoin替我们拼接好url</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_base_url</span><span class="params">(text, baseurl=<span class="string">''</span>, encoding=<span class="string">'utf-8'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Return the base url if declared in the given HTML `text`,</span></span><br><span class="line"><span class="string">    relative to the given base url.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If no base url is found, the given `baseurl` is returned.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    text = to_unicode(text, encoding)</span><br><span class="line">    m = _baseurl_re.search(text)</span><br><span class="line">    <span class="keyword">if</span> m:</span><br><span class="line">        <span class="keyword">return</span> moves.urllib.parse.urljoin(</span><br><span class="line">            safe_url_string(baseurl),</span><br><span class="line">            safe_url_string(m.group(<span class="number">1</span>), encoding=encoding)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> safe_url_string(baseurl)</span><br></pre></td></tr></table></figure><h4 id="编写rule规则"><a href="#编写rule规则" class="headerlink" title="编写rule规则"></a>编写rule规则</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">    Rule(LinkExtractor(allow=(<span class="string">"zhaopin/.*"</span>,)), follow=<span class="keyword">True</span>),</span><br><span class="line">    Rule(LinkExtractor(allow=(<span class="string">"gongsi/j\d+.html"</span>,)), follow=<span class="keyword">True</span>),</span><br><span class="line">    Rule(LinkExtractor(allow=<span class="string">r'jobs/\d+.html'</span>), callback=<span class="string">'parse_job'</span>, follow=<span class="keyword">True</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="3-设计lagou的items"><a href="#3-设计lagou的items" class="headerlink" title="3. 设计lagou的items"></a>3. 设计lagou的items</h3><p><strong>需要用到的方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> w3lib.html <span class="keyword">import</span> remove_tags</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_splash</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="comment">#去掉工作城市的斜线</span></span><br><span class="line">    <span class="keyword">return</span> value.replace(<span class="string">"/"</span>,<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_jobaddr</span><span class="params">(value)</span>:</span></span><br><span class="line">    addr_list = value.split(<span class="string">"\n"</span>)</span><br><span class="line">    addr_list = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> addr_list <span class="keyword">if</span> item.strip()!=<span class="string">"查看地图"</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">""</span>.join(addr_list)</span><br></pre></td></tr></table></figure><p><strong>定义好的item</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouJobItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment">#拉勾网职位信息</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_object_id = scrapy.Field()</span><br><span class="line">    salary = scrapy.Field()</span><br><span class="line">    job_city = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_splash),</span><br><span class="line">    )</span><br><span class="line">    work_years = scrapy.Field(</span><br><span class="line">        input_processor = MapCompose(remove_splash),</span><br><span class="line">    )</span><br><span class="line">    degree_need = scrapy.Field(</span><br><span class="line">        input_processor = MapCompose(remove_splash),</span><br><span class="line">    )</span><br><span class="line">    job_type = scrapy.Field()</span><br><span class="line">    publish_time = scrapy.Field()</span><br><span class="line">    job_advantage = scrapy.Field()</span><br><span class="line">    job_desc = scrapy.Field()</span><br><span class="line">    job_addr = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags, handle_jobaddr),</span><br><span class="line">    )</span><br><span class="line">    company_name = scrapy.Field()</span><br><span class="line">    company_url = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field(</span><br><span class="line">        input_processor = Join(<span class="string">","</span>)</span><br><span class="line">    )</span><br><span class="line">    crawl_time = scrapy.Field()</span><br></pre></td></tr></table></figure><p></p><p><strong>重写的itemloader</strong><br>设置默认只提取第一个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouJobItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    <span class="comment">#自定义itemloader</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br></pre></td></tr></table></figure><h4 id="4-提取字段值并存入数据库"><a href="#4-提取字段值并存入数据库" class="headerlink" title="4. 提取字段值并存入数据库"></a>4. 提取字段值并存入数据库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_job</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#解析拉勾网的职位</span></span><br><span class="line">    item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response)</span><br><span class="line">    item_loader.add_css(<span class="string">"title"</span>, <span class="string">".job-name::attr(title)"</span>)</span><br><span class="line">    item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line">    item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</span><br><span class="line">    item_loader.add_css(<span class="string">"salary"</span>, <span class="string">".job_request .salary::text"</span>)</span><br><span class="line">    item_loader.add_xpath(<span class="string">"job_city"</span>, <span class="string">"//*[@class='job_request']/p/span[2]/text()"</span>)</span><br><span class="line">    item_loader.add_xpath(<span class="string">"work_years"</span>, <span class="string">"//*[@class='job_request']/p/span[3]/text()"</span>)</span><br><span class="line">    item_loader.add_xpath(<span class="string">"degree_need"</span>, <span class="string">"//*[@class='job_request']/p/span[4]/text()"</span>)</span><br><span class="line">    item_loader.add_xpath(<span class="string">"job_type"</span>, <span class="string">"//*[@class='job_request']/p/span[5]/text()"</span>)</span><br><span class="line"></span><br><span class="line">    item_loader.add_css(<span class="string">"tags"</span>, <span class="string">'.position-label li::text'</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"publish_time"</span>, <span class="string">".publish_time::text"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"job_advantage"</span>, <span class="string">".job-advantage p::text"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"job_desc"</span>, <span class="string">".job_bt div"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"job_addr"</span>, <span class="string">".work_addr"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"company_name"</span>, <span class="string">"#job_company dt a img::attr(alt)"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"company_url"</span>, <span class="string">"#job_company dt a::attr(href)"</span>)</span><br><span class="line">    item_loader.add_value(<span class="string">"crawl_time"</span>, datetime.now())</span><br><span class="line"></span><br><span class="line">    job_item = item_loader.load_item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> job_item</span><br></pre></td></tr></table></figure><p><strong>获得的拉勾网item数据</strong><br><img src="http://upload-images.jianshu.io/upload_images/1779926-1247e16b04708ea3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="拉勾网item数据"></p><h4 id="5-items中添加get-insert-sql实现存入数据库"><a href="#5-items中添加get-insert-sql实现存入数据库" class="headerlink" title="5. items中添加get_insert_sql实现存入数据库"></a>5. items中添加get_insert_sql实现存入数据库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></span><br><span class="line">       insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">           insert into lagou_job(title, url, url_object_id, salary, job_city, work_years, degree_need,</span></span><br><span class="line"><span class="string">           job_type, publish_time, job_advantage, job_desc, job_addr, company_name, company_url,</span></span><br><span class="line"><span class="string">           tags, crawl_time) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span></span><br><span class="line"><span class="string">           ON DUPLICATE KEY UPDATE salary=VALUES(salary), job_desc=VALUES(job_desc)</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">       params = (</span><br><span class="line">           self[<span class="string">"title"</span>], self[<span class="string">"url"</span>], self[<span class="string">"url_object_id"</span>], self[<span class="string">"salary"</span>], self[<span class="string">"job_city"</span>],</span><br><span class="line">           self[<span class="string">"work_years"</span>], self[<span class="string">"degree_need"</span>], self[<span class="string">"job_type"</span>],</span><br><span class="line">           self[<span class="string">"publish_time"</span>], self[<span class="string">"job_advantage"</span>], self[<span class="string">"job_desc"</span>],</span><br><span class="line">           self[<span class="string">"job_addr"</span>], self[<span class="string">"company_name"</span>], self[<span class="string">"company_url"</span>],</span><br><span class="line">           self[<span class="string">"job_addr"</span>], self[<span class="string">"crawl_time"</span>].strftime(SQL_DATETIME_FORMAT),</span><br><span class="line">       )</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> insert_sql, params</span><br></pre></td></tr></table></figure></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/post/d083b798.html">Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取</a></p><p><span>文章作者:</span><a href="/" title="访问 mtianyan 的个人博客">mtianyan</a></p><p><span>发布时间:</span>2017年07月01日 - 00:07</p><p><span>最后更新:</span>2018年02月02日 - 20:02</p><p><span>原始链接:</span><a href="/post/d083b798.html" title="Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取">http://blog.mtianyan.cn/post/d083b798.html</a><span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://blog.mtianyan.cn/post/d083b798.html" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"",text:"复制成功",icon:"success",showConfirmButton:!0})})})</script></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>请博主吃包辣条</div> <button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'> <span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"> <img id="wechat_qr" src="/images/wechatpay.png" alt="mtianyan 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"> <img id="alipay_qr" src="/images/alipay.jpg" alt="mtianyan 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/爬虫/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a><a href="/tags/搜索引擎/" rel="tag"><i class="fa fa-tag"></i> 搜索引擎</a><a href="/tags/Scrapy/" rel="tag"><i class="fa fa-tag"></i> Scrapy</a><a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/post/b9bf70b2.html" rel="next" title="Scrapy分布式爬虫打造搜索引擎- (三)知乎网问题和答案爬取"><i class="fa fa-chevron-left"></i> Scrapy分布式爬虫打造搜索引擎- (三)知乎网问题和答案爬取</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/post/791a397f.html" rel="prev" title="Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争">Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div class="jiathis_style"> <span class="jiathis_txt">分享到：</span> <a class="jiathis_button_fav">收藏夹</a> <a class="jiathis_button_copy">复制网址</a> <a class="jiathis_button_email">邮件</a> <a class="jiathis_button_weixin">微信</a> <a class="jiathis_button_qzone">QQ空间</a> <a class="jiathis_button_tqq">腾讯微博</a> <a class="jiathis_button_douban">豆瓣</a> <a class="jiathis_button_share">一键分享</a> <a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a><a class="jiathis_counter_style"></a></div><script type="text/javascript">var jiathis_config={data_track_clickback:!0,summary:"",shortUrl:!1,hideMore:!1}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=2154292" charset="utf-8"></script></div></div></div><div class="comments" id="comments"><div id="SOHUCS"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> 站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="mtianyan"><p class="site-author-name" itemprop="name">mtianyan</p><p class="site-description motion-element" itemprop="description">爱分享，爱技术，爱生活。</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">37</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">5</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">23</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/mtianyan" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://www.jianshu.com/u/db9a7a0daa1f" target="_blank" title="简书"><i class="fa fa-fw fa-book"></i> 简书</a></span><span class="links-of-author-item"><a href="https://plus.google.com/u/0/114963812195952881148" target="_blank" title="Google"><i class="fa fa-fw fa-google"></i> Google</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="http://mtianyan.gitee.io/" title="本站孪生站(国内码云托管)" target="_blank">本站孪生站(国内码云托管)</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-设计拉勾网的数据表结构"><span class="nav-number">1.</span> <span class="nav-text">1. 设计拉勾网的数据表结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-初始化拉钩网项目并解读crawl源码"><span class="nav-number">2.</span> <span class="nav-text">2. 初始化拉钩网项目并解读crawl源码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#编写rule规则"><span class="nav-number">2.1.</span> <span class="nav-text">编写rule规则</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-设计lagou的items"><span class="nav-number">3.</span> <span class="nav-text">3. 设计lagou的items</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-提取字段值并存入数据库"><span class="nav-number">3.1.</span> <span class="nav-text">4. 提取字段值并存入数据库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-items中添加get-insert-sql实现存入数据库"><span class="nav-number">3.2.</span> <span class="nav-text">5. items中添加get_insert_sql实现存入数据库</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">mtianyan</span><div class="theme-info"><div class="powered-by"></div> <span class="post-count">博客全站共182.8k字</span></div></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script> <span class="site-uv">本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人次</span> <span class="site-pv">本站总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><script type="text/javascript">!function(){var t="1eb79150519fd9b791c77e8eef6f3632";if((window.innerWidth||document.documentElement.clientWidth)<960)window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=cyrJmJ3rL&conf='+t+'"><\/script>');else{!function(t,e){var n=document.getElementsByTagName("head")[0]||document.head||document.documentElement,a=document.createElement("script");a.setAttribute("type","text/javascript"),a.setAttribute("charset","UTF-8"),a.setAttribute("src",t),"function"==typeof e&&(window.attachEvent?a.onreadystatechange=function(){var t=a.readyState;"loaded"!==t&&"complete"!==t||(a.onreadystatechange=null,e())}:a.onload=e),n.appendChild(a)}("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:"cyrJmJ3rL",conf:t})})}}()</script><script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script><script type="text/javascript">var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")};function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){var r=!1,s=0,a=0,i=n.title.trim(),c=i.toLowerCase(),l=n.content.trim().replace(/<[^>]+>/g,""),h=l.toLowerCase(),p=decodeURIComponent(n.url),u=[],f=[];if(""!=i&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}u=u.concat(e(t,c,!1)),f=f.concat(e(t,h,!1))}),(u.length>0||f.length>0)&&(r=!0,s=u.length+f.length)),r){[u,f].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});function d(e,o,n,r){for(var s=r[r.length-1],i=s.position,c=s.word,l=[],h=0;i+c.length<=n&&0!=r.length;){c===t&&h++,l.push({position:i,length:c.length});var p=i+c.length;for(r.pop();0!=r.length&&(i=(s=r[r.length-1]).position,c=s.word,p>i);)r.pop()}return a+=h,{hits:l,start:o,end:n,searchTextCount:h}}var g=[];0!=u.length&&g.push(d(0,0,i.length,u));for(var v=[];0!=f.length;){var $=f[f.length-1],C=$.position,m=$.word,x=C-20,w=C+80;x<0&&(x=0),w<C+m.length&&(w=C+m.length),w>l.length&&(w=l.length),v.push(d(0,x,w,f))}v.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var y=parseInt("1");y>=0&&(v=v.slice(0,y));function T(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var b="";0!=g.length?b+="<li><a href='"+p+"' class='search-result-title'>"+T(i,g[0])+"</a>":b+="<li><a href='"+p+"' class='search-result-title'>"+i+"</a>",v.forEach(function(t){b+="<a href='"+p+'\'><p class="search-result">'+T(l,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:a,hitCount:s,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),!1===isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){27===t.which&&$(".search-popup").is(":visible")&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("2uqmIjYredIvFy6tEXbKG4Fj-gzGzoHsz","CWl1rE8cQlIseOg2Cq3hzxYi")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var i=0;i<e.length;i++){var s=e[i],r=s.get("url"),l=s.get("time"),c=document.getElementById(r);$(c).find(t).text(l)}for(i=0;i<n.length;i++){r=n[i],c=document.getElementById(r);var u=$(c).find(t);""==u.text()&&u.text(0)}}else o.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var s=new e,r=new AV.ACL;r.setPublicReadAccess(!0),r.setPublicWriteAccess(!0),s.setACL(r),s.set("title",o),s.set("url",n),s.set("time",1),s.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={},pbOptions.iconStyle="default",pbOptions.boxForm="horizontal",pbOptions.position="bottomCenter",pbOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-postbottom",pbOptions),flOptions={},flOptions.iconStyle="default",flOptions.boxForm="horizontal",flOptions.position="middleRight",flOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-float",flOptions)</script><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><div id="hexo-helper-live2d"><canvas id="live2dcanvas" width="150" height="300"></canvas></div><style>#live2dcanvas{position:fixed;width:150px;height:300px;opacity:.7;right:-30px;z-index:999;pointer-events:none;bottom:40px}</style><script type="text/javascript" src="/live2d/device.min.js"></script><script type="text/javascript">
const loadScript = function loadScript(c,b){var a=document.createElement("script");a.type="text/javascript";"undefined"!=typeof b&&(a.readyState?a.onreadystatechange=function(){if("loaded"==a.readyState||"complete"==a.readyState)a.onreadystatechange=null,b()}:a.onload=function(){b()});a.src=c;document.body.appendChild(a)};
(function(){
  if((typeof(device) != 'undefined') && (device.mobile())){
    var trElement = document.getElementById('hexo-helper-live2d');
    trElement.parentNode.removeChild(trElement);
    return;
  }else
    if (typeof(device) === 'undefined') console.error('Cannot find current-device script.');
  loadScript("/live2d/script.js", function(){loadlive2d("live2dcanvas", "/live2d/assets/hijiki.model.json", 0.5);});
})();
</script></body></html><script type="text/javascript" src="/js/src/love.js"></script>
<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script></script><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="爬虫,搜索引擎,Scrapy,Python,"><link rel="alternate" href="/atom.xml" title="mtianyan's blog" type="application/atom+xml"><meta name="description" content="震惊了李彦宏 , 看哭了马化腾    基于Scrapy、Redis、elasticsearch和django打造一个完整的搜索引擎网站 本教程一共八章：从零开始，直到搭建一个搜索引擎。 项目github代码地址：https://github.com/mtianyan/ArticleSpider"><meta name="keywords" content="爬虫,搜索引擎,Scrapy,Python"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy分布式爬虫打造搜索引擎- 系列完整版"><meta property="og:url" content="http://blog.mtianyan.cn/post/3c234171.html"><meta property="og:site_name" content="mtianyan&#39;s blog"><meta property="og:description" content="震惊了李彦宏 , 看哭了马化腾    基于Scrapy、Redis、elasticsearch和django打造一个完整的搜索引擎网站 本教程一共八章：从零开始，直到搭建一个搜索引擎。 项目github代码地址：https://github.com/mtianyan/ArticleSpider"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-d0f10707f0b49b08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-b46e91f577c5bf6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-e3b33dc61bc8ab3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-909b623357abad96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-d34dc1f60a28f9f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-f01f9c33e578427d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-e5d75b510a604f78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-31ff3c83ea890269.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-cf6b7ac1027726fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-e972fd3af04fc8f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-ce3f15b285ed1e64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-4be3a480d6f73679.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-942806c2e9ed83cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-c12492078a2369f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-c22f6fb27848ef5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-1247e16b04708ea3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-d48e923ecb3d20ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-77314219c9f1757e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-8381473a93549bc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-8f93d40d66c9fe04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-b3de4a107a839138.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-82031cfa2b9af70c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-84340c76287bd524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-45b0ab9d1030c91c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-f7ba853e61e8b637.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-a2d90a6f90511675.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-e61a277016449a3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-e61a277016449a3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-b39e996d99c70d2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-1604a12ed995ea8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-1c550de187cbf9f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-9608d37c848236d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:updated_time" content="2018-02-02T12:40:13.960Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Scrapy分布式爬虫打造搜索引擎- 系列完整版"><meta name="twitter:description" content="震惊了李彦宏 , 看哭了马化腾    基于Scrapy、Redis、elasticsearch和django打造一个完整的搜索引擎网站 本教程一共八章：从零开始，直到搭建一个搜索引擎。 项目github代码地址：https://github.com/mtianyan/ArticleSpider"><meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/1779926-d0f10707f0b49b08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://blog.mtianyan.cn/post/3c234171.html"><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement("script"),n=t.getElementsByTagName("script")[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,0,("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"e28768be"}),daovoice("update")</script><title>Scrapy分布式爬虫打造搜索引擎- 系列完整版 | mtianyan's blog</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?415372bd35fec36f7558dd96b48ec03f";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div> <a href="https://github.com/mtianyan" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">mtianyan's blog</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">天涯明月笙的博客小站(Github托管)</p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br> 公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://blog.mtianyan.cn/post/3c234171.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="mtianyan"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="mtianyan's blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Scrapy分布式爬虫打造搜索引擎- 系列完整版</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-01T20:00:03+08:00">2018-01-01</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Scrapy分布式爬虫打造搜索引擎/" itemprop="url" rel="index"><span itemprop="name">Scrapy分布式爬虫打造搜索引擎</span></a></span></span> <span id="/post/3c234171.html" class="leancloud_visitors" data-flag-title="Scrapy分布式爬虫打造搜索引擎- 系列完整版"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">热度&#58;</span><span class="leancloud-visitors-count"></span> <span>℃</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">17,785</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">86</span></div></div></header><div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center"><p>震惊了李彦宏 , 看哭了马化腾</p></blockquote><div class="note warning"><p> 基于Scrapy、Redis、elasticsearch和django打造一个完整的搜索引擎网站</p><p>本教程一共八章：从零开始，直到搭建一个搜索引擎。</p><p>项目github代码地址：<a href="https://github.com/mtianyan/ArticleSpider" target="_blank" rel="noopener">https://github.com/mtianyan/ArticleSpider</a></p></div><a id="more"></a><p>未来是什么时代？是数据时代！数据分析服务、互联网金融，数据建模、自然语言处理、医疗病例分析……越来越多的工作会基于数据来做，而爬虫正是快速获取数据最重要的方式，相比其它语言，Python爬虫更简单、高效</p><h2 id="一、基础知识学习"><a href="#一、基础知识学习" class="headerlink" title="一、基础知识学习:"></a>一、基础知识学习:</h2><h3 id="1-爬取策略的深度优先和广度优先"><a href="#1-爬取策略的深度优先和广度优先" class="headerlink" title="1. 爬取策略的深度优先和广度优先"></a>1. 爬取策略的深度优先和广度优先</h3><p>目录：</p><blockquote><ol><li>网站的树结构</li><li>深度优先算法和实现</li><li>广度优先算法和实现</li></ol></blockquote><p>网站url树结构分层设计:</p><ul><li>bogbole.com<ul><li>blog.bogbole.com</li><li>python.bogbole.com<ul><li>python.bogbole.com/123</li></ul></li></ul></li></ul><p>环路链接问题：</p><blockquote><p>从首页到下面节点。<br>但是下面的链接节点又会有链接指向首页</p></blockquote><p>所以：我们需要对于链接进行去重</p><p><strong>1. 深度优先</strong><br><strong>2. 广度优先</strong></p><blockquote><p>跳过已爬取的链接<br>对于二叉树的遍历问题</p></blockquote><p>深度优先(递归实现)：<br> 顺着一条路，走到最深处。然后回头</p><p>广度优先(队列实现):<br> 分层遍历：遍历完儿子辈。然后遍历孙子辈</p><p>Python实现深度优先过程code：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">depth_tree</span><span class="params">(tree_node)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> tree_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">print</span> (tree_node._data)</span><br><span class="line">        <span class="keyword">if</span> tree_node._left <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> depth_tree(tree_node.left)</span><br><span class="line">        <span class="keyword">if</span> tree_node._right <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> depth_tree(tree_node,_right)</span><br></pre></td></tr></table></figure><p></p><p>Python实现广度优先过程code：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">level_queue</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="comment">#利用队列实现树的广度优先遍历</span></span><br><span class="line">    <span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    my_queue = []</span><br><span class="line">    node = root</span><br><span class="line">    my_queue.append(node)</span><br><span class="line">    <span class="keyword">while</span> my_queue:</span><br><span class="line">        node = my_queue.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">print</span> (node.elem)</span><br><span class="line">        <span class="keyword">if</span> node.lchild <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            my_queue.append(node.lchild)</span><br><span class="line">        <span class="keyword">if</span> node.rchild <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            my_queue.append(node.rchild)</span><br></pre></td></tr></table></figure><p></p><h3 id="2-爬虫网址去重策略"><a href="#2-爬虫网址去重策略" class="headerlink" title="2. 爬虫网址去重策略"></a>2. 爬虫网址去重策略</h3><ol><li>将访问过的url保存到数据库中</li><li>将url保存到set中。只需要O(1)的代价就可以查询到url<blockquote><p>100000000*2byte*50个字符/1024/1024/1024 = 9G</p></blockquote></li><li>url经过md5等方法哈希后保存到set中，将url压缩到固定长度而且不重复</li><li>用bitmap方法，将访问过的url通过hash函数映射到某一位</li><li>bloomfilter方法对bitmap进行改进，多重hash函数降低冲突</li></ol><p>scrapy去重使用的是第三种方法：后面分布式scrapy-redis会讲解bloomfilter方法。</p><h3 id="3-Python字符串编码问题解决："><a href="#3-Python字符串编码问题解决：" class="headerlink" title="3. Python字符串编码问题解决："></a>3. Python字符串编码问题解决：</h3><blockquote><ol><li>计算机只能处理数字，文本转换为数字才能处理，计算机中8个bit作为一个字节，<br>所以一个字节能表示的最大数字就是255</li><li>计算机是美国人发明的，所以一个字节就可以标识所有单个字符<br>，所以ASCII(一个字节)编码就成为美国人的标准编码</li><li>但是ASCII处理中文明显不够，中文不止255个汉字，所以中国制定了GB2312编码<br>，用两个字节表示一个汉字。GB2312将ASCII也包含进去了。同理，日文，韩文，越来越多的国家为了解决这个问题就都发展了一套编码，标准越来越多，如果出现多种语言混合显示就一定会出现乱码</li><li>于是unicode出现了，它将所有语言包含进去了。</li><li>看一下ASCII和unicode编码:<ol><li>字母A用ASCII编码十进制是65，二进制 0100 0001</li><li>汉字”中” 已近超出ASCII编码的范围，用unicode编码是20013二进制是01001110 00101101</li><li>A用unicode编码只需要前面补0二进制是 00000000 0100 0001</li></ol></li><li>乱码问题解决的，但是如果内容全是英文，unicode编码比ASCII编码需要多一倍的存储空间，传输也会变慢。</li><li>所以此时出现了可变长的编码”utf-8” ,把英文：1字节，汉字3字节，特别生僻的变成4-6字节，如果传输大量的英文，utf8作用就很明显。</li></ol></blockquote><p><strong>读取文件，进行操作时转换为unicode编码进行处理</strong><br><strong>保存文件时，转换为utf-8编码。以便于传输</strong><br>读文件的库会将转换为unicode</p><p><em>python2 默认编码格式为<code>ASCII</code>，Python3 默认编码为 <code>utf-8</code></em><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python3</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.getdefaultencoding()</span><br><span class="line">s.encoding(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python2</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.getdefaultencoding()</span><br><span class="line">s = <span class="string">"我和你"</span></span><br><span class="line">su = <span class="string">u"我和你"</span></span><br><span class="line">~~s.encode(<span class="string">"utf-8"</span>)<span class="comment">#会报错~~</span></span><br><span class="line">s.decode(<span class="string">"gb2312"</span>).encode(<span class="string">"utf-8"</span>)</span><br><span class="line">su.encode(<span class="string">"utf-8"</span>)</span><br></pre></td></tr></table></figure><h2 id="二、伯乐在线爬取所有文章"><a href="#二、伯乐在线爬取所有文章" class="headerlink" title="二、伯乐在线爬取所有文章"></a>二、伯乐在线爬取所有文章</h2><h3 id="1-初始化文件目录"><a href="#1-初始化文件目录" class="headerlink" title="1. 初始化文件目录"></a>1. 初始化文件目录</h3><p>基础环境</p><blockquote><ol><li>python 3.5.1</li><li>JetBrains PyCharm 2016.3.2</li><li>mysql+navicat</li></ol></blockquote><p>为了便于日后的部署：我们开发使用了虚拟环境。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenv</span><br><span class="line">pip install virtualenvwrapper-win</span><br><span class="line">安装虚拟环境管理</span><br><span class="line">mkvirtualenv articlespider3</span><br><span class="line">创建虚拟环境</span><br><span class="line">workon articlespider3</span><br><span class="line">直接进入虚拟环境</span><br><span class="line">deactivate</span><br><span class="line">退出激活状态</span><br><span class="line">workon</span><br><span class="line">知道有哪些虚拟环境</span><br></pre></td></tr></table></figure><h4 id="scrapy项目初始化介绍"><a href="#scrapy项目初始化介绍" class="headerlink" title="scrapy项目初始化介绍"></a>scrapy项目初始化介绍</h4><blockquote><p>自行官网下载py35对应得whl文件进行pip离线安装<br>Scrapy 1.3.3</p></blockquote><p><strong>命令行创建scrapy项目</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd desktop</span><br><span class="line"></span><br><span class="line">scrapy startproject ArticleSpider</span><br></pre></td></tr></table></figure><p></p><p><strong>scrapy目录结构</strong></p><p>scrapy借鉴了django的项目思想</p><blockquote><ul><li><code>scrapy.cfg</code>：配置文件。</li><li><code>setings.py</code>：设置</li></ul></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPIDER_MODULES = [&apos;ArticleSpider.spiders&apos;] #存放spider的路径</span><br><span class="line">NEWSPIDER_MODULE = &apos;ArticleSpider.spiders&apos;</span><br></pre></td></tr></table></figure><p>pipelines.py:</p><blockquote><p>做跟数据存储相关的东西</p></blockquote><p>middilewares.py:</p><blockquote><p>自己定义的middlewares 定义方法，处理响应的IO操作</p></blockquote><p><strong>init</strong>.py:</p><blockquote><p>项目的初始化文件。</p></blockquote><p>items.py：</p><blockquote><p>定义我们所要爬取的信息的相关属性。Item对象是种类似于表单，用来保存获取到的数据</p></blockquote><p><strong>创建我们的spider</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ArticleSpider</span><br><span class="line">scrapy genspider jobbole blog.jobbole.com</span><br></pre></td></tr></table></figure><p></p><p>可以看到直接为我们创建好的空项目里已经有了模板代码。如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    <span class="comment"># start_urls是一个带爬的列表，</span></span><br><span class="line">    <span class="comment">#spider会为我们把请求下载网页做到，直接到parse阶段</span></span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>scray在命令行启动某一个Spyder的命令:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl jobbole</span><br></pre></td></tr></table></figure><p></p><p><strong>在windows报出错误</strong></p><p><code>ImportError: No module named &#39;win32api&#39;</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pypiwin32#解决</span><br></pre></td></tr></table></figure><p><strong>创建我们的调试工具类*</strong></p><p>在项目根目录里创建main.py<br>作为调试工具文件<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></span><br><span class="line">__author__ = <span class="string">'mtianyan'</span></span><br><span class="line">__date__ = <span class="string">'2017/3/28 12:06'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">#将系统当前目录设置为项目根目录</span></span><br><span class="line"><span class="comment">#os.path.abspath(__file__)为当前文件所在绝对路径</span></span><br><span class="line"><span class="comment">#os.path.dirname为文件所在目录</span></span><br><span class="line"><span class="comment">#H:\CodePath\spider\ArticleSpider\main.py</span></span><br><span class="line"><span class="comment">#H:\CodePath\spider\ArticleSpider</span></span><br><span class="line">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line"><span class="comment">#执行命令，相当于在控制台cmd输入改名了</span></span><br><span class="line">execute([<span class="string">"scrapy"</span>, <span class="string">"crawl"</span> , <span class="string">"jobbole"</span>])</span><br></pre></td></tr></table></figure><p></p><p><strong>settings.py的设置不遵守reboots协议</strong></p><p><code>ROBOTSTXT_OBEY = False</code></p><p>在jobble.py打上断点:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure><p></p><p>可以看到他返回的htmlresponse对象:<br>对象内部：</p><blockquote><ul><li>body:网页内容</li><li>_DEFAULT_ENCODING= ‘ascii’</li><li>encoding= ‘utf-8’</li></ul></blockquote><p>可以看出scrapy已经为我们做到了将网页下载下来。而且编码也进行了转换.</p><h3 id="2-提取伯乐在线内容"><a href="#2-提取伯乐在线内容" class="headerlink" title="2. 提取伯乐在线内容"></a>2. 提取伯乐在线内容</h3><h4 id="xpath的使用"><a href="#xpath的使用" class="headerlink" title="xpath的使用"></a>xpath的使用</h4><p>xpath让你可以不懂前端html，不看html的详细结构，只需要会右键查看就能获取网页上任何内容。速度远超beautifulsoup。<br>目录:</p><pre><code>1. xpath简介
2. xpath术语与语法
3. xpath抓取误区：javasrcipt生成html与html源文件的区别
4. xpath抓取实例
</code></pre><p>为什么要使用xpath？</p><ul><li>xpath使用路径表达式在xml和html中进行导航</li><li>xpath包含有一个标准函数库</li><li>xpath是一个w3c的标准</li><li>xpath速度要远远超beautifulsoup。</li></ul><p><strong>xpath节点关系</strong></p><ol><li>父节点<code>*上一层节点*</code></li><li>子节点</li><li>兄弟节点<code>*同胞节点*</code></li><li>先辈节点<code>*父节点，爷爷节点*</code></li><li>后代节点<code>*儿子，孙子*</code><br>xpath语法:</li></ol><table><thead><tr><th>表达式</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td>article</td><td style="text-align:center">选取所有article元素的所有子节点</td></tr><tr><td>/article</td><td style="text-align:center">选取根元素article</td></tr><tr><td>article/a</td><td style="text-align:center">选取所有属于article的子元素的a元素</td></tr><tr><td>//div</td><td style="text-align:center">选取所有div元素（不管出现在文档里的任何地方）</td></tr><tr><td>article//div</td><td style="text-align:center">选取所有属于article元素的后代的div元素，不管它出现在article之下的任何位置</td></tr><tr><td>//@class</td><td style="text-align:center">选取所有名为class的属性</td></tr></tbody></table><p><strong>xpath语法-谓语:</strong></p><table><thead><tr><th>表达式</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td>/article/div[1</td><td style="text-align:center">选取属于article子元素的第一个div元素</td></tr><tr><td>/article/div[last()]</td><td style="text-align:center">选取属于article子元素的最后一个div元素</td></tr><tr><td>/article/div[last()-1]</td><td style="text-align:center">选取属于article子元素的倒数第二个div元素</td></tr><tr><td>//div[@color]</td><td style="text-align:center">选取所有拥有color属性的div元素</td></tr><tr><td>//div[@color=’red’]</td><td style="text-align:center">选取所有color属性值为red的div元素</td></tr></tbody></table><p><strong>xpath语法:</strong></p><table><thead><tr><th>表达式</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td>/div/*</td><td style="text-align:center">选取属于div元素的所有子节点</td></tr><tr><td>//*</td><td style="text-align:center">选取所有元素</td></tr><tr><td>//div[@*]</td><td style="text-align:center">选取所有带属性的div 元素</td></tr><tr><td>//div/a 丨//div/p</td><td style="text-align:center">选取所有div元素的a和p元素</td></tr><tr><td>//span丨//ul</td><td style="text-align:center">选取文档中的span和ul元素</td></tr><tr><td>article/div/p丨//span</td><td style="text-align:center">选取所有属于article元素的div元素的p元素以及文档中所有的 span元素</td></tr></tbody></table><p><strong>xpath抓取误区</strong></p><p><a href="https://addons.mozilla.org/en-us/firefox/addon/firebug/?src=dp-dl-dependencies" target="_blank" rel="noopener">firebugs插件</a></p><p>取某一个网页上元素的xpath地址</p><blockquote><p>如:<a href="http://blog.jobbole.com/110287/" target="_blank" rel="noopener">http://blog.jobbole.com/110287/</a></p></blockquote><p>在标题处右键使用firebugs查看元素。<br>然后在<code>&lt;h1&gt;2016 腾讯软件开发面试题（部分）&lt;/h1&gt;</code>右键查看xpath</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        re_selector = response.xpath(<span class="string">"/html/body/div[3]/div[3]/div[1]/div[1]/h1"</span>)</span><br><span class="line">        <span class="comment"># print(re_selector)</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>调试debug可以看到<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re_selector =(selectorlist)[]</span><br></pre></td></tr></table></figure><p></p><p>可以看到返回的是一个空列表，<br>列表是为了如果我们当前的xpath路径下还有层级目录时可以进行选取<br>空说明没取到值：</p><p><strong>我们可以来chorme里观察一下</strong></p><blockquote><p>chorme取到的值<br><code>//*[@id=&quot;post-110287&quot;]/div[1]/h1</code></p></blockquote><p>chormexpath代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        re_selector = response.xpath(<span class="string">'//*[@id="post-110287"]/div[1]/h1'</span>)</span><br><span class="line">        <span class="comment"># print(re_selector)</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>可以看出此时可以取到值</p><blockquote><p>分析页面，可以发现页面内有一部html是通过JavaScript ajax交互来生成的，因此在f12检查元素时的页面结构里有，而xpath不对<br>xpath是基于html源代码文件结构来找的</p></blockquote><p>xpath可以有多种多样的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">re_selector = response.xpath(<span class="string">"/html/body/div[1]/div[3]/div[1]/div[1]/h1/text()"</span>)</span><br><span class="line">re2_selector = response.xpath(<span class="string">'//*[@id="post-110287"]/div[1]/h1/text()'</span>)</span><br><span class="line">re3_selector = response.xpath(<span class="string">'//div[@class="entry-header“]/h1/text()'</span>)</span><br></pre></td></tr></table></figure><p><del>推荐使用id型。因为页面id唯一。</del></p><p>推荐使用class型，因为后期循环爬取可扩展通用性强。</p><p>通过了解了这些此时我们已经可以抓取到页面的标题，此时可以使用xpath利器照猫画虎抓取任何内容。只需要点击右键查看xpath。</p><p><strong>开启控制台调试</strong></p><p><code>scrapy shell http://blog.jobbole.com/110287/</code></p><p><strong>完整的xpath提取伯乐在线字段代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment">#提取文章的具体字段</span></span><br><span class="line">        title = response.xpath(<span class="string">'//div[@class="entry-header"]/h1/text()'</span>).extract_first(<span class="string">""</span>)</span><br><span class="line">        create_date = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/text()"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>,<span class="string">""</span>).strip()</span><br><span class="line">        praise_nums = response.xpath(<span class="string">"//span[contains(@class, 'vote-post-up')]/h10/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        fav_nums = response.xpath(<span class="string">"//span[contains(@class, 'bookmark-btn')]/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        match_re = re.match(<span class="string">".*?(\d+).*"</span>, fav_nums)</span><br><span class="line">        <span class="keyword">if</span> match_re:</span><br><span class="line">            fav_nums = match_re.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        comment_nums = response.xpath(<span class="string">"//a[@href='#article-comment']/span/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        match_re = re.match(<span class="string">".*?(\d+).*"</span>, comment_nums)</span><br><span class="line">        <span class="keyword">if</span> match_re:</span><br><span class="line">            comment_nums = match_re.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        content = response.xpath(<span class="string">"//div[@class='entry']"</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        tag_list = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/a/text()"</span>).extract()</span><br><span class="line">        tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</span><br><span class="line">        tags = <span class="string">","</span>.join(tag_list)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="css选择器的使用："><a href="#css选择器的使用：" class="headerlink" title="css选择器的使用："></a>css选择器的使用：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过css选择器提取字段</span></span><br><span class="line">        <span class="comment"># front_image_url = response.meta.get("front_image_url", "")  #文章封面图</span></span><br><span class="line">        title = response.css(<span class="string">".entry-header h1::text"</span>).extract_first()</span><br><span class="line">        create_date = response.css(<span class="string">"p.entry-meta-hide-on-mobile::text"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>,<span class="string">""</span>).strip()</span><br><span class="line">        praise_nums = response.css(<span class="string">".vote-post-up h10::text"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        match_re = re.match(<span class="string">".*?(\d+).*"</span>, fav_nums)</span><br><span class="line">        <span class="keyword">if</span> match_re:</span><br><span class="line">            fav_nums = int(match_re.group(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fav_nums = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        match_re = re.match(<span class="string">".*?(\d+).*"</span>, comment_nums)</span><br><span class="line">        <span class="keyword">if</span> match_re:</span><br><span class="line">            comment_nums = int(match_re.group(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            comment_nums = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        content = response.css(<span class="string">"div.entry"</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text"</span>).extract()</span><br><span class="line">        tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</span><br><span class="line">        tags = <span class="string">","</span>.join(tag_list)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="3-爬取所有文章"><a href="#3-爬取所有文章" class="headerlink" title="3. 爬取所有文章"></a>3. 爬取所有文章</h3><p><strong>yield关键字</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用request下载详情页面，下载完成后回调方法parse_detail()提取文章内容中的字段</span></span><br><span class="line"><span class="keyword">yield</span> Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail)</span><br></pre></td></tr></table></figure><p><strong>scrapy.http import Request下载网页</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request</span><br><span class="line">Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail)</span><br></pre></td></tr></table></figure><p><strong>parse拼接网址应对herf内有可能网址不全</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line">url=parse.urljoin(response.url,post_url)</span><br><span class="line">parse.urljoin(<span class="string">"http://blog.jobbole.com/all-posts/"</span>,<span class="string">"http://blog.jobbole.com/111535/"</span>)</span><br><span class="line"><span class="comment">#结果为http://blog.jobbole.com/111535/</span></span><br></pre></td></tr></table></figure><p><strong>class层级关系</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">next_url = response.css(<span class="string">".next.page-numbers::attr(href)"</span>).extract_first(<span class="string">""</span>)</span><br><span class="line"><span class="comment">#如果.next .pagenumber 是指两个class为层级关系。而不加空格为同一个标签</span></span><br></pre></td></tr></table></figure><p><strong>twist异步机制</strong></p><p>Scrapy使用了Twisted作为框架，Twisted有些特殊的地方是它是事件驱动的，并且比较适合异步的代码。在任何情况下，都不要写阻塞的代码。阻塞的代码包括:</p><ul><li>访问文件、数据库或者Web</li><li>产生新的进程并需要处理新进程的输出，如运行shell命令</li><li>执行系统层次操作的代码，如等待系统队列</li></ul><p><strong>实现全部文章字段下载的代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">            1. 获取文章列表页中的文章url并交给scrapy下载后并进行解析</span></span><br><span class="line"><span class="string">            2. 获取下一页的url并交给scrapy进行下载， 下载完成后交给parse</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">    <span class="comment"># 解析列表页中的所有文章url并交给scrapy下载后并进行解析</span></span><br><span class="line">    post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</span><br><span class="line">    <span class="keyword">for</span> post_url <span class="keyword">in</span> post_urls:</span><br><span class="line">        <span class="comment">#request下载完成之后，回调parse_detail进行文章详情页的解析</span></span><br><span class="line">        <span class="comment"># Request(url=post_url,callback=self.parse_detail)</span></span><br><span class="line">        print(response.url)</span><br><span class="line">        print(post_url)</span><br><span class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail)</span><br><span class="line">        <span class="comment">#遇到href没有域名的解决方案</span></span><br><span class="line">        <span class="comment">#response.url + post_url</span></span><br><span class="line">        print(post_url)</span><br><span class="line">    <span class="comment"># 提取下一页并交给scrapy进行下载</span></span><br><span class="line">    next_url = response.css(<span class="string">".next.page-numbers::attr(href)"</span>).extract_first(<span class="string">""</span>)</span><br><span class="line">    <span class="keyword">if</span> next_url:</span><br><span class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), callback=self.parse)</span><br></pre></td></tr></table></figure><p><strong>全部文章的逻辑流程图</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-d0f10707f0b49b08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="所有文章流程图"></p><h3 id="4-scrapy的items整合字段"><a href="#4-scrapy的items整合字段" class="headerlink" title="4. scrapy的items整合字段"></a>4. scrapy的items整合字段</h3><p>数据爬取的任务就是从非结构的数据中提取出结构性的数据。<br>items 可以让我们自定义自己的字段（类似于字典，但比字典的功能更齐全）</p><p><strong>在当前页，需要提取多个url</strong></p><p>原始写法,extract之后则生成list列表，无法进行二次筛选：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</span><br></pre></td></tr></table></figure><p>改进写法:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">post_nodes = response.css(<span class="string">"#archive .floated-thumb .post-thumb a"</span>)</span><br><span class="line">        <span class="keyword">for</span> post_node <span class="keyword">in</span> post_nodes:</span><br><span class="line">            <span class="comment">#获取封面图的url</span></span><br><span class="line">            image_url = post_node.css(<span class="string">"img::attr(src)"</span>).extract_first(<span class="string">""</span>)</span><br><span class="line">            post_url = post_node.css(<span class="string">"::attr(href)"</span>).extract_first(<span class="string">""</span>)</span><br></pre></td></tr></table></figure><p><strong>在下载网页的时候把获取到的封面图的url传给parse_detail的response</strong><br>在下载网页时将这个封面url获取到，并通过meta将他发送出去。在callback的回调函数中接收该值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> Request(url=parse.urljoin(response.url,post_url),meta=&#123;<span class="string">"front_image_url"</span>:image_url&#125;,callback=self.parse_detail)</span><br><span class="line"></span><br><span class="line">front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)</span><br></pre></td></tr></table></figure><p><strong>urljoin的好处</strong><br>如果你没有域名，我就从response里取出来，如果你有域名则我对你起不了作用了</p><p><strong>编写我们自定义的item并在jobboled.py中填充。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_object_id = scrapy.Field()</span><br><span class="line">    front_image_url = scrapy.Field()</span><br><span class="line">    front_image_path = scrapy.Field()</span><br><span class="line">    praise_nums = scrapy.Field()</span><br><span class="line">    comment_nums = scrapy.Field()</span><br><span class="line">    fav_nums = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field()</span><br></pre></td></tr></table></figure><p>import之后实例化，实例化之后填充：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="keyword">from</span> ArticleSpider.items <span class="keyword">import</span> JobBoleArticleItem</span><br><span class="line"><span class="number">2.</span> article_item = JobBoleArticleItem()</span><br><span class="line"><span class="number">3.</span> article_item[<span class="string">"title"</span>] = title</span><br><span class="line">        article_item[<span class="string">"url"</span>] = response.url</span><br><span class="line">        article_item[<span class="string">"create_date"</span>] = create_date</span><br><span class="line">        article_item[<span class="string">"front_image_url"</span>] = [front_image_url]</span><br><span class="line">        article_item[<span class="string">"praise_nums"</span>] = praise_nums</span><br><span class="line">        article_item[<span class="string">"comment_nums"</span>] = comment_nums</span><br><span class="line">        article_item[<span class="string">"fav_nums"</span>] = fav_nums</span><br><span class="line">        article_item[<span class="string">"tags"</span>] = tags</span><br><span class="line">        article_item[<span class="string">"content"</span>] = content</span><br></pre></td></tr></table></figure><p><strong>yield article_item将这个item传送到pipelines中</strong><br>pipelines可以接收到传送过来的item<br>将setting.py中的pipeline配置取消注释<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Configure item pipelines</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &apos;ArticleSpider.pipelines.ArticlespiderPipeline&apos;: 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>当我们的item被传输到pipeline我们可以将其进行存储到数据库等工作</p><p><strong>setting设置下载图片pipeline</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;</span><br><span class="line">&apos;scrapy.pipelines.images.ImagesPipeline&apos;: 1,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>H:\CodePath\pyEnvs\articlespider3\Lib\site-packages\scrapy\pipelines<br>里面有三个scrapy默认提供的pipeline<br>提供了文件，图片，媒体。</p><p>ITEM_PIPELINES是一个数据管道的登记表，每一项具体的数字代表它的优先级，数字越小，越早进入。</p><p><strong>setting设置下载图片的地址</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># IMAGES_MIN_HEIGHT = 100</span><br><span class="line"># IMAGES_MIN_WIDTH = 100</span><br></pre></td></tr></table></figure><p></p><p>设置下载图片的最小高度，宽度。</p><p>新建文件夹images在</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_URLS_FIELD = <span class="string">"front_image_url"</span></span><br><span class="line">project_dir = os.path.abspath(os.path.dirname(__file__))</span><br><span class="line">IMAGES_STORE = os.path.join(project_dir, <span class="string">'images'</span>)</span><br></pre></td></tr></table></figure><p><strong>安装PIL</strong><br><code>pip install pillow</code></p><p><strong>定制自己的pipeline使其下载图片后能保存下它的本地路径</strong><br>get_media_requests()接收一个迭代器对象下载图片<br>item_completed获取到图片的下载地址</p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-b46e91f577c5bf6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="自定义图片pipeline的调试信息"></p><p>继承并重写item_completed()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="comment">#重写该方法可从result中获取到图片的实际下载地址</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</span><br><span class="line">            image_file_path = value[<span class="string">"path"</span>]</span><br><span class="line">        item[<span class="string">"front_image_path"</span>] = image_file_path</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>setting中设置使用我们自定义的pipeline，而不是系统自带的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'ArticleSpider.pipelines.ArticlespiderPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="comment"># 'scrapy.pipelines.images.ImagesPipeline': 1,</span></span><br><span class="line">    <span class="string">'ArticleSpider.pipelines.ArticleImagePipeline'</span>:<span class="number">1</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/1779926-e3b33dc61bc8ab3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="保存下来的本地地址"></p><p><strong>图片url的md5处理</strong><br>新建package utils</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></span><br><span class="line">    m = hashlib.md5()</span><br><span class="line">    m.update(url)</span><br><span class="line">    <span class="keyword">return</span> m.hexdigest()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    print(get_md5(<span class="string">"http://jobbole.com"</span>.encode(<span class="string">"utf-8"</span>)))</span><br></pre></td></tr></table></figure><p>不确定用户传入的是不是:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></span><br><span class="line">	<span class="comment">#str就是unicode了</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(url, str):</span><br><span class="line">        url = url.encode(<span class="string">"utf-8"</span>)</span><br><span class="line">    m = hashlib.md5()</span><br><span class="line">    m.update(url)</span><br><span class="line">    <span class="keyword">return</span> m.hexdigest()</span><br></pre></td></tr></table></figure><p>在jobbole.py中将url的md5保存下来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ArticleSpider.utils.common <span class="keyword">import</span> get_md5</span><br><span class="line">article_item[<span class="string">"url_object_id"</span>] = get_md5(response.url)</span><br></pre></td></tr></table></figure><h3 id="5-数据保存到本地文件以及mysql中"><a href="#5-数据保存到本地文件以及mysql中" class="headerlink" title="5. 数据保存到本地文件以及mysql中"></a>5. 数据保存到本地文件以及mysql中</h3><h4 id="保存到本地json文件"><a href="#保存到本地json文件" class="headerlink" title="保存到本地json文件"></a>保存到本地json文件</h4><p><strong>import codecs打开文件避免一些编码问题，自定义JsonWithEncodingPipeline实现json本地保存</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWithEncodingPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#自定义json文件的导出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = codecs.open(<span class="string">'article.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment">#将item转换为dict，然后生成json对象，false避免中文出错</span></span><br><span class="line">        lines = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(lines)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="comment">#当spider关闭的时候</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure><p>setting.py注册pipeline</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'ArticleSpider.pipelines.JsonWithEncodingPipeline'</span>: <span class="number">2</span>,</span><br><span class="line">   <span class="comment"># 'scrapy.pipelines.images.ImagesPipeline': 1,</span></span><br><span class="line">    <span class="string">'ArticleSpider.pipelines.ArticleImagePipeline'</span>:<span class="number">1</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>scrapy exporters JsonItemExporter导出</strong></p><p>scrapy自带的导出：</p><pre><code>- &apos;CsvItemExporter&apos;, 
- &apos;XmlItemExporter&apos;,
- &apos;JsonItemExporter&apos;
</code></pre><p><code>from scrapy.exporters import JsonItemExporter</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonExporterPipleline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#调用scrapy提供的json export导出json文件</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'articleexport.json'</span>, <span class="string">'wb'</span>)</span><br><span class="line">        self.exporter = JsonItemExporter(self.file, encoding=<span class="string">"utf-8"</span>, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>设置setting.py注册该pipeline</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'ArticleSpider.pipelines.JsonExporterPipleline '</span>: <span class="number">2</span></span><br></pre></td></tr></table></figure><h4 id="保存到数据库-mysql"><a href="#保存到数据库-mysql" class="headerlink" title="保存到数据库(mysql)"></a>保存到数据库(mysql)</h4><p>数据库设计数据表，表的内容字段是和item一致的。数据库与item的关系。类似于django中model与form的关系。<br><strong>日期的转换，将字符串转换为datetime</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"> <span class="keyword">try</span>:</span><br><span class="line">            create_date = datetime.datetime.strptime(create_date, <span class="string">"%Y/%m/%d"</span>).date()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            create_date = datetime.datetime.now().date()</span><br></pre></td></tr></table></figure><p><strong>数据库表设计</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-909b623357abad96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="jobbole数据表设计"></p><ul><li>三个num字段均设置不能为空，然后默认0.</li><li>content设置为longtext</li><li>主键设置为url_object_id</li></ul><p><strong>数据库驱动安装</strong><br><code>pip install mysqlclient</code></p><p>Linux报错解决方案:<br>ubuntu:<br><code>sudo apt-get install libmysqlclient-dev</code><br>centos:<br><code>sudo yum install python-devel mysql-devel</code></p><p><strong>保存到数据库pipeline(同步）编写</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#采用同步的机制写入mysql</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.conn = MySQLdb.connect(<span class="string">'127.0.0.1'</span>, <span class="string">'root'</span>, <span class="string">'mima'</span>, <span class="string">'article_spider'</span>, charset=<span class="string">"utf8"</span>, use_unicode=<span class="keyword">True</span>)</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">            insert into jobbole_article(title, url, create_date, fav_nums)</span></span><br><span class="line"><span class="string">            VALUES (%s, %s, %s, %s)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</span><br><span class="line">        self.conn.commit()</span><br></pre></td></tr></table></figure><p><strong>保存到数据库的(异步Twisted)编写</strong><br>因为我们的爬取速度可能大于数据库存储的速度。异步操作。<br>设置可配置参数<br>seeting.py设置<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MYSQL_HOST = &quot;127.0.0.1&quot;</span><br><span class="line">MYSQL_DBNAME = &quot;article_spider&quot;</span><br><span class="line">MYSQL_USER = &quot;root&quot;</span><br><span class="line">MYSQL_PASSWORD = &quot;123456&quot;</span><br></pre></td></tr></table></figure><p></p><p>代码中获取到设置的可配置参数<br>twisted异步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> MySQLdb.cursors</span><br><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"></span><br><span class="line"><span class="comment">#连接池ConnectionPool</span></span><br><span class="line"><span class="comment">#    def __init__(self, dbapiName, *connargs, **connkw):</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dbpool)</span>:</span></span><br><span class="line">        self.dbpool = dbpool</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">        dbparms = dict(</span><br><span class="line">            host = settings[<span class="string">"MYSQL_HOST"</span>],</span><br><span class="line">            db = settings[<span class="string">"MYSQL_DBNAME"</span>],</span><br><span class="line">            user = settings[<span class="string">"MYSQL_USER"</span>],</span><br><span class="line">            passwd = settings[<span class="string">"MYSQL_PASSWORD"</span>],</span><br><span class="line">            charset=<span class="string">'utf8'</span>,</span><br><span class="line">            cursorclass=MySQLdb.cursors.DictCursor,</span><br><span class="line">            use_unicode=<span class="keyword">True</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#**dbparms--&gt;("MySQLdb",host=settings['MYSQL_HOST']</span></span><br><span class="line">        dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparms)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(dbpool)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment">#使用twisted将mysql插入变成异步执行</span></span><br><span class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</span><br><span class="line">        query.addErrback(self.handle_error, item, spider) <span class="comment">#处理异常</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure, item, spider)</span>:</span></span><br><span class="line">        <span class="comment">#处理异步插入的异常</span></span><br><span class="line">        <span class="keyword">print</span> (failure)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></span><br><span class="line">        <span class="comment">#执行具体的插入</span></span><br><span class="line">        <span class="comment">#根据不同的item 构建不同的sql语句并插入到mysql中</span></span><br><span class="line">        insert_sql, params = item.get_insert_sql()</span><br><span class="line">        cursor.execute(insert_sql, params)</span><br><span class="line"></span><br><span class="line">``` </span><br><span class="line">可选django.items</span><br><span class="line"></span><br><span class="line">https://github.com/scrapy-plugins/scrapy-djangoitem</span><br><span class="line"></span><br><span class="line">可以让我们保存的item直接变成django的models.</span><br><span class="line"></span><br><span class="line"><span class="comment">#### scrapy的itemloader来维护提取代码</span></span><br><span class="line"></span><br><span class="line">itemloadr提供了一个容器，让我们配置某一个字段该使用哪种规则。</span><br><span class="line">add_css add_value add_xpath</span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="comment"># 通过item loader加载item</span></span><br><span class="line">        front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)  <span class="comment"># 文章封面图</span></span><br><span class="line">        item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)</span><br><span class="line">        item_loader.add_css(<span class="string">"title"</span>, <span class="string">".entry-header h1::text"</span>)</span><br><span class="line">        item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line">        item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</span><br><span class="line">        item_loader.add_css(<span class="string">"create_date"</span>, <span class="string">"p.entry-meta-hide-on-mobile::text"</span>)</span><br><span class="line">        item_loader.add_value(<span class="string">"front_image_url"</span>, [front_image_url])</span><br><span class="line">        item_loader.add_css(<span class="string">"praise_nums"</span>, <span class="string">".vote-post-up h10::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"comment_nums"</span>, <span class="string">"a[href='#article-comment'] span::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"fav_nums"</span>, <span class="string">".bookmark-btn::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"tags"</span>, <span class="string">"p.entry-meta-hide-on-mobile a::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"content"</span>, <span class="string">"div.entry"</span>)</span><br><span class="line">        <span class="comment">#调用这个方法来对规则进行解析生成item对象</span></span><br><span class="line">        article_item = item_loader.load_item()</span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/1779926-d34dc1f60a28f9f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="直接使用itemloader的问题"></p><ol><li>所有值变成了list</li><li>对于这些值做一些处理函数<br><strong>item.py中对于item process处理函数</strong><br>MapCompose可以传入函数对于该字段进行处理，而且可以传入多个</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> MapCompose</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_mtianyan</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value+<span class="string">"-mtianyan"</span></span><br><span class="line"></span><br><span class="line"> title = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"mtianyan"</span>,add_mtianyan),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p><em>注意：此处的自定义方法一定要写在代码前面。</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create_date = scrapy.Field(</span><br><span class="line">    input_processor=MapCompose(date_convert),</span><br><span class="line">    output_processor=TakeFirst()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>只取list中的第一个值。</p><p><strong>自定义itemloader实现默认提取第一个</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    <span class="comment">#自定义itemloader实现默认提取第一个</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br></pre></td></tr></table></figure><p><strong>list保存原值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">return_value</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line">front_image_url = scrapy.Field(</span><br><span class="line">        output_processor=MapCompose(return_value)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p><strong>下载图片pipeline增加if增强通用性</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="comment">#重写该方法可从result中获取到图片的实际下载地址</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"front_image_url"</span> <span class="keyword">in</span> item:</span><br><span class="line">            <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</span><br><span class="line">                image_file_path = value[<span class="string">"path"</span>]</span><br><span class="line">            item[<span class="string">"front_image_path"</span>] = image_file_path</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p><strong>自定义的item带处理函数的完整代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(date_convert),</span><br><span class="line">    )</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_object_id = scrapy.Field()</span><br><span class="line">    front_image_url = scrapy.Field(</span><br><span class="line">        output_processor=MapCompose(return_value)</span><br><span class="line">    )</span><br><span class="line">    front_image_path = scrapy.Field()</span><br><span class="line">    praise_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    comment_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    fav_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#因为tag本身是list，所以要重写</span></span><br><span class="line">    tags = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_comment_tags),</span><br><span class="line">        output_processor=Join(<span class="string">","</span>)</span><br><span class="line">    )</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure><h2 id="三、知乎网问题和答案爬取"><a href="#三、知乎网问题和答案爬取" class="headerlink" title="三、知乎网问题和答案爬取"></a>三、知乎网问题和答案爬取</h2><h3 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1. 基础知识"></a>1. 基础知识</h3><h4 id="session和cookie机制"><a href="#session和cookie机制" class="headerlink" title="session和cookie机制"></a>session和cookie机制</h4><blockquote><p>cookie：<br>浏览器支持的存储方式<br>key-value</p><p>http无状态请求，两次请求没有联系</p></blockquote><p><img src="http://upload-images.jianshu.io/upload_images/1779926-f01f9c33e578427d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="50%" height="50%"></p><p>session的工作原理</p><p>（1）当一个session第一次被启用时，一个唯一的标识被存储于本地的cookie中。</p><p>（2）首先使用session_start()函数，从session仓库中加载已经存储的session变量。</p><p>（3）通过使用session_register()函数注册session变量。</p><p>（4）脚本执行结束时，未被销毁的session变量会被自动保存在本地一定路径下的session库中.</p><h4 id="request模拟知乎的登录"><a href="#request模拟知乎的登录" class="headerlink" title="request模拟知乎的登录"></a>request模拟知乎的登录</h4><p><strong>http状态码</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-e5d75b510a604f78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" width="50%" height="50%"></p><p><strong>获取crsftoken</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_xsrf</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#获取xsrf code</span></span><br><span class="line">    response = requests.get(<span class="string">"https://www.zhihu.com"</span>,headers =header)</span><br><span class="line">    <span class="comment"># # print(response.text)</span></span><br><span class="line">    <span class="comment"># text ='&lt;input type="hidden" name="_xsrf" value="ca70366e5de5d133c3ae09fb16d9b0fa"/&gt;'</span></span><br><span class="line">    match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response.text)</span><br><span class="line">    <span class="keyword">if</span> match_obj:</span><br><span class="line">        <span class="keyword">return</span> (match_obj.group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br></pre></td></tr></table></figure><p>python模拟知乎登录代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> cookielib</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">import</span> http.cookiejar <span class="keyword">as</span> cookielib</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'mtianyan'</span></span><br><span class="line">__date__ = <span class="string">'2017/5/23 16:42'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> cookielib</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">import</span> http.cookiejar <span class="keyword">as</span> cookielib</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">session = requests.session()</span><br><span class="line">session.cookies = cookielib.LWPCookieJar(filename=<span class="string">"cookies.txt"</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    session.cookies.load(ignore_discard=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"cookie未能加载"</span>)</span><br><span class="line"></span><br><span class="line">agent = <span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36"</span></span><br><span class="line">header = &#123;</span><br><span class="line">    <span class="string">"HOST"</span>:<span class="string">"www.zhihu.com"</span>,</span><br><span class="line">    <span class="string">"Referer"</span>: <span class="string">"https://www.zhizhu.com"</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: agent</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_login</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#通过个人中心页面返回状态码来判断是否为登录状态</span></span><br><span class="line">    inbox_url = <span class="string">"https://www.zhihu.com/question/56250357/answer/148534773"</span></span><br><span class="line">    response = session.get(inbox_url, headers=header, allow_redirects=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">if</span> response.status_code != <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_xsrf</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#获取xsrf code</span></span><br><span class="line">    response = session.get(<span class="string">"https://www.zhihu.com"</span>, headers=header)</span><br><span class="line">    response_text = response.text</span><br><span class="line">    <span class="comment">#reDOTAll 匹配全文</span></span><br><span class="line">    match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response_text, re.DOTALL)</span><br><span class="line">    xsrf = <span class="string">''</span></span><br><span class="line">    <span class="keyword">if</span> match_obj:</span><br><span class="line">        xsrf = (match_obj.group(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> xsrf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_index</span><span class="params">()</span>:</span></span><br><span class="line">    response = session.get(<span class="string">"https://www.zhihu.com"</span>, headers=header)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"index_page.html"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(response.text.encode(<span class="string">"utf-8"</span>))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"ok"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_captcha</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    t = str(int(time.time()*<span class="number">1000</span>))</span><br><span class="line">    captcha_url = <span class="string">"https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login"</span>.format(t)</span><br><span class="line">    t = session.get(captcha_url, headers=header)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"captcha.jpg"</span>,<span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(t.content)</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        im = Image.open(<span class="string">'captcha.jpg'</span>)</span><br><span class="line">        im.show()</span><br><span class="line">        im.close()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    captcha = input(<span class="string">"输入验证码\n&gt;"</span>)</span><br><span class="line">    <span class="keyword">return</span> captcha</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zhihu_login</span><span class="params">(account, password)</span>:</span></span><br><span class="line">    <span class="comment">#知乎登录</span></span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">"^1\d&#123;10&#125;"</span>,account):</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"手机号码登录"</span>)</span><br><span class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></span><br><span class="line">        post_data = &#123;</span><br><span class="line">            <span class="string">"_xsrf"</span>: get_xsrf(),</span><br><span class="line">            <span class="string">"phone_num"</span>: account,</span><br><span class="line">            <span class="string">"password"</span>: password,</span><br><span class="line">            <span class="string">"captcha"</span>:get_captcha()</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">"@"</span> <span class="keyword">in</span> account:</span><br><span class="line">            <span class="comment">#判断用户名是否为邮箱</span></span><br><span class="line">            print(<span class="string">"邮箱方式登录"</span>)</span><br><span class="line">            post_url = <span class="string">"https://www.zhihu.com/login/email"</span></span><br><span class="line">            post_data = &#123;</span><br><span class="line">                <span class="string">"_xsrf"</span>: get_xsrf(),</span><br><span class="line">                <span class="string">"email"</span>: account,</span><br><span class="line">                <span class="string">"password"</span>: password</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">    response_text = session.post(post_url, data=post_data, headers=header)</span><br><span class="line">    session.cookies.save()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get_index()</span></span><br><span class="line"><span class="comment"># is_login()</span></span><br><span class="line"><span class="comment"># get_captcha()</span></span><br><span class="line">zhihu_login(<span class="string">"phone"</span>, <span class="string">"mima"</span>)</span><br></pre></td></tr></table></figure><h3 id="2-scrapy创建知乎爬虫登录"><a href="#2-scrapy创建知乎爬虫登录" class="headerlink" title="2. scrapy创建知乎爬虫登录"></a>2. scrapy创建知乎爬虫登录</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider zhihu www.zhihu.com</span><br></pre></td></tr></table></figure><p>因为知乎我们需要先进行登录，所以我们重写它的start_requests</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [scrapy.Request(<span class="string">'https://www.zhihu.com/#signin'</span>, headers=self.headers, callback=self.login)]</span><br></pre></td></tr></table></figure><ol><li><p>下载首页然后回调login函数。</p></li><li><p>login函数请求验证码并回调login_after_captcha函数.此处通过meta将post_data传送出去，后面的回调函数来用。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    response_text = response.text</span><br><span class="line">    <span class="comment">#获取xsrf。</span></span><br><span class="line">    match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response_text, re.DOTALL)</span><br><span class="line">    xsrf = <span class="string">''</span></span><br><span class="line">    <span class="keyword">if</span> match_obj:</span><br><span class="line">        xsrf = (match_obj.group(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> xsrf:</span><br><span class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></span><br><span class="line">        post_data = &#123;</span><br><span class="line">            <span class="string">"_xsrf"</span>: xsrf,</span><br><span class="line">            <span class="string">"phone_num"</span>: <span class="string">"phone"</span>,</span><br><span class="line">            <span class="string">"password"</span>: <span class="string">"mima"</span>,</span><br><span class="line">            <span class="string">"captcha"</span>: <span class="string">""</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> time</span><br><span class="line">        t = str(int(time.time() * <span class="number">1000</span>))</span><br><span class="line">        captcha_url = <span class="string">"https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login"</span>.format(t)</span><br><span class="line">        <span class="comment">#请求验证码并回调login_after_captcha.</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(captcha_url, headers=self.headers, </span><br><span class="line">        	meta=&#123;<span class="string">"post_data"</span>:post_data&#125;, callback=self.login_after_captcha)</span><br></pre></td></tr></table></figure><ol><li>login_after_captcha函数将验证码图片保存到本地，然后使用PIL库打开图片，肉眼识别后在控制台输入验证码值<br>然后接受步骤一的meta数据，一并提交至登录接口。回调check_login检查是否登录成功。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_after_captcha</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"captcha.jpg"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(response.body)</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        im = Image.open(<span class="string">'captcha.jpg'</span>)</span><br><span class="line">        im.show()</span><br><span class="line">        im.close()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    captcha = input(<span class="string">"输入验证码\n&gt;"</span>)</span><br><span class="line"></span><br><span class="line">    post_data = response.meta.get(<span class="string">"post_data"</span>, &#123;&#125;)</span><br><span class="line">    post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></span><br><span class="line">    post_data[<span class="string">"captcha"</span>] = captcha</span><br><span class="line">    <span class="keyword">return</span> [scrapy.FormRequest(</span><br><span class="line">        url=post_url,</span><br><span class="line">        formdata=post_data,</span><br><span class="line">        headers=self.headers,</span><br><span class="line">        callback=self.check_login</span><br><span class="line">    )]</span><br></pre></td></tr></table></figure><ol><li>check_login函数，验证服务器的返回数据判断是否成功<br>scrapy会对request的URL去重(RFPDupeFilter)，加上dont_filter则告诉它这个URL不参与去重.</li></ol><p>源码中的startrequest:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">        <span class="keyword">yield</span> self.make_requests_from_url(url)</span><br></pre></td></tr></table></figure><p>我们将原本的start_request的代码放在了现在重写的，回调链最后的check_login</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">       <span class="comment">#验证服务器的返回数据判断是否成功</span></span><br><span class="line">       text_json = json.loads(response.text)</span><br><span class="line">       <span class="keyword">if</span> <span class="string">"msg"</span> <span class="keyword">in</span> text_json <span class="keyword">and</span> text_json[<span class="string">"msg"</span>] == <span class="string">"登录成功"</span>:</span><br><span class="line">           <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">               <span class="keyword">yield</span> scrapy.Request(url, dont_filter=<span class="keyword">True</span>, headers=self.headers)</span><br></pre></td></tr></table></figure><p><img src="http://upload-images.jianshu.io/upload_images/1779926-31ff3c83ea890269.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="登录代码流程"></p><p>###3. 知乎数据表设计<br><img src="http://upload-images.jianshu.io/upload_images/1779926-cf6b7ac1027726fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="知乎答案版本1"></p><blockquote><p>上图为知乎答案版本1</p></blockquote><p><img src="http://upload-images.jianshu.io/upload_images/1779926-e972fd3af04fc8f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="知乎答案版本2"></p><blockquote><p>上图为知乎答案版本2</p></blockquote><p><strong>设置数据表字段</strong></p><table><thead><tr><th>问题字段</th><th style="text-align:center">回答字段</th></tr></thead><tbody><tr><td>zhihu_id</td><td style="text-align:center">zhihu_id</td></tr><tr><td>topics</td><td style="text-align:center">url</td></tr><tr><td>url</td><td style="text-align:center">question_id</td></tr><tr><td>title</td><td style="text-align:center">author_id</td></tr><tr><td>content</td><td style="text-align:center">content</td></tr><tr><td>answer_num</td><td style="text-align:center">parise_num</td></tr><tr><td>comments_num</td><td style="text-align:center">comments_num</td></tr><tr><td>watch_user_num</td><td style="text-align:center">create_time</td></tr><tr><td>click_num</td><td style="text-align:center">update_time</td></tr><tr><td>crawl_time</td><td style="text-align:center">crawl_time</td></tr></tbody></table><p><img src="http://upload-images.jianshu.io/upload_images/1779926-ce3f15b285ed1e64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="知乎问题表"></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-4be3a480d6f73679.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="知乎答案表"></p><p><strong>知乎url分析</strong></p><p>点具体问题下查看更多。<br>可获得接口：</p><blockquote><p><a href="https://www.zhihu.com/api/v4/questions/25914034/answers?include=data%5B%2A%5D.is_normal%2Cis_collapsed%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=20&amp;offset=43&amp;sort_by=default" target="_blank" rel="noopener">https://www.zhihu.com/api/v4/questions/25914034/answers?include=data%5B%2A%5D.is_normal%2Cis_collapsed%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=20&amp;offset=43&amp;sort_by=default</a></p></blockquote><p><strong>重点参数：</strong><br><code>offset=43</code><br><code>isend = true</code><br><code>next</code><br><img src="http://upload-images.jianshu.io/upload_images/1779926-942806c2e9ed83cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="点击更多接口返回"></p><p><strong>href=”/question/25460323”</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_urls = [parse.urljoin(response.url, url) <span class="keyword">for</span> url <span class="keyword">in</span> all_urls]</span><br></pre></td></tr></table></figure><ol><li>从首页获取所有a标签。如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数parse_question<br>如果不是question页面则直接进一步跟踪。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">            提取出html页面中的所有url 并跟踪这些url进行一步爬取</span></span><br><span class="line"><span class="string">            如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">    all_urls = response.css(<span class="string">"a::attr(href)"</span>).extract()</span><br><span class="line">    all_urls = [parse.urljoin(response.url, url) <span class="keyword">for</span> url <span class="keyword">in</span> all_urls]</span><br><span class="line">    <span class="comment">#使用lambda函数对于每一个url进行过滤，如果是true放回列表，返回false去除。</span></span><br><span class="line">    all_urls = filter(<span class="keyword">lambda</span> x:<span class="keyword">True</span> <span class="keyword">if</span> x.startswith(<span class="string">"https"</span>) <span class="keyword">else</span> <span class="keyword">False</span>, all_urls)</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> all_urls:</span><br><span class="line">        match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, url)</span><br><span class="line">        <span class="keyword">if</span> match_obj:</span><br><span class="line">            <span class="comment"># 如果提取到question相关的页面则下载后交由提取函数进行提取</span></span><br><span class="line">            request_url = match_obj.group(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(request_url, headers=self.headers, callback=self.parse_question)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果不是question页面则直接进一步跟踪</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, headers=self.headers, callback=self.parse)</span><br></pre></td></tr></table></figure><ol><li>进入parse_question函数处理<br><strong>创建我们的item</strong></li></ol><p>item要用到的方法ArticleSpider\utils\common.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_num</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="comment">#从字符串中提取出数字</span></span><br><span class="line">    match_re = re.match(<span class="string">".*?(\d+).*"</span>, text)</span><br><span class="line">    <span class="keyword">if</span> match_re:</span><br><span class="line">        nums = int(match_re.group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nums = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><p>setting.py中设置<br><code>SQL_DATETIME_FORMAT = &quot;%Y-%m-%d %H:%M:%S&quot; SQL_DATE_FORMAT = &quot;%Y-%m-%d&quot;</code><br>使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ArticleSpider.settings <span class="keyword">import</span> SQL_DATETIME_FORMAT</span><br></pre></td></tr></table></figure><p><strong>知乎的问题 item</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuQuestionItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment">#知乎的问题 item</span></span><br><span class="line">    zhihu_id = scrapy.Field()</span><br><span class="line">    topics = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    answer_num = scrapy.Field()</span><br><span class="line">    comments_num = scrapy.Field()</span><br><span class="line">    watch_user_num = scrapy.Field()</span><br><span class="line">    click_num = scrapy.Field()</span><br><span class="line">    crawl_time = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#插入知乎question表的sql语句</span></span><br><span class="line">        insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">            insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num,</span></span><br><span class="line"><span class="string">              watch_user_num, click_num, crawl_time</span></span><br><span class="line"><span class="string">              )</span></span><br><span class="line"><span class="string">            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span></span><br><span class="line"><span class="string">            ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num),</span></span><br><span class="line"><span class="string">              watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        zhihu_id = self[<span class="string">"zhihu_id"</span>][<span class="number">0</span>]</span><br><span class="line">        topics = <span class="string">","</span>.join(self[<span class="string">"topics"</span>])</span><br><span class="line">        url = self[<span class="string">"url"</span>][<span class="number">0</span>]</span><br><span class="line">        title = <span class="string">""</span>.join(self[<span class="string">"title"</span>])</span><br><span class="line">        content = <span class="string">""</span>.join(self[<span class="string">"content"</span>])</span><br><span class="line">        answer_num = extract_num(<span class="string">""</span>.join(self[<span class="string">"answer_num"</span>]))</span><br><span class="line">        comments_num = extract_num(<span class="string">""</span>.join(self[<span class="string">"comments_num"</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(self[<span class="string">"watch_user_num"</span>]) == <span class="number">2</span>:</span><br><span class="line">            watch_user_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">0</span>])</span><br><span class="line">            click_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            watch_user_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">0</span>])</span><br><span class="line">            click_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT)</span><br><span class="line"></span><br><span class="line">        params = (zhihu_id, topics, url, title, content, answer_num, comments_num,</span><br><span class="line">                  watch_user_num, click_num, crawl_time)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> insert_sql, params</span><br></pre></td></tr></table></figure><p><strong>知乎问题回答item</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuAnswerItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment">#知乎的问题回答item</span></span><br><span class="line">    zhihu_id = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    question_id = scrapy.Field()</span><br><span class="line">    author_id = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    parise_num = scrapy.Field()</span><br><span class="line">    comments_num = scrapy.Field()</span><br><span class="line">    create_time = scrapy.Field()</span><br><span class="line">    update_time = scrapy.Field()</span><br><span class="line">    crawl_time = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#插入知乎question表的sql语句</span></span><br><span class="line">        insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">            insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, parise_num, comments_num,</span></span><br><span class="line"><span class="string">              create_time, update_time, crawl_time</span></span><br><span class="line"><span class="string">              ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span></span><br><span class="line"><span class="string">              ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), parise_num=VALUES(parise_num),</span></span><br><span class="line"><span class="string">              update_time=VALUES(update_time)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        create_time = datetime.datetime.fromtimestamp(self[<span class="string">"create_time"</span>]).strftime(SQL_DATETIME_FORMAT)</span><br><span class="line">        update_time = datetime.datetime.fromtimestamp(self[<span class="string">"update_time"</span>]).strftime(SQL_DATETIME_FORMAT)</span><br><span class="line">        params = (</span><br><span class="line">            self[<span class="string">"zhihu_id"</span>], self[<span class="string">"url"</span>], self[<span class="string">"question_id"</span>],</span><br><span class="line">            self[<span class="string">"author_id"</span>], self[<span class="string">"content"</span>], self[<span class="string">"parise_num"</span>],</span><br><span class="line">            self[<span class="string">"comments_num"</span>], create_time, update_time,</span><br><span class="line">            self[<span class="string">"crawl_time"</span>].strftime(SQL_DATETIME_FORMAT),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> insert_sql, params</span><br></pre></td></tr></table></figure><p><strong>有了两个item之后，我们继续完善我们的逻辑</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_question</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment">#处理question页面， 从页面中提取出具体的question item</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"QuestionHeader-title"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">        <span class="comment">#处理新版本</span></span><br><span class="line">        match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, response.url)</span><br><span class="line">        <span class="keyword">if</span> match_obj:</span><br><span class="line">            question_id = int(match_obj.group(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response)</span><br><span class="line">        item_loader.add_css(<span class="string">"title"</span>, <span class="string">"h1.QuestionHeader-title::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"content"</span>, <span class="string">".QuestionHeader-detail"</span>)</span><br><span class="line">        item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line">        item_loader.add_value(<span class="string">"zhihu_id"</span>, question_id)</span><br><span class="line">        item_loader.add_css(<span class="string">"answer_num"</span>, <span class="string">".List-headerText span::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"comments_num"</span>, <span class="string">".QuestionHeader-actions button::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"watch_user_num"</span>, <span class="string">".NumberBoard-value::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"topics"</span>, <span class="string">".QuestionHeader-topics .Popover div::text"</span>)</span><br><span class="line"></span><br><span class="line">        question_item = item_loader.load_item()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#处理老版本页面的item提取</span></span><br><span class="line">        match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, response.url)</span><br><span class="line">        <span class="keyword">if</span> match_obj:</span><br><span class="line">            question_id = int(match_obj.group(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response)</span><br><span class="line">        <span class="comment"># item_loader.add_css("title", ".zh-question-title h2 a::text")</span></span><br><span class="line">        item_loader.add_xpath(<span class="string">"title"</span>, <span class="string">"//*[@id='zh-question-title']/h2/a/text()|//*[@id='zh-question-title']/h2/span/text()"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"content"</span>, <span class="string">"#zh-question-detail"</span>)</span><br><span class="line">        item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line">        item_loader.add_value(<span class="string">"zhihu_id"</span>, question_id)</span><br><span class="line">        item_loader.add_css(<span class="string">"answer_num"</span>, <span class="string">"#zh-question-answer-num::text"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"comments_num"</span>, <span class="string">"#zh-question-meta-wrap a[name='addcomment']::text"</span>)</span><br><span class="line">        <span class="comment"># item_loader.add_css("watch_user_num", "#zh-question-side-header-wrap::text")</span></span><br><span class="line">        item_loader.add_xpath(<span class="string">"watch_user_num"</span>, <span class="string">"//*[@id='zh-question-side-header-wrap']/text()|//*[@class='zh-question-followers-sidebar']/div/a/strong/text()"</span>)</span><br><span class="line">        item_loader.add_css(<span class="string">"topics"</span>, <span class="string">".zm-tag-editor-labels a::text"</span>)</span><br><span class="line"></span><br><span class="line">        question_item = item_loader.load_item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(self.start_answer_url.format(question_id, <span class="number">20</span>, <span class="number">0</span>), headers=self.headers, callback=self.parse_answer)</span><br><span class="line">    <span class="keyword">yield</span> question_item</span><br></pre></td></tr></table></figure><p><strong>处理问题回答提取出需要的字段</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_answer</span><span class="params">(self, reponse)</span>:</span></span><br><span class="line">    <span class="comment">#处理question的answer</span></span><br><span class="line">    ans_json = json.loads(reponse.text)</span><br><span class="line">    is_end = ans_json[<span class="string">"paging"</span>][<span class="string">"is_end"</span>]</span><br><span class="line">    next_url = ans_json[<span class="string">"paging"</span>][<span class="string">"next"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#提取answer的具体字段</span></span><br><span class="line">    <span class="keyword">for</span> answer <span class="keyword">in</span> ans_json[<span class="string">"data"</span>]:</span><br><span class="line">        answer_item = ZhihuAnswerItem()</span><br><span class="line">        answer_item[<span class="string">"zhihu_id"</span>] = answer[<span class="string">"id"</span>]</span><br><span class="line">        answer_item[<span class="string">"url"</span>] = answer[<span class="string">"url"</span>]</span><br><span class="line">        answer_item[<span class="string">"question_id"</span>] = answer[<span class="string">"question"</span>][<span class="string">"id"</span>]</span><br><span class="line">        answer_item[<span class="string">"author_id"</span>] = answer[<span class="string">"author"</span>][<span class="string">"id"</span>] <span class="keyword">if</span> <span class="string">"id"</span> <span class="keyword">in</span> answer[<span class="string">"author"</span>] <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">        answer_item[<span class="string">"content"</span>] = answer[<span class="string">"content"</span>] <span class="keyword">if</span> <span class="string">"content"</span> <span class="keyword">in</span> answer <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">        answer_item[<span class="string">"parise_num"</span>] = answer[<span class="string">"voteup_count"</span>]</span><br><span class="line">        answer_item[<span class="string">"comments_num"</span>] = answer[<span class="string">"comment_count"</span>]</span><br><span class="line">        answer_item[<span class="string">"create_time"</span>] = answer[<span class="string">"created_time"</span>]</span><br><span class="line">        answer_item[<span class="string">"update_time"</span>] = answer[<span class="string">"updated_time"</span>]</span><br><span class="line">        answer_item[<span class="string">"crawl_time"</span>] = datetime.datetime.now()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> answer_item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_end:</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(next_url, headers=self.headers, callback=self.parse_answer)</span><br></pre></td></tr></table></figure><p><strong>知乎提取字段流程图：</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-c12492078a2369f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="知乎问题及答案提取流程图"></p><p>深度优先：</p><ol><li>提取出页面所有的url，并过滤掉不需要的url</li><li>如果是questionurl就进入question的解析</li><li>把该问题的爬取完了然后就返回初始解析</li></ol><h4 id="将item写入数据库"><a href="#将item写入数据库" class="headerlink" title="将item写入数据库"></a>将item写入数据库</h4><p><strong>pipelines.py错误处理</strong><br>插入时错误可通过该方法监控<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure, item, spider)</span>:</span></span><br><span class="line">    <span class="comment">#处理异步插入的异常</span></span><br><span class="line">    <span class="keyword">print</span> (failure)</span><br></pre></td></tr></table></figure><p></p><p><strong>改造pipeline使其变得更通用</strong><br>原本具体硬编码的pipeline</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></span><br><span class="line">      <span class="comment">#执行具体的插入</span></span><br><span class="line">      insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">                  insert into jobbole_article(title, url, create_date, fav_nums)</span></span><br><span class="line"><span class="string">                  VALUES (%s, %s, %s, %s)</span></span><br><span class="line"><span class="string">              """</span></span><br><span class="line">      cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</span><br></pre></td></tr></table></figure><p>改写后的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></span><br><span class="line"><span class="comment">#根据不同的item 构建不同的sql语句并插入到mysql中</span></span><br><span class="line">insert_sql, params = item.get_insert_sql()</span><br><span class="line">cursor.execute(insert_sql, params)</span><br></pre></td></tr></table></figure><p>可选方法一：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> item.__class__.__name__ == <span class="string">"JobBoleArticleItem"</span>:</span><br><span class="line">	<span class="comment">#执行具体的插入</span></span><br><span class="line">	insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">	            insert into jobbole_article(title, url, create_date, fav_nums)</span></span><br><span class="line"><span class="string">	            VALUES (%s, %s, %s, %s)</span></span><br><span class="line"><span class="string">	        """</span></span><br><span class="line">	cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</span><br></pre></td></tr></table></figure><p>推荐方法：<br>把sql语句等放到item里面：<br>jobboleitem类内部方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></span><br><span class="line">    insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">        insert into jobbole_article(title, url, create_date, fav_nums)</span></span><br><span class="line"><span class="string">        VALUES (%s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(fav_nums)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    params = (self[<span class="string">"title"</span>], self[<span class="string">"url"</span>], self[<span class="string">"create_date"</span>], self[<span class="string">"fav_nums"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> insert_sql, params</span><br></pre></td></tr></table></figure><p>知乎问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment">#插入知乎question表的sql语句</span></span><br><span class="line">    insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">        insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num,</span></span><br><span class="line"><span class="string">          watch_user_num, click_num, crawl_time</span></span><br><span class="line"><span class="string">          )</span></span><br><span class="line"><span class="string">        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span></span><br><span class="line"><span class="string">        ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num),</span></span><br><span class="line"><span class="string">          watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    zhihu_id = self[<span class="string">"zhihu_id"</span>][<span class="number">0</span>]</span><br><span class="line">    topics = <span class="string">","</span>.join(self[<span class="string">"topics"</span>])</span><br><span class="line">    url = self[<span class="string">"url"</span>][<span class="number">0</span>]</span><br><span class="line">    title = <span class="string">""</span>.join(self[<span class="string">"title"</span>])</span><br><span class="line">    content = <span class="string">""</span>.join(self[<span class="string">"content"</span>])</span><br><span class="line">    answer_num = extract_num(<span class="string">""</span>.join(self[<span class="string">"answer_num"</span>]))</span><br><span class="line">    comments_num = extract_num(<span class="string">""</span>.join(self[<span class="string">"comments_num"</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(self[<span class="string">"watch_user_num"</span>]) == <span class="number">2</span>:</span><br><span class="line">        watch_user_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">0</span>])</span><br><span class="line">        click_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        watch_user_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">0</span>])</span><br><span class="line">        click_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT)</span><br><span class="line"></span><br><span class="line">    params = (zhihu_id, topics, url, title, content, answer_num, comments_num,</span><br><span class="line">              watch_user_num, click_num, crawl_time)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> insert_sql, params</span><br></pre></td></tr></table></figure><p>知乎回答：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment">#插入知乎回答表的sql语句</span></span><br><span class="line">    insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">        insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, parise_num, comments_num,</span></span><br><span class="line"><span class="string">          create_time, update_time, crawl_time</span></span><br><span class="line"><span class="string">          ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span></span><br><span class="line"><span class="string">          ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), parise_num=VALUES(parise_num),</span></span><br><span class="line"><span class="string">          update_time=VALUES(update_time)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    create_time = datetime.datetime.fromtimestamp(self[<span class="string">"create_time"</span>]).strftime(SQL_DATETIME_FORMAT)</span><br><span class="line">    update_time = datetime.datetime.fromtimestamp(self[<span class="string">"update_time"</span>]).strftime(SQL_DATETIME_FORMAT)</span><br><span class="line">    params = (</span><br><span class="line">        self[<span class="string">"zhihu_id"</span>], self[<span class="string">"url"</span>], self[<span class="string">"question_id"</span>],</span><br><span class="line">        self[<span class="string">"author_id"</span>], self[<span class="string">"content"</span>], self[<span class="string">"parise_num"</span>],</span><br><span class="line">        self[<span class="string">"comments_num"</span>], create_time, update_time,</span><br><span class="line">        self[<span class="string">"crawl_time"</span>].strftime(SQL_DATETIME_FORMAT),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> insert_sql, params</span><br></pre></td></tr></table></figure><p><strong>第二次爬取到相同数据，更新数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ON DUPLICATE KEY <span class="keyword">UPDATE</span> <span class="keyword">content</span>=<span class="keyword">VALUES</span>(<span class="keyword">content</span>), answer_num=<span class="keyword">VALUES</span>(answer_num), comments_num=<span class="keyword">VALUES</span>(comments_num),</span><br><span class="line">              watch_user_num=<span class="keyword">VALUES</span>(watch_user_num), click_num=<span class="keyword">VALUES</span>(click_num)</span><br></pre></td></tr></table></figure><p><strong>调试技巧</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> match_obj:</span><br><span class="line">    <span class="comment">#如果提取到question相关的页面则下载后交由提取函数进行提取</span></span><br><span class="line">    request_url = match_obj.group(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(request_url, headers=self.headers, callback=self.parse_question)</span><br><span class="line">    <span class="comment">#方便调试</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment">#方便调试</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    <span class="comment">#如果不是question页面则直接进一步跟踪</span></span><br><span class="line">    <span class="comment">#方便调试</span></span><br><span class="line">    <span class="comment"># yield scrapy.Request(url, headers=self.headers, callback=self.parse)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方便调试</span></span><br><span class="line">    <span class="comment"># yield question_item</span></span><br></pre></td></tr></table></figure><p><strong>错误排查</strong><br>[key error] title<br>pipeline中debug定位到哪一个item的错误。</p><h2 id="四、通过CrawlSpider对招聘网站拉钩网进行整站爬取"><a href="#四、通过CrawlSpider对招聘网站拉钩网进行整站爬取" class="headerlink" title="四、通过CrawlSpider对招聘网站拉钩网进行整站爬取"></a>四、通过CrawlSpider对招聘网站拉钩网进行整站爬取</h2><p><strong>推荐工具cmder</strong><br><a href="http://cmder.net/" target="_blank" rel="noopener">http://cmder.net/</a><br>下载full版本，使我们在windows环境下也可以使用linux部分命令。<br>配置path环境变量</p><h3 id="1-设计拉勾网的数据表结构"><a href="#1-设计拉勾网的数据表结构" class="headerlink" title="1. 设计拉勾网的数据表结构"></a>1. 设计拉勾网的数据表结构</h3><p><img src="http://upload-images.jianshu.io/upload_images/1779926-c22f6fb27848ef5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="拉勾网数据库表设计"></p><h3 id="2-初始化拉钩网项目并解读crawl源码"><a href="#2-初始化拉钩网项目并解读crawl源码" class="headerlink" title="2. 初始化拉钩网项目并解读crawl源码"></a>2. 初始化拉钩网项目并解读crawl源码</h3><p><code>scrapy genspider --list</code><br>查看可使用的初始化模板<br>ailable templates:</p><ul><li>basic</li><li>crawl</li><li>csvfeed</li><li>xmlfeed</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t crawl lagou www.lagou.com</span><br></pre></td></tr></table></figure><p><strong>cmd与pycharm不同，mark root</strong><br>setting.py 设置目录<br><strong>crawl模板</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'lagou'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.lagou.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.lagou.com/'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r'Items/'</span>), callback=<span class="string">'parse_item'</span>, follow=<span class="keyword">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        <span class="comment">#i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()</span></span><br><span class="line">        <span class="comment">#i['name'] = response.xpath('//div[@id="name"]').extract()</span></span><br><span class="line">        <span class="comment">#i['description'] = response.xpath('//div[@id="description"]').extract()</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre></td></tr></table></figure><p><strong>源码阅读剖析</strong><br><a href="https://doc.scrapy.org/en/1.3/topics/spiders.html#crawlspider" target="_blank" rel="noopener">https://doc.scrapy.org/en/1.3/topics/spiders.html#crawlspider</a></p><p>提供了一些可以让我们进行简单的follow的规则，link，迭代爬取</p><p>rules：</p><blockquote><p>规则，crawel spider读取并执行</p></blockquote><p>parse_start_url(response)：</p><p>example：</p><p>rules是一个可迭代对象，里面有Rule实例-&gt;LinkExtractor的分析<br><code>allow=(&#39;category\.php&#39;, ), callback=&#39;parse_item&#39;,</code><br>allow允许的url模式。callback，要回调的函数名。<br>因为rules里面没有self，无法获取到方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># Extract links matching 'category.php' (but not matching 'subsection.php')</span></span><br><span class="line">        <span class="comment"># and follow links from them (since no callback means follow=True by default).</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Extract links matching 'item.php' and parse them with the spider's method parse_item</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'Hi, this is an item page! %s'</span>, response.url)</span><br><span class="line">        item = scrapy.Item()</span><br><span class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</span><br><span class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</span><br><span class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p><strong>分析拉勾网模板代码</strong></p><ol><li>将http加上s</li><li>重命名parse_item为我们自定义的parse_job</li><li>点击<code>class LagouSpider(CrawlSpider):</code>的CrawlSpider，进入crawl源码</li><li><code>class CrawlSpider(Spider):</code>可以看出它继承于spider</li><li><em>入口：<code>def start_requests(self):</code></em></li><li>alt+左右方向键，不同代码跳转</li><li><em>5-&gt;之后默认parse CrawlSpider里面有parse函数。但是这次我们不能向以前一样覆盖</em></li></ol><p>Crawl.py核心函数parse。</p><blockquote><p>parse函数调用_parse_response</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">       <span class="keyword">return</span> self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><code>_parse_response</code></p><ol><li>判断是否有callback即有没有self.parse_start_url</li><li>我们可以重载parse_start_url加入自己的处理</li><li>把参数传递给函数，并调用process_results函数</li></ol><p><code>_parse_response函数</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(self, response, callback, cb_kwargs, follow=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> callback:</span><br><span class="line">        cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</span><br><span class="line">        cb_res = self.process_results(response, cb_res)</span><br><span class="line">        <span class="keyword">for</span> requests_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</span><br><span class="line">            <span class="keyword">yield</span> requests_or_item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> follow <span class="keyword">and</span> self._follow_links:</span><br><span class="line">        <span class="keyword">for</span> request_or_item <span class="keyword">in</span> self._requests_to_follow(response):</span><br><span class="line">            <span class="keyword">yield</span> request_or_item</span><br></pre></td></tr></table></figure><p>parse_start_url的return值将会被process_results方法接收处理<br>如果不重写，因为返回为空，然后就相当于什么都没做</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_results</span><span class="params">(self, response, results)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><p>点击followlink</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_crawler</span><span class="params">(self, crawler)</span>:</span></span><br><span class="line">    super(CrawlSpider, self).set_crawler(crawler)</span><br><span class="line">    self._follow_links = crawler.settings.getbool(<span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>如果setting中有这个参数，则可以进一步执行到parse</p><p><code>_requests_to_follow</code></p><ol><li>判断传入的是不是response，如果不是直接returns</li><li>针对当前response设置一个空set，去重</li><li>把self的rules通过enumerate变成一个可迭代对象</li><li>跳转rules详情</li><li>拿到link通过link_extractor.extract_links抽取出具体的link</li><li>执行我们的process_links</li><li>link制作完成发起Request,回调_response_downloaded函数</li><li>然后执行parse_respose</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    seen = set()</span><br><span class="line">    <span class="keyword">for</span> n, rule <span class="keyword">in</span> enumerate(self._rules):</span><br><span class="line">        links = [lnk <span class="keyword">for</span> lnk <span class="keyword">in</span> rule.link_extractor.extract_links(response)</span><br><span class="line">                 <span class="keyword">if</span> lnk <span class="keyword">not</span> <span class="keyword">in</span> seen]</span><br><span class="line">        <span class="keyword">if</span> links <span class="keyword">and</span> rule.process_links:</span><br><span class="line">            links = rule.process_links(links)</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">            seen.add(link)</span><br><span class="line">            r = Request(url=link.url, callback=self._response_downloaded)</span><br><span class="line">            r.meta.update(rule=n, link_text=link.text)</span><br><span class="line">            <span class="keyword">yield</span> rule.process_request(r)</span><br></pre></td></tr></table></figure><p><code>_compile_rules</code></p><ol><li>在我们初始化时会调用_compile_rules</li><li><code>copy.copy(r) for r in self.rules]</code>将我们的rules进行一个copy</li><li>调用回调函数get_method。</li><li>调用rules里面我们定义的process_links</li><li>调用rules里面我们定义的process_request</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_method</span><span class="params">(method)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> callable(method):</span><br><span class="line">            <span class="keyword">return</span> method</span><br><span class="line">        <span class="keyword">elif</span> isinstance(method, six.string_types):</span><br><span class="line">            <span class="keyword">return</span> getattr(self, method, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    self._rules = [copy.copy(r) <span class="keyword">for</span> r <span class="keyword">in</span> self.rules]</span><br><span class="line">    <span class="keyword">for</span> rule <span class="keyword">in</span> self._rules:</span><br><span class="line">        rule.callback = get_method(rule.callback)</span><br><span class="line">        rule.process_links = get_method(rule.process_links)</span><br><span class="line">        rule.process_request = get_method(rule.process_request)</span><br></pre></td></tr></table></figure><pre><code>self.process_links = process_links
self.process_request = process_request
</code></pre><p>可以通过在rules里面传入我们自己的处理函数，实现对url的自定义。<br>达到负载均衡，多地不同ip访问。</p><p><code>_response_downloaded</code><br>通过rule取到具体的rule<br>调用我们自己的回调函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_response_downloaded</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    rule = self._rules[response.meta[<span class="string">'rule'</span>]]</span><br><span class="line">    <span class="keyword">return</span> self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)</span><br></pre></td></tr></table></figure><ul><li>allow ：符合这个url我就爬取</li><li>deny : 符合这个url规则我就放弃</li><li>allow_domin : 这个域名下的我才处理</li><li>allow_domin : 这个域名下的我不处理</li><li>restrict_xpaths：进一步限定xpath</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),</span><br><span class="line">                 tags=(<span class="string">'a'</span>, <span class="string">'area'</span>), attrs=(<span class="string">'href'</span>,), canonicalize=<span class="keyword">True</span>,</span><br><span class="line">                 unique=<span class="keyword">True</span>, process_value=<span class="keyword">None</span>, deny_extensions=<span class="keyword">None</span>, restrict_css=()</span><br></pre></td></tr></table></figure><p>extract_links<br>如果有restrict_xpaths，他会进行读取执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_links</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    base_url = get_base_url(response)</span><br><span class="line">    <span class="keyword">if</span> self.restrict_xpaths:</span><br><span class="line">        docs = [subdoc</span><br><span class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> self.restrict_xpaths</span><br><span class="line">                <span class="keyword">for</span> subdoc <span class="keyword">in</span> response.xpath(x)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        docs = [response.selector]</span><br><span class="line">    all_links = []</span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">        links = self._extract_links(doc, response.url, response.encoding, base_url)</span><br><span class="line">        all_links.extend(self._process_links(links))</span><br><span class="line">    <span class="keyword">return</span> unique_list(all_links)</span><br></pre></td></tr></table></figure><p>get_base_url:</p><p>urllib.parse.urljoin替我们拼接好url</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_base_url</span><span class="params">(text, baseurl=<span class="string">''</span>, encoding=<span class="string">'utf-8'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Return the base url if declared in the given HTML `text`,</span></span><br><span class="line"><span class="string">    relative to the given base url.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If no base url is found, the given `baseurl` is returned.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    text = to_unicode(text, encoding)</span><br><span class="line">    m = _baseurl_re.search(text)</span><br><span class="line">    <span class="keyword">if</span> m:</span><br><span class="line">        <span class="keyword">return</span> moves.urllib.parse.urljoin(</span><br><span class="line">            safe_url_string(baseurl),</span><br><span class="line">            safe_url_string(m.group(<span class="number">1</span>), encoding=encoding)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> safe_url_string(baseurl)</span><br></pre></td></tr></table></figure><h4 id="编写rule规则"><a href="#编写rule规则" class="headerlink" title="编写rule规则"></a>编写rule规则</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">    Rule(LinkExtractor(allow=(<span class="string">"zhaopin/.*"</span>,)), follow=<span class="keyword">True</span>),</span><br><span class="line">    Rule(LinkExtractor(allow=(<span class="string">"gongsi/j\d+.html"</span>,)), follow=<span class="keyword">True</span>),</span><br><span class="line">    Rule(LinkExtractor(allow=<span class="string">r'jobs/\d+.html'</span>), callback=<span class="string">'parse_job'</span>, follow=<span class="keyword">True</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="3-设计lagou的items"><a href="#3-设计lagou的items" class="headerlink" title="3. 设计lagou的items"></a>3. 设计lagou的items</h3><p><strong>需要用到的方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> w3lib.html <span class="keyword">import</span> remove_tags</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_splash</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="comment">#去掉工作城市的斜线</span></span><br><span class="line">    <span class="keyword">return</span> value.replace(<span class="string">"/"</span>,<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_jobaddr</span><span class="params">(value)</span>:</span></span><br><span class="line">    addr_list = value.split(<span class="string">"\n"</span>)</span><br><span class="line">    addr_list = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> addr_list <span class="keyword">if</span> item.strip()!=<span class="string">"查看地图"</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">""</span>.join(addr_list)</span><br></pre></td></tr></table></figure><p><strong>定义好的item</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouJobItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment">#拉勾网职位信息</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_object_id = scrapy.Field()</span><br><span class="line">    salary = scrapy.Field()</span><br><span class="line">    job_city = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_splash),</span><br><span class="line">    )</span><br><span class="line">    work_years = scrapy.Field(</span><br><span class="line">        input_processor = MapCompose(remove_splash),</span><br><span class="line">    )</span><br><span class="line">    degree_need = scrapy.Field(</span><br><span class="line">        input_processor = MapCompose(remove_splash),</span><br><span class="line">    )</span><br><span class="line">    job_type = scrapy.Field()</span><br><span class="line">    publish_time = scrapy.Field()</span><br><span class="line">    job_advantage = scrapy.Field()</span><br><span class="line">    job_desc = scrapy.Field()</span><br><span class="line">    job_addr = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags, handle_jobaddr),</span><br><span class="line">    )</span><br><span class="line">    company_name = scrapy.Field()</span><br><span class="line">    company_url = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field(</span><br><span class="line">        input_processor = Join(<span class="string">","</span>)</span><br><span class="line">    )</span><br><span class="line">    crawl_time = scrapy.Field()</span><br></pre></td></tr></table></figure><p></p><p><strong>重写的itemloader</strong><br>设置默认只提取第一个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouJobItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    <span class="comment">#自定义itemloader</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br></pre></td></tr></table></figure><h4 id="4-提取字段值并存入数据库"><a href="#4-提取字段值并存入数据库" class="headerlink" title="4. 提取字段值并存入数据库"></a>4. 提取字段值并存入数据库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_job</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#解析拉勾网的职位</span></span><br><span class="line">    item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response)</span><br><span class="line">    item_loader.add_css(<span class="string">"title"</span>, <span class="string">".job-name::attr(title)"</span>)</span><br><span class="line">    item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line">    item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</span><br><span class="line">    item_loader.add_css(<span class="string">"salary"</span>, <span class="string">".job_request .salary::text"</span>)</span><br><span class="line">    item_loader.add_xpath(<span class="string">"job_city"</span>, <span class="string">"//*[@class='job_request']/p/span[2]/text()"</span>)</span><br><span class="line">    item_loader.add_xpath(<span class="string">"work_years"</span>, <span class="string">"//*[@class='job_request']/p/span[3]/text()"</span>)</span><br><span class="line">    item_loader.add_xpath(<span class="string">"degree_need"</span>, <span class="string">"//*[@class='job_request']/p/span[4]/text()"</span>)</span><br><span class="line">    item_loader.add_xpath(<span class="string">"job_type"</span>, <span class="string">"//*[@class='job_request']/p/span[5]/text()"</span>)</span><br><span class="line"></span><br><span class="line">    item_loader.add_css(<span class="string">"tags"</span>, <span class="string">'.position-label li::text'</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"publish_time"</span>, <span class="string">".publish_time::text"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"job_advantage"</span>, <span class="string">".job-advantage p::text"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"job_desc"</span>, <span class="string">".job_bt div"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"job_addr"</span>, <span class="string">".work_addr"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"company_name"</span>, <span class="string">"#job_company dt a img::attr(alt)"</span>)</span><br><span class="line">    item_loader.add_css(<span class="string">"company_url"</span>, <span class="string">"#job_company dt a::attr(href)"</span>)</span><br><span class="line">    item_loader.add_value(<span class="string">"crawl_time"</span>, datetime.now())</span><br><span class="line"></span><br><span class="line">    job_item = item_loader.load_item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> job_item</span><br></pre></td></tr></table></figure><p><strong>获得的拉勾网item数据</strong><br><img src="http://upload-images.jianshu.io/upload_images/1779926-1247e16b04708ea3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="拉勾网item数据"></p><h4 id="5-items中添加get-insert-sql实现存入数据库"><a href="#5-items中添加get-insert-sql实现存入数据库" class="headerlink" title="5. items中添加get_insert_sql实现存入数据库"></a>5. items中添加get_insert_sql实现存入数据库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></span><br><span class="line">       insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">           insert into lagou_job(title, url, url_object_id, salary, job_city, work_years, degree_need,</span></span><br><span class="line"><span class="string">           job_type, publish_time, job_advantage, job_desc, job_addr, company_name, company_url,</span></span><br><span class="line"><span class="string">           tags, crawl_time) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span></span><br><span class="line"><span class="string">           ON DUPLICATE KEY UPDATE salary=VALUES(salary), job_desc=VALUES(job_desc)</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">       params = (</span><br><span class="line">           self[<span class="string">"title"</span>], self[<span class="string">"url"</span>], self[<span class="string">"url_object_id"</span>], self[<span class="string">"salary"</span>], self[<span class="string">"job_city"</span>],</span><br><span class="line">           self[<span class="string">"work_years"</span>], self[<span class="string">"degree_need"</span>], self[<span class="string">"job_type"</span>],</span><br><span class="line">           self[<span class="string">"publish_time"</span>], self[<span class="string">"job_advantage"</span>], self[<span class="string">"job_desc"</span>],</span><br><span class="line">           self[<span class="string">"job_addr"</span>], self[<span class="string">"company_name"</span>], self[<span class="string">"company_url"</span>],</span><br><span class="line">           self[<span class="string">"job_addr"</span>], self[<span class="string">"crawl_time"</span>].strftime(SQL_DATETIME_FORMAT),</span><br><span class="line">       )</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> insert_sql, params</span><br></pre></td></tr></table></figure><h2 id="五、爬虫与反爬虫"><a href="#五、爬虫与反爬虫" class="headerlink" title="五、爬虫与反爬虫"></a>五、爬虫与反爬虫</h2><h3 id="1-基础知识-1"><a href="#1-基础知识-1" class="headerlink" title="1. 基础知识"></a>1. 基础知识</h3><p>如何使我们的爬虫不被禁止掉</p><p>爬虫：</p><blockquote><p>自动获取数据的程序，关键是批量的获取</p></blockquote><p>反爬虫：</p><blockquote><p>使用技术手段防止爬虫程序的方法</p></blockquote><p>误伤：</p><blockquote><p>反爬虫技术将普通用户识别为爬虫，效果再好也不能用</p></blockquote><p>学校，网吧，出口的公网ip只有一个，所以禁止ip不能用。</p><p>ip动态分配。a爬封b</p><p>成本：</p><blockquote><p>反爬虫人力和机器成本</p></blockquote><p>拦截：</p><blockquote><p>拦截率越高，误伤率越高</p></blockquote><p>反爬虫的目的：</p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-d48e923ecb3d20ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="反爬虫的目的"></p><p>爬虫与反爬虫的对抗过程：</p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-77314219c9f1757e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="爬虫与反爬虫斗争"></p><p>使用检查可以查看到价格，而查看网页源代码无法查看到价格字段。<br>scrapy下载到的网页时网页源代码。<br>js（ajax）填充的动态数据无法通过网页获取到。</p><h3 id="2-scrapy架构及源码介绍"><a href="#2-scrapy架构及源码介绍" class="headerlink" title="2. scrapy架构及源码介绍"></a>2. scrapy架构及源码介绍</h3><p><img src="http://upload-images.jianshu.io/upload_images/1779926-8381473a93549bc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="scrapy组件分析图"></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-8f93d40d66c9fe04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="scrapy官方架构图"></p><ol><li>我们编写的spider，然后yield一个request发送给engine</li><li>engine拿到什么都不做然后给scheduler</li><li>engine会生成一个request给engine</li><li>engine拿到之后通过downloadermiddleware 给downloader</li><li>downloader再发送response回来给engine。</li><li>engine拿到之后，response给spider。</li><li>spider进行处理，解析出item &amp; request，</li><li>item-&gt;给itempipeline；如果是request，跳转步骤二</li></ol><p>path：articlespider3\Lib\site-packages\scrapy\core</p><ul><li>engine.py：</li><li>scheduler.py</li><li><p>downloader</p></li><li><p>item</p></li><li>pipeline</li><li>spider</li></ul><p><strong>engine.py：重要函数schedule</strong></p><ol><li>enqueue_request：把request放scheduler</li><li><code>_next_request_from_scheduler</code>:从调度器拿。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">schedule</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">    self.signals.send_catch_log(signal=signals.request_scheduled,</span><br><span class="line">            request=request, spider=spider)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.slot.scheduler.enqueue_request(request):</span><br><span class="line">        self.signals.send_catch_log(signal=signals.request_dropped,</span><br><span class="line">                                    request=request, spider=spider)</span><br></pre></td></tr></table></figure><p>articlespider3\Lib\site-packages\scrapy\core\downloader\handlers</p><blockquote><p>支持文件，ftp，http下载(https).</p></blockquote><p>后期定制middleware：</p><ul><li>spidermiddlewire</li><li>downloadmiddlewire</li></ul><p>django和scrapy结构类似</p><h3 id="3-scrapy的两个重要类：request和response"><a href="#3-scrapy的两个重要类：request和response" class="headerlink" title="3. scrapy的两个重要类：request和response"></a>3. scrapy的两个重要类：request和response</h3><p>类似于django httprequest</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url))</span><br></pre></td></tr></table></figure><p>request参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Request</span><span class="params">(object_ref)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url, callback=None, method=<span class="string">'GET'</span>, headers=None, body=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cookies=None, meta=None, encoding=<span class="string">'utf-8'</span>, priority=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dont_filter=False, errback=None)</span>:</span></span><br></pre></td></tr></table></figure><p>cookies：<br>Lib\site-packages\scrapy\downloadermiddlewares\cookies.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cookiejarkey = request.meta.get(<span class="string">"cookiejar"</span>)</span><br></pre></td></tr></table></figure><ul><li>priority: 优先级，影响调度顺序</li><li>dont_filter：我的同样的request不会被过滤</li><li>errback：错误时的回调函数</li></ul><p><a href="https://doc.scrapy.org/en/1.2/topics/request-response.html?highlight=response" target="_blank" rel="noopener">https://doc.scrapy.org/en/1.2/topics/request-response.html?highlight=response</a></p><p><strong>errback example：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ErrbackSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"errback_example"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.httpbin.org/"</span>,              <span class="comment"># HTTP 200 expected</span></span><br><span class="line">        <span class="string">"http://www.httpbin.org/status/404"</span>,    <span class="comment"># Not found error</span></span><br><span class="line">        <span class="string">"http://www.httpbin.org/status/500"</span>,    <span class="comment"># server issue</span></span><br><span class="line">        <span class="string">"http://www.httpbin.org:12345/"</span>,        <span class="comment"># non-responding host, timeout expected</span></span><br><span class="line">        <span class="string">"http://www.httphttpbinbin.org/"</span>,       <span class="comment"># DNS error expected</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(u, callback=self.parse_httpbin,</span><br><span class="line">                                    errback=self.errback_httpbin,</span><br><span class="line">                                    dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_httpbin</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'Got successful response from &#123;&#125;'</span>.format(response.url))</span><br><span class="line">        <span class="comment"># do something useful here...</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">errback_httpbin</span><span class="params">(self, failure)</span>:</span></span><br><span class="line">        <span class="comment"># log all failures</span></span><br><span class="line">        self.logger.error(repr(failure))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># in case you want to do something special for some errors,</span></span><br><span class="line">        <span class="comment"># you may need the failure's type:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> failure.check(HttpError):</span><br><span class="line">            <span class="comment"># these exceptions come from HttpError spider middleware</span></span><br><span class="line">            <span class="comment"># you can get the non-200 response</span></span><br><span class="line">            response = failure.value.response</span><br><span class="line">            self.logger.error(<span class="string">'HttpError on %s'</span>, response.url)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> failure.check(DNSLookupError):</span><br><span class="line">            <span class="comment"># this is the original request</span></span><br><span class="line">            request = failure.request</span><br><span class="line">            self.logger.error(<span class="string">'DNSLookupError on %s'</span>, request.url)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> failure.check(TimeoutError, TCPTimedOutError):</span><br><span class="line">            request = failure.request</span><br><span class="line">            self.logger.error(<span class="string">'TimeoutError on %s'</span>, request.url)</span><br></pre></td></tr></table></figure><p><strong>response类</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url, status=<span class="number">200</span>, headers=None, body=<span class="string">b''</span>, flags=None, request=None)</span>:</span></span><br><span class="line">       self.headers = Headers(headers <span class="keyword">or</span> &#123;&#125;)</span><br></pre></td></tr></table></figure><p><strong>response的参数：</strong><br>request：yield出来的request，会放在response，让我们知道它是从哪里来的</p><h3 id="4-自行编写随机更换useagent"><a href="#4-自行编写随机更换useagent" class="headerlink" title="4. 自行编写随机更换useagent"></a>4. 自行编写随机更换useagent</h3><ol><li>setting中设置</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user_agent_list = [</span><br><span class="line">    <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36'</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>然后在代码中使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> settings <span class="keyword">import</span> user_agent_list</span><br><span class="line">   <span class="keyword">import</span> random</span><br><span class="line">   random_index =random.randint(<span class="number">0</span>,len(user_agent_list))</span><br><span class="line">   random_agent = user_agent_list[random_index]</span><br><span class="line"></span><br><span class="line">   <span class="string">'User-Agent'</span>: random_agent</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random_index = random.randint(<span class="number">0</span>, len(user_agent_list))</span><br><span class="line">random_agent = user_agent_list[random_index]</span><br><span class="line">self.headers[<span class="string">"User-Agent"</span>] = random_agent</span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(request_url, headers=self.headers, callback=self.parse_question)</span><br></pre></td></tr></table></figure><p>但是问题：每个request之前都得这样做。</p><h3 id="5-middlewire配置及编写fake-UseAgent代理池"><a href="#5-middlewire配置及编写fake-UseAgent代理池" class="headerlink" title="5. middlewire配置及编写fake UseAgent代理池"></a>5. middlewire配置及编写fake UseAgent代理池</h3><p>取消DOWNLOADER_MIDDLEWARES的注释状态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">'ArticleSpider.middlewares.MyCustomDownloaderMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>articlespider3\Lib\site-packages\scrapy\downloadermiddlewares\useragent.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserAgentMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""This middleware allows spiders to override the user_agent"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, user_agent=<span class="string">'Scrapy'</span>)</span>:</span></span><br><span class="line">        self.user_agent = user_agent</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        o = cls(crawler.settings[<span class="string">'USER_AGENT'</span>])</span><br><span class="line">        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> o</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.user_agent = getattr(spider, <span class="string">'user_agent'</span>, self.user_agent)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.user_agent:</span><br><span class="line">            request.headers.setdefault(<span class="string">b'User-Agent'</span>, self.user_agent)</span><br></pre></td></tr></table></figure><p>重要方法process_request</p><p><strong>配置默认useagent为none</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">'ArticleSpider.middlewares.MyCustomDownloaderMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>使用fakeuseragent</strong><br><code>pip install fake-useragent</code></p><p>settinf.py设置随机模式<code>RANDOM_UA_TYPE = &quot;random&quot;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddlware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#随机更换user-agent</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, crawler)</span>:</span></span><br><span class="line">        super(RandomUserAgentMiddlware, self).__init__()</span><br><span class="line">        self.ua = UserAgent()</span><br><span class="line">        self.ua_type = crawler.settings.get(<span class="string">"RANDOM_UA_TYPE"</span>, <span class="string">"random"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(crawler)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_ua</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">return</span> getattr(self.ua, self.ua_type)</span><br><span class="line"></span><br><span class="line">        request.headers.setdefault(<span class="string">'User-Agent'</span>, get_ua())</span><br></pre></td></tr></table></figure><h3 id="6-使用西刺代理创建ip代理池保存到数据库"><a href="#6-使用西刺代理创建ip代理池保存到数据库" class="headerlink" title="6. 使用西刺代理创建ip代理池保存到数据库*"></a>6. 使用西刺代理创建ip代理池保存到数据库*</h3><p>ip动态变化：重启路由器等</p><p>ip代理的原理：</p><p>不直接发送自己真实ip，而使用中间代理商（代理服务器），那么服务器不知道我们的ip也就不会把我们禁掉<br>setting.py设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#动态设置ip代理</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.meta[<span class="string">"proxy"</span>] = <span class="string">"http://111.198.219.151:8118"</span></span><br></pre></td></tr></table></figure><p><strong>使用西刺代理创建代理池保存到数据库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></span><br><span class="line">__author__ = <span class="string">'mtianyan'</span></span><br><span class="line">__date__ = <span class="string">'2017/5/24 16:27'</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"></span><br><span class="line">conn = MySQLdb.connect(host=<span class="string">"127.0.0.1"</span>, user=<span class="string">"root"</span>, passwd=<span class="string">"mima"</span>, db=<span class="string">"article_spider"</span>, charset=<span class="string">"utf8"</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl_ips</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#爬取西刺的免费ip代理</span></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0"</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1568</span>):</span><br><span class="line">        re = requests.get(<span class="string">"http://www.xicidaili.com/nn/&#123;0&#125;"</span>.format(i), headers=headers)</span><br><span class="line"></span><br><span class="line">        selector = Selector(text=re.text)</span><br><span class="line">        all_trs = selector.css(<span class="string">"#ip_list tr"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ip_list = []</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> all_trs[<span class="number">1</span>:]:</span><br><span class="line">            speed_str = tr.css(<span class="string">".bar::attr(title)"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> speed_str:</span><br><span class="line">                speed = float(speed_str.split(<span class="string">"秒"</span>)[<span class="number">0</span>])</span><br><span class="line">            all_texts = tr.css(<span class="string">"td::text"</span>).extract()</span><br><span class="line"></span><br><span class="line">            ip = all_texts[<span class="number">0</span>]</span><br><span class="line">            port = all_texts[<span class="number">1</span>]</span><br><span class="line">            proxy_type = all_texts[<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">            ip_list.append((ip, port, proxy_type, speed))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ip_info <span class="keyword">in</span> ip_list:</span><br><span class="line">            cursor.execute(</span><br><span class="line">                <span class="string">"insert proxy_ip(ip, port, speed, proxy_type) VALUES('&#123;0&#125;', '&#123;1&#125;', &#123;2&#125;, 'HTTP')"</span>.format(</span><br><span class="line">                    ip_info[<span class="number">0</span>], ip_info[<span class="number">1</span>], ip_info[<span class="number">3</span>]</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            conn.commit()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GetIP</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delete_ip</span><span class="params">(self, ip)</span>:</span></span><br><span class="line">        <span class="comment">#从数据库中删除无效的ip</span></span><br><span class="line">        delete_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">            delete from proxy_ip where ip='&#123;0&#125;'</span></span><br><span class="line"><span class="string">        """</span>.format(ip)</span><br><span class="line">        cursor.execute(delete_sql)</span><br><span class="line">        conn.commit()</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">judge_ip</span><span class="params">(self, ip, port)</span>:</span></span><br><span class="line">        <span class="comment">#判断ip是否可用</span></span><br><span class="line">        http_url = <span class="string">"http://www.baidu.com"</span></span><br><span class="line">        proxy_url = <span class="string">"http://&#123;0&#125;:&#123;1&#125;"</span>.format(ip, port)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            proxy_dict = &#123;</span><br><span class="line">                <span class="string">"http"</span>:proxy_url,</span><br><span class="line">            &#125;</span><br><span class="line">            response = requests.get(http_url, proxies=proxy_dict)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"invalid ip and port"</span>)</span><br><span class="line">            self.delete_ip(ip)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            code = response.status_code</span><br><span class="line">            <span class="keyword">if</span> code &gt;= <span class="number">200</span> <span class="keyword">and</span> code &lt; <span class="number">300</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"effective ip"</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span>  (<span class="string">"invalid ip and port"</span>)</span><br><span class="line">                self.delete_ip(ip)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_random_ip</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#从数据库中随机获取一个可用的ip</span></span><br><span class="line">        random_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">              SELECT ip, port FROM proxy_ip</span></span><br><span class="line"><span class="string">            ORDER BY RAND()</span></span><br><span class="line"><span class="string">            LIMIT 1</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">        result = cursor.execute(random_sql)</span><br><span class="line">        <span class="keyword">for</span> ip_info <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line">            ip = ip_info[<span class="number">0</span>]</span><br><span class="line">            port = ip_info[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            judge_re = self.judge_ip(ip, port)</span><br><span class="line">            <span class="keyword">if</span> judge_re:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"http://&#123;0&#125;:&#123;1&#125;"</span>.format(ip, port)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> self.get_random_ip()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># print (crawl_ips())</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    get_ip = GetIP()</span><br><span class="line">    get_ip.get_random_ip()</span><br></pre></td></tr></table></figure><p><strong>使用scrapy_proxies创建ip代理池</strong></p><p><code>pip install scrapy_proxies</code></p><p>收费，但是简单<br><a href="https://github.com/scrapy-plugins/scrapy-crawlera" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-crawlera</a></p><p>tor隐藏。vpn<br><a href="http://www.theonionrouter.com/" target="_blank" rel="noopener">http://www.theonionrouter.com/</a></p><h3 id="7-通过云打码实现验证码的识别"><a href="#7-通过云打码实现验证码的识别" class="headerlink" title="7. 通过云打码实现验证码的识别"></a>7. 通过云打码实现验证码的识别</h3><p><a href="http://www.yundama.com/" target="_blank" rel="noopener">http://www.yundama.com/</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></span><br><span class="line">__author__ = <span class="string">'mtianyan'</span></span><br><span class="line">__date__ = <span class="string">'2017/6/24 16:48'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YDMHttp</span><span class="params">(object)</span>:</span></span><br><span class="line">    apiurl = <span class="string">'http://api.yundama.com/api.php'</span></span><br><span class="line">    username = <span class="string">''</span></span><br><span class="line">    password = <span class="string">''</span></span><br><span class="line">    appid = <span class="string">''</span></span><br><span class="line">    appkey = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, username, password, appid, appkey)</span>:</span></span><br><span class="line">        self.username = username</span><br><span class="line">        self.password = password</span><br><span class="line">        self.appid = str(appid)</span><br><span class="line">        self.appkey = appkey</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">balance</span><span class="params">(self)</span>:</span></span><br><span class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'balance'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid, <span class="string">'appkey'</span>: self.appkey&#125;</span><br><span class="line">        response_data = requests.post(self.apiurl, data=data)</span><br><span class="line">        ret_data = json.loads(response_data.text)</span><br><span class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"获取剩余积分"</span>, ret_data[<span class="string">"balance"</span>])</span><br><span class="line">            <span class="keyword">return</span> ret_data[<span class="string">"balance"</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self)</span>:</span></span><br><span class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'login'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid, <span class="string">'appkey'</span>: self.appkey&#125;</span><br><span class="line">        response_data = requests.post(self.apiurl, data=data)</span><br><span class="line">        ret_data = json.loads(response_data.text)</span><br><span class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"登录成功"</span>, ret_data[<span class="string">"uid"</span>])</span><br><span class="line">            <span class="keyword">return</span> ret_data[<span class="string">"uid"</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, filename, codetype, timeout)</span>:</span></span><br><span class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'upload'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid, <span class="string">'appkey'</span>: self.appkey, <span class="string">'codetype'</span>: str(codetype), <span class="string">'timeout'</span>: str(timeout)&#125;</span><br><span class="line">        files = &#123;<span class="string">'file'</span>: open(filename, <span class="string">'rb'</span>)&#125;</span><br><span class="line">        response_data = requests.post(self.apiurl, files=files, data=data)</span><br><span class="line">        ret_data = json.loads(response_data.text)</span><br><span class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"识别成功"</span>, ret_data[<span class="string">"text"</span>])</span><br><span class="line">            <span class="keyword">return</span> ret_data[<span class="string">"text"</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ydm</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    username = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 密码</span></span><br><span class="line">    password = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appid = </span><br><span class="line">    <span class="comment"># 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appkey = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 图片文件</span></span><br><span class="line">    filename = <span class="string">'image/1.jpg'</span></span><br><span class="line">    <span class="comment"># 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html</span></span><br><span class="line">    codetype = <span class="number">5000</span></span><br><span class="line">    <span class="comment"># 超时时间，秒</span></span><br><span class="line">    timeout = <span class="number">60</span></span><br><span class="line">    <span class="comment"># 检查</span></span><br><span class="line"></span><br><span class="line">    yundama = YDMHttp(username, password, appid, appkey)</span><br><span class="line">    <span class="keyword">if</span> (username == <span class="string">'username'</span>):</span><br><span class="line">        print(<span class="string">'请设置好相关参数再测试'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果</span></span><br><span class="line">        <span class="keyword">return</span> yundama.decode(file_path, codetype, timeout);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 用户名</span></span><br><span class="line">    username = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 密码</span></span><br><span class="line">    password = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appid = </span><br><span class="line">    <span class="comment"># 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appkey = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 图片文件</span></span><br><span class="line">    filename = <span class="string">'image/captcha.jpg'</span></span><br><span class="line">    <span class="comment"># 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html</span></span><br><span class="line">    codetype = <span class="number">5000</span></span><br><span class="line">    <span class="comment"># 超时时间，秒</span></span><br><span class="line">    timeout = <span class="number">60</span></span><br><span class="line">    <span class="comment"># 检查</span></span><br><span class="line">    <span class="keyword">if</span> (username == <span class="string">'username'</span>):</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'请设置好相关参数再测试'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        yundama = YDMHttp(username, password, appid, appkey)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 登陆云打码</span></span><br><span class="line">        uid = yundama.login();</span><br><span class="line">        print(<span class="string">'uid: %s'</span> % uid)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 登陆云打码</span></span><br><span class="line">        uid = yundama.login();</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'uid: %s'</span> % uid)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查询余额</span></span><br><span class="line">        balance = yundama.balance();</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'balance: %s'</span> % balance)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果</span></span><br><span class="line">        text = yundama.decode(filename, codetype, timeout);</span><br></pre></td></tr></table></figure><h3 id="8-cookie的禁用。-amp-设置下载速度"><a href="#8-cookie的禁用。-amp-设置下载速度" class="headerlink" title="8. cookie的禁用。&amp; 设置下载速度"></a>8. cookie的禁用。&amp; 设置下载速度</h3><p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html</a></p><p>setting.py:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line">COOKIES_ENABLED = <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>设置下载速度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The initial download delay</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_START_DELAY = 5</span></span><br></pre></td></tr></table></figure><p>给不同的spider设置自己的setting值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">    <span class="string">"COOKIES_ENABLED"</span>: <span class="keyword">True</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="六、scrapy进阶开发"><a href="#六、scrapy进阶开发" class="headerlink" title="六、scrapy进阶开发"></a>六、scrapy进阶开发</h2><h3 id="1-Selenium动态页面抓取"><a href="#1-Selenium动态页面抓取" class="headerlink" title="1. Selenium动态页面抓取"></a>1. Selenium动态页面抓取</h3><p>Selenium （浏览器自动化测试框架)<br>Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本</p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-b3de4a107a839138.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Selenium架构图"><br>安装<br><code>pip install selenium</code></p><p>文档地址：<br><a href="http://selenium-python.readthedocs.io/api.html" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html</a><br>安装webdriver.exe</p><h4 id="天猫价格获取"><a href="#天猫价格获取" class="headerlink" title="天猫价格获取"></a>天猫价格获取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome(executable_path=<span class="string">"C:/chromedriver.exe"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#天猫价格获取</span></span><br><span class="line">browser.get(<span class="string">"https://detail.tmall.com/item.htm?spm=a230r.1.14.3.yYBVG6&amp;id=538286972599&amp;cm_id=140105335569ed55e27b&amp;abbucket=15&amp;sku_properties=10004:709990523;5919063:6536025"</span>)</span><br><span class="line">t_selector = Selector(text=browser.page_source)</span><br><span class="line"><span class="keyword">print</span> (t_selector.css(<span class="string">".tm-price::text"</span>).extract())</span><br><span class="line"><span class="comment"># print (browser.page_source)</span></span><br><span class="line">browser.quit()</span><br></pre></td></tr></table></figure><h4 id="知乎模拟登录"><a href="#知乎模拟登录" class="headerlink" title="知乎模拟登录"></a>知乎模拟登录</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome(executable_path=<span class="string">"C:/chromedriver.exe"</span>)</span><br><span class="line"><span class="comment">#知乎模拟登陆</span></span><br><span class="line">browser.get(<span class="string">"https://www.zhihu.com/#signin"</span>)</span><br><span class="line"></span><br><span class="line">browser.find_element_by_css_selector(<span class="string">".view-signin input[name='account']"</span>).send_keys(<span class="string">"phone"</span>)</span><br><span class="line">browser.find_element_by_css_selector(<span class="string">".view-signin input[name='password']"</span>).send_keys(<span class="string">"mima"</span>)</span><br><span class="line"></span><br><span class="line">browser.find_element_by_css_selector(<span class="string">".view-signin button.sign-button"</span>).click()</span><br></pre></td></tr></table></figure><h4 id="微博模拟登录"><a href="#微博模拟登录" class="headerlink" title="微博模拟登录"></a>微博模拟登录</h4><p>微博开放平台api</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome(executable_path=<span class="string">"C:/chromedriver.exe"</span>)</span><br><span class="line"><span class="comment">#selenium 完成微博模拟登录</span></span><br><span class="line">browser.get(<span class="string">"http://weibo.com/"</span>)</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line">browser.find_element_by_css_selector(<span class="string">"#loginname"</span>).send_keys(<span class="string">"1147727180@qq.com"</span>)</span><br><span class="line">browser.find_element_by_css_selector(<span class="string">".info_list.password input[node-type='password'] "</span>).send_keys(<span class="string">"mima"</span>)</span><br><span class="line">browser.find_element_by_xpath(<span class="string">'//*[@id="pl_login_form"]/div/div[3]/div[6]/a'</span>).click()</span><br></pre></td></tr></table></figure><h4 id="模拟JavaScript鼠标下滑"><a href="#模拟JavaScript鼠标下滑" class="headerlink" title="模拟JavaScript鼠标下滑"></a>模拟JavaScript鼠标下滑</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome(executable_path=<span class="string">"C:/chromedriver.exe"</span>)</span><br><span class="line"><span class="comment">#开源中国博客</span></span><br><span class="line">browser.get(<span class="string">"https://www.oschina.net/blog"</span>)</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    browser.execute_script(<span class="string">"window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;"</span>)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h4 id="页面不加载图片"><a href="#页面不加载图片" class="headerlink" title="页面不加载图片"></a>页面不加载图片</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置chromedriver不加载图片</span></span><br><span class="line">chrome_opt = webdriver.ChromeOptions()</span><br><span class="line">prefs = &#123;<span class="string">"profile.managed_default_content_settings.images"</span>:<span class="number">2</span>&#125;</span><br><span class="line">chrome_opt.add_experimental_option(<span class="string">"prefs"</span>, prefs)</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome(executable_path=<span class="string">"C:/chromedriver.exe"</span>,chrome_options=chrome_opt)</span><br><span class="line">browser.get(<span class="string">"https://www.oschina.net/blog"</span>)</span><br></pre></td></tr></table></figure><h4 id="phantomjs无界面的浏览器获取天猫价格"><a href="#phantomjs无界面的浏览器获取天猫价格" class="headerlink" title="phantomjs无界面的浏览器获取天猫价格"></a>phantomjs无界面的浏览器获取天猫价格</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#phantomjs, 无界面的浏览器， 多进程情况下phantomjs性能会下降很严重</span></span><br><span class="line"></span><br><span class="line">browser = webdriver.PhantomJS(executable_path=<span class="string">"C:/phantomjs-2.1.1-windows/bin/phantomjs.exe"</span>)</span><br><span class="line">browser.get(<span class="string">"https://detail.tmall.com/item.htm?spm=a230r.1.14.3.yYBVG6&amp;id=538286972599&amp;cm_id=140105335569ed55e27b&amp;abbucket=15&amp;sku_properties=10004:709990523;5919063:6536025"</span>)</span><br><span class="line">t_selector = Selector(text=browser.page_source)</span><br><span class="line"><span class="keyword">print</span> (t_selector.css(<span class="string">".tm-price::text"</span>).extract())</span><br><span class="line"><span class="keyword">print</span> (browser.page_source)</span><br><span class="line"><span class="comment"># browser.quit()</span></span><br></pre></td></tr></table></figure><h3 id="2-selenium集成进scrapy"><a href="#2-selenium集成进scrapy" class="headerlink" title="2.selenium集成进scrapy"></a>2.selenium集成进scrapy</h3><p><strong>如何集成</strong></p><h4 id="创建中间件。"><a href="#创建中间件。" class="headerlink" title="创建中间件。"></a>创建中间件。</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JSPageMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#通过chrome请求动态网页</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> spider.name == <span class="string">"jobbole"</span>:</span><br><span class="line">            browser = webdriver.Chrome(executable_path=<span class="string">"C:/chromedriver.exe"</span>)</span><br><span class="line">            spider.browser.get(request.url)</span><br><span class="line">            <span class="keyword">import</span> time</span><br><span class="line">            time.sleep(<span class="number">3</span>)</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"访问:&#123;0&#125;"</span>.format(request.url))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding=<span class="string">"utf-8"</span>, request=request)</span><br></pre></td></tr></table></figure><p><strong>使用selenium集成到具体spider中</strong></p><h4 id="信号量："><a href="#信号量：" class="headerlink" title="信号量："></a>信号量：</h4><p>dispatcher.connect 信号的映射，当spider结束该做什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.xlib.pydispatch <span class="keyword">import</span> dispatcher</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line">    <span class="comment">#使用selenium</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.browser = webdriver.Chrome(executable_path=<span class="string">"D:/Temp/chromedriver.exe"</span>)</span><br><span class="line">        super(JobboleSpider, self).__init__()</span><br><span class="line">        dispatcher.connect(self.spider_closed, signals.spider_closed)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="comment">#当爬虫退出的时候关闭chrome</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"spider closed"</span>)</span><br><span class="line">        self.browser.quit()</span><br></pre></td></tr></table></figure><h4 id="python下无界面浏览器"><a href="#python下无界面浏览器" class="headerlink" title="python下无界面浏览器"></a>python下无界面浏览器</h4><p><code>pip install pyvirtualdisplay</code></p><p>linux使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyvirtualdisplay <span class="keyword">import</span> Display</span><br><span class="line">display = Display(visible=<span class="number">0</span>, size=(<span class="number">800</span>, <span class="number">600</span>))</span><br><span class="line">display.start()</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get()</span><br></pre></td></tr></table></figure><p>错误：cmd=[‘xvfb’,’help’]<br>os error</p><p><code>sudo apt-get install xvfb</code></p><p><code>pip install xvfbwrapper</code></p><p>scrapy-splash:<br>支持分布式，稳定性不如chorme</p><blockquote><p><a href="https://github.com/scrapy-plugins/scrapy-splash" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-splash</a></p></blockquote><p>selenium grid<br>支持分布式</p><p>splinter<br><a href="https://github.com/cobrateam/splinter" target="_blank" rel="noopener">https://github.com/cobrateam/splinter</a></p><h4 id="scrapy的暂停重启"><a href="#scrapy的暂停重启" class="headerlink" title="scrapy的暂停重启"></a>scrapy的暂停重启</h4><p><code>scrapy crawl lagou -s JOBDIR=job_info/001</code></p><p>pycharm进程直接杀死 kiil -9</p><p>一次 ctrl+c可接受信号</p><p>Lib\site-packages\scrapy\dupefilters.py</p><p>先hash将url变成定长的字符串<br>然后使用集合set去重</p><p><strong>telnet</strong><br>远程登录</p><p><code>telnet localhost 6023</code> 连接当前spider<br><code>est()</code>命令查看spider当前状态</p><p><code>spider.settings[&quot;COOKIES_ENABLED&quot;]</code></p><p>Lib\site-packages\scrapy\extensions\telnet.py</p><p><strong>数据收集 &amp; 状态收集</strong><br>Scrapy提供了方便的收集数据的机制。数据以key/value方式存储，值大多是计数值。 该机制叫做数据收集器(Stats Collector)，可以通过 Crawler API 的属性 stats 来使用。在下面的章节 常见数据收集器使用方法 将给出例子来说明。</p><p>无论数据收集(stats collection)开启或者关闭，数据收集器永远都是可用的。 因此您可以import进自己的模块并使用其API(增加值或者设置新的状态键(stat keys))。 该做法是为了简化数据收集的方法: 您不应该使用超过一行代码来收集您的spider，Scrpay扩展或任何您使用数据收集器代码里头的状态。</p><p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/stats.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/stats.html</a></p><p>状态收集，数据收集器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 收集伯乐在线所有404的url以及404页面数</span></span><br><span class="line">handle_httpstatus_list = [<span class="number">404</span>]</span><br></pre></td></tr></table></figure><h2 id="七、scrapy-redis-分布式爬虫"><a href="#七、scrapy-redis-分布式爬虫" class="headerlink" title="七、scrapy-redis 分布式爬虫"></a>七、scrapy-redis 分布式爬虫</h2><h3 id="1-分布式爬虫设计及redis介绍"><a href="#1-分布式爬虫设计及redis介绍" class="headerlink" title="1. 分布式爬虫设计及redis介绍"></a>1. 分布式爬虫设计及redis介绍</h3><p>多个爬虫如何进行调度，一个集中的状态管理器</p><p>优点：</p><ul><li>利用多机器带宽</li><li>利用多ip加速爬取速度</li></ul><p>两个问题：</p><ol><li>request队列的集中管理</li><li>去重集中管理</li></ol><p>分布式。</p><h3 id="2-redis命令"><a href="#2-redis命令" class="headerlink" title="2. redis命令"></a>2. redis命令</h3><p>hexists course_dict mtianyan<br>hexists course_dict mtianyan2</p><blockquote><p>Redis HEXISTS命令被用来检查哈希字段是否存在。<br>返回值<br>回复整数，1或0。</p><ul><li>1, 如果哈希包含字段。</li><li>0 如果哈希不包含字段，或key不存在。</li></ul></blockquote><p>hdel course_dict mtianyan</p><blockquote><p>Redis HDEL命令用于从存储在键散列删除指定的字段。如果没有这个哈希中存在指定的字段将被忽略。如果键不存在，它将被视为一个空的哈希与此命令将返回0。<br>返回值回复整数，从散列中删除的字段的数量，不包括指定的但不是现有字段。</p></blockquote><p>hgetall course_dict</p><blockquote><p>Redis Hgetall 命令用于返回哈希表中，所有的字段和值。<br>在返回值里，紧跟每个字段名(field name)之后是字段的值(value)，所以返回值的长度是哈希表大小的两倍。</p></blockquote><p>hset course_dict bobby “python scrapy”</p><blockquote><p>Redis Hset 命令用于为哈希表中的字段赋值 。<br>如果哈希表不存在，一个新的哈希表被创建并进行 HSET 操作。<br>如果字段已经存在于哈希表中，旧值将被覆盖。</p></blockquote><p>hkey course_dict</p><blockquote><p>Redis Keys 命令用于查找所有符合给定模式 pattern 的 key 。。</p></blockquote><p>hvals course_dict</p><blockquote><p>Redis Hvals 命令返回哈希表所有字段的值。</p></blockquote><p>lpush mtianyan “scary”<br>rpush mtianyan “scary”</p><blockquote><p>存入key-value</p></blockquote><p>lrange mtianyan 0 10</p><blockquote><p>取出mtianyan的0到10</p></blockquote><p><img src="http://upload-images.jianshu.io/upload_images/1779926-82031cfa2b9af70c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis命令.png"></p><table><thead><tr><th>命令</th><th style="text-align:right">说明</th></tr></thead><tbody><tr><td>lpop/rpop</td><td style="text-align:right">左删除/右删除</td></tr><tr><td>llen mtianyan</td><td style="text-align:right">长度</td></tr><tr><td>lindex mtianyan 3</td><td style="text-align:right">第几个元素</td></tr><tr><td>sadd</td><td style="text-align:right">集合做减法</td></tr><tr><td>siner</td><td style="text-align:right">交集</td></tr><tr><td>spop</td><td style="text-align:right">随机删除</td></tr><tr><td>srandmember</td><td style="text-align:right">随机选择多个元素</td></tr><tr><td>smembers</td><td style="text-align:right">获取set所有元素</td></tr><tr><td>srandmember</td><td style="text-align:right">随机选择多个元素</td></tr><tr><td>zadd</td><td style="text-align:right">每个数有分数</td></tr><tr><td>zcount key 0 100</td><td style="text-align:right">0-100分数据量统计</td></tr></tbody></table><h3 id="3-scrapy-redis搭建分布式爬虫"><a href="#3-scrapy-redis搭建分布式爬虫" class="headerlink" title="3. scrapy-redis搭建分布式爬虫"></a>3. scrapy-redis搭建分布式爬虫</h3><p>需要的环境：</p><blockquote><p>Python 2.7, 3.4 or 3.5<br>Redis &gt;= 2.8<br>Scrapy &gt;= 1.1<br>redis-py &gt;= 2.10</p></blockquote><p><code>pip install redis</code></p><p><strong>setting.py设置</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SCHEDULER = <span class="string">"scrapy_redis.scheduler.Scheduler"</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'scrapy_redis.pipelines.RedisPipeline'</span>: <span class="number">300</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>要继承redisspider</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy_redis.spiders <span class="keyword">import</span> RedisSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(RedisSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'myspider'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># do stuff</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p><strong>启动spider</strong></p><p><code>scrapy runspider myspider.py</code></p><h4 id="push-urls-to-redis-放置初始url进入队列"><a href="#push-urls-to-redis-放置初始url进入队列" class="headerlink" title="push urls to redis:放置初始url进入队列"></a>push urls to redis:放置初始url进入队列</h4><p><code>redis-cli lpush myspider:start_urls http://google.com</code></p><p><strong>搭建示例</strong></p><ol><li>创建新的scrapy项目</li><li>去github拷贝scrapy-redis源码</li></ol><p>不同spider使用不同redis list<br>将队列从内存放入redis中<br>next_requests</p><p>所有的yield出去的request会被<br>ScrapyRedisTest\scrapy_redis\scheduler.py<br>的以及重写的enqueue_request接收</p><h2 id="八、elasticsearch搭建搜索引擎"><a href="#八、elasticsearch搭建搜索引擎" class="headerlink" title="八、elasticsearch搭建搜索引擎"></a>八、elasticsearch搭建搜索引擎</h2><blockquote><p>elasticsearch介绍：一个基于lucene的搜索服务器，分布式多用户的全文搜索引擎 java开发的 基于restful web接口<br>自己搭建的网站或者程序，添加搜索功能比较困难<br>所以我们希望搜索解决方案要高效<br>零配置并且免费<br>能够简单的通过json和http与搜索引擎交互<br>希望搜索服务很稳定<br>简单的将一台服务器扩展到多台服务器</p></blockquote><p><strong>内部功能：</strong><br>分词 搜索结果打分 解析搜索要求<br>全文搜索引擎：solr sphinx<br>很多大公司都用elasticsearch 戴尔 Facebook 微软等等</p><ol><li><p>elasticsearch对Lucene进行了封装，既能存储数据，又能分析数据，适合与做搜索引擎<br>关系数据搜索缺点：<br> 无法对搜素结果进行打分排序<br> 没有分布式，搜索麻烦，对程序员的要求比较高<br> 无法解析搜索请求，对搜索的内容无法进行解析，如分词等<br> 数据多了，效率低<br> 需要分词，把关系，数据，重点分出来</p></li><li><p>nosql数据库：<br> 文档数据库 json代码，在关系数据库中数据存储，需要存到多个表，内部有多对多等关系之类的，需要涉及到多个表才能将json里面的内容存下来，nosql直接将一个json的内容存起来，作为一个文档存档到数据库。<br>mongodb：</p></li></ol><h3 id="1-elasticsearch安装与配置"><a href="#1-elasticsearch安装与配置" class="headerlink" title="1. elasticsearch安装与配置"></a>1. elasticsearch安装与配置</h3><blockquote><ol><li>java sdk安装</li><li>elasticsearch安装官网下载 不使用官网的版本，提供原始的插件不多</li><li>elasticsearc-rtf github搜索，中文发行版，已经安装了很多插件 <a href="https://github.com/medcl/elasticsearch-rtf" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-rtf</a></li><li>运行elasticsearch的方法，在bin文件目录下进入命令行，执行elasticsearch.bat<br>5.配置文件：elasticsearch-rtf\elasticsearch-rtf-master\config\elasticsearch.yml</li></ol></blockquote><p><img src="http://upload-images.jianshu.io/upload_images/1779926-84340c76287bd524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="查看elasticsearch安装情况.png"></p><h3 id="2-elasticsearch两个重要插件：head和kibana的安装"><a href="#2-elasticsearch两个重要插件：head和kibana的安装" class="headerlink" title="2. elasticsearch两个重要插件：head和kibana的安装"></a>2. elasticsearch两个重要插件：head和kibana的安装</h3><p>head插件相当于Navicat，用于管理数据库，基于浏览器</p><p><a href="https://github.com/mobz/elasticsearch-head" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Running with built in server</span><br><span class="line"></span><br><span class="line">git clone git://github.com/mobz/elasticsearch-head.git</span><br><span class="line">cd elasticsearch-head</span><br><span class="line">npm install</span><br><span class="line">npm run start</span><br><span class="line">open http://localhost:9100/</span><br></pre></td></tr></table></figure><h4 id="配置elasticsearch与heade互通"><a href="#配置elasticsearch与heade互通" class="headerlink" title="配置elasticsearch与heade互通"></a>配置elasticsearch与heade互通</h4><p><img src="http://upload-images.jianshu.io/upload_images/1779926-45b0ab9d1030c91c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="head安装完成"></p><h4 id="kibana-bat"><a href="#kibana-bat" class="headerlink" title="kibana.bat"></a>kibana.bat</h4><p><img src="http://upload-images.jianshu.io/upload_images/1779926-f7ba853e61e8b637.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="kibana.png"></p><h3 id="2-elasticsearch基础概念"><a href="#2-elasticsearch基础概念" class="headerlink" title="2. elasticsearch基础概念"></a>2. elasticsearch基础概念</h3><ol><li>集群：一个或多个节点组织在一起</li><li>节点：一个集群中的一台服务器</li><li>分片：索引划分为多份的能力，允许水平分割，扩展容量，多个分片响应请求</li><li>副本：分片的一份或多分，一个节点失败，其他节点顶上</li></ol><p>|index | 数据库|<br>|type | 表|<br>|document | 行|<br>|fields | 列|</p><p>集合搜索和保存：增加了五种方法：<br>OPTIONS &amp; PUT &amp; DELETE &amp; TRACE &amp; CONNECT</p><h3 id="3-倒排索引："><a href="#3-倒排索引：" class="headerlink" title="3. 倒排索引："></a>3. 倒排索引：</h3><p><img src="http://upload-images.jianshu.io/upload_images/1779926-a2d90a6f90511675.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="倒排索引"></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-e61a277016449a3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="倒排索引"></p><h4 id="倒排索引待解决的问题："><a href="#倒排索引待解决的问题：" class="headerlink" title="倒排索引待解决的问题："></a>倒排索引待解决的问题：</h4><p><img src="http://upload-images.jianshu.io/upload_images/1779926-e61a277016449a3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="倒排索引"></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-b39e996d99c70d2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="创建索引"></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-1604a12ed995ea8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="head查看索引.png"></p><h3 id="4-elasticsearch命令"><a href="#4-elasticsearch命令" class="headerlink" title="4. elasticsearch命令"></a>4. elasticsearch命令</h3><blockquote><p>PUT lagou/job/1<br>1为id</p><p>PUT lagou/job/<br>不指明id自动生成uuid。</p><p>修改部分字段<br><code>POST lagou/job/1/_update</code></p><p>DELETE lagou/job/1</p></blockquote><p>elasticserach批量操作:</p><p><code>查询index为testdb下的job1表的id为1和job2表的id为2的数据</code><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">GET _mget</span><br><span class="line">&#123;</span><br><span class="line">    &quot;docs&quot;:[</span><br><span class="line">    &#123;</span><br><span class="line">    &quot;_index&quot;:&quot;testdb&quot;,</span><br><span class="line">    &quot;_type&quot;:&quot;job1&quot;,</span><br><span class="line">    &quot;_id&quot;:1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">    &quot;_index&quot;:&quot;testdb&quot;,</span><br><span class="line">    &quot;_type&quot;:&quot;job2&quot;,</span><br><span class="line">    &quot;_id&quot;:2</span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p><code>index已经指定了，所有在doc中就不用指定了</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">GET testdb/_mget&#123;</span><br><span class="line">    &quot;docs&quot;:[</span><br><span class="line">    &#123;</span><br><span class="line">    &quot;_type&quot;:&quot;job1&quot;,</span><br><span class="line">    &quot;_id&quot;:1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">    &quot;_type&quot;:&quot;job2&quot;,</span><br><span class="line">    &quot;_id&quot;:2</span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>连type都一样，只是id不一样</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">GET testdb/job1/_megt</span><br><span class="line">&#123;</span><br><span class="line">    &quot;docs&quot;:[</span><br><span class="line">    &#123;</span><br><span class="line">    &quot;_id&quot;:1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">    &quot;_id&quot;:2</span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>或者继续简写</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GET testdb/job1/_megt</span><br><span class="line">&#123;</span><br><span class="line">    &quot;ids&quot;:[1,2]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>elasticsearch的bulk批量操作：可以合并多个操作，比如index，delete，update，create等等，包括从一个索引到另一个索引：</p><ul><li>action_and_meta_data\n</li><li>option_source\n</li><li>action_and_meta_data\n</li><li>option_source\n</li><li>….</li><li>action_and_meta_data\n</li><li>option_source\n</li></ul><p>每个操作都是由两行构成，除了delete除外，由元信息行和数据行组成<br>注意数据不能美化，即只能是两行的形式，而不能是经过解析的标准的json排列形式，否则会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">POST _bulk</span><br><span class="line">&#123;&quot;index&quot;:...&#125;</span><br><span class="line">&#123;&quot;field&quot;:...&#125;</span><br></pre></td></tr></table></figure><h4 id="elasticserach的mapping映射"><a href="#elasticserach的mapping映射" class="headerlink" title="elasticserach的mapping映射"></a>elasticserach的mapping映射</h4><blockquote><p>elasticserach的mapping映射：创建索引时，可以预先定义字段的类型以及相关属性，每个字段定义一种类型，属性比mysql里面丰富，前面没有传入，因为elasticsearch会根据json源数据来猜测是什么基础类型。M挨批评就是我们自己定义的字段的数据类型，同时告诉elasticsearch如何索引数据以及是否可以被搜索。<br>作用：会让索引建立的更加细致和完善，对于大多数是不需要我们自己定义</p></blockquote><p>相关属性的配置</p><ul><li>String类型： 两种text keyword。text会对内部的内容进行分析，索引，进行倒排索引等，为设置为keyword则会当成字符串，不会被分析，只能完全匹配才能找到String。 在es5已经被废弃了</li><li>日期类型：date 以及datetime等</li><li>数据类型:integer long double等等</li><li>bool类型</li><li>binary类型</li><li>复杂类型：object nested</li><li>geo类型：geo-point地理位置</li><li>专业类型：ip competition</li><li>object ：json里面内置的还有下层{}的对象</li><li>nested：数组形式的数据</li></ul><h4 id="elasticserach查询："><a href="#elasticserach查询：" class="headerlink" title="elasticserach查询："></a>elasticserach查询：</h4><p>大概分为三类：</p><ul><li>基本查询:</li><li>组合查询：</li><li>过滤：查询同时，通过filter条件在不影响打分的情况下筛选数据</li></ul><p><strong>match查询:</strong></p><blockquote><p>后面为关键词，关于python的都会提取出来，match查询会对内容进行分词，并且会自动对传入的关键词进行大小写转换，内置ik分词器会进行切分，如python网站，只要搜到存在的任何一部分，都会返回<br><code>GET lagou/job/_search</code></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;:&#123;</span><br><span class="line">        &quot;match&quot;:&#123;</span><br><span class="line">            &quot;title&quot;:&quot;python&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>term查询</strong></p><blockquote><p>区别，对传入的值不会做任何处理，就像keyword，只能查包含整个传入的内容的，一部分也不行，只能完全匹配</p></blockquote><p><strong>terms查询</strong></p><blockquote><p>title里传入多个值，只要有一个匹配，就会返回结果</p></blockquote><p><strong>控制查询的返回数量</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">GET lagou/_serach</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;:&#123;</span><br><span class="line">        &quot;match&quot;:&#123;</span><br><span class="line">            &quot;title&quot;:&quot;python&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;form&quot;:1,</span><br><span class="line">    &quot;size&quot;:2</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过这里就可以完成分页处理洛，从第一条开始查询两条</p><p><strong>match_all 返回所有</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">GET lagou/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;:&#123;</span><br><span class="line">        &quot;match_all&quot;:&#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">**match_phrase查询 短语查询**</span><br><span class="line"></span><br><span class="line">GET lagou/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;:&#123;</span><br><span class="line">        &quot;match_phrase&quot;:&#123;</span><br><span class="line">            &quot;title&quot;:&#123;</span><br><span class="line">                &quot;query&quot;:&quot;python系统&quot;,</span><br><span class="line">                &quot;slop&quot;:6</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>python系统，将其分词，分为词条，满足词条里面的所有词才会返回结果，slop参数说明两个词条之间的最小距离</p><p><strong>multi_match查询</strong></p><blockquote><p>比如可以指定多个字段，比如查询title和desc这两个字段包含python的关键词文档</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">GET lagou/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;:&#123;</span><br><span class="line">        &quot;multi_match&quot;:&#123;</span><br><span class="line">            &quot;query&quot;:&quot;python&quot;,</span><br><span class="line">            &quot;fileds&quot;:[&quot;title^3&quot;,&quot;desc&quot;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>query为要查询的关键词 fileds在哪些字段里查询关键词，只要其中某个字段中出现了都返回<br>^3的意思为设置权重，在title中找到的权值为在desc字段中找到的权值的三倍</p></blockquote><p><strong>指定返回字段</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET lagou/_search&#123;</span><br><span class="line">    &quot;stored_fields&quot;:[&quot;title&quot;,&quot;company_name&quot;],</span><br><span class="line">    &quot;query&quot;:&#123;</span><br><span class="line">        &quot;match&quot;:&#123;</span><br><span class="line">            &quot;title&quot;:&quot;pyhton&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>通过sort把结果排序</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">GET lagou/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;;&#123;</span><br><span class="line">        &quot;match_all&quot;:&#123;&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;sort&quot;:[&#123;</span><br><span class="line">        &quot;comments&quot;:&#123;</span><br><span class="line">            &quot;order&quot;:&quot;desc&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>sort是一个数组，里面是一个字典，key就是要sort的字段，asc desc是升序降序的意思</p><p><strong>查询范围 range查询</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">GET lagou/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;;&#123;</span><br><span class="line">        &quot;range&quot;:&#123;</span><br><span class="line">            &quot;comments&quot;:&#123;</span><br><span class="line">                &quot;gte&quot;:10,</span><br><span class="line">                &quot;lte&quot;:20,</span><br><span class="line">                &quot;boost&quot;:2.0</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>range是在query里面的，boost是权重，gte lte是大于等于 小于等于的意思<br>对时间的范围查询，则是以字符串的形式传入</p></blockquote><p><strong>wildcard模糊查询，可以使用通配符</strong><br><code>*</code></p><p><strong>组合查询：bool查询</strong></p><p>bool查询包括了must should must_not filter来完成<br>格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bool:&#123;</span><br><span class="line">    &quot;filter&quot;:[],</span><br><span class="line">    &quot;must&quot;:[],</span><br><span class="line">    &quot;should&quot;:[],</span><br><span class="line">    &quot;must_not&quot;:[],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-把爬取的数据保存至elasticsearch"><a href="#5-把爬取的数据保存至elasticsearch" class="headerlink" title="5. 把爬取的数据保存至elasticsearch"></a>5. 把爬取的数据保存至elasticsearch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ElasticsearchPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#将数据写入到es中</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment">#将item转换为es的数据</span></span><br><span class="line">        item.save_to_es()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h4 id="elasticsearch-dsl-py"><a href="#elasticsearch-dsl-py" class="headerlink" title="elasticsearch-dsl-py"></a>elasticsearch-dsl-py</h4><blockquote><p>High level Python client for Elasticsearch</p></blockquote><p><code>pip install elasticsearch-dsl</code></p><h4 id="items-py-中将数据保存至es"><a href="#items-py-中将数据保存至es" class="headerlink" title="items.py 中将数据保存至es"></a>items.py 中将数据保存至es</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_es</span><span class="params">(self)</span>:</span></span><br><span class="line">       article = ArticleType()</span><br><span class="line">       article.title = self[<span class="string">'title'</span>]</span><br><span class="line">       article.create_date = self[<span class="string">"create_date"</span>]</span><br><span class="line">       article.content = remove_tags(self[<span class="string">"content"</span>])</span><br><span class="line">       article.front_image_url = self[<span class="string">"front_image_url"</span>]</span><br><span class="line">       <span class="keyword">if</span> <span class="string">"front_image_path"</span> <span class="keyword">in</span> self:</span><br><span class="line">           article.front_image_path = self[<span class="string">"front_image_path"</span>]</span><br><span class="line">       article.praise_nums = self[<span class="string">"praise_nums"</span>]</span><br><span class="line">       article.fav_nums = self[<span class="string">"fav_nums"</span>]</span><br><span class="line">       article.comment_nums = self[<span class="string">"comment_nums"</span>]</span><br><span class="line">       article.url = self[<span class="string">"url"</span>]</span><br><span class="line">       article.tags = self[<span class="string">"tags"</span>]</span><br><span class="line">       article.meta.id = self[<span class="string">"url_object_id"</span>]</span><br><span class="line"></span><br><span class="line">       article.suggest = gen_suggests(ArticleType._doc_type.index, ((article.title,<span class="number">10</span>),(article.tags, <span class="number">7</span>)))</span><br><span class="line"></span><br><span class="line">       article.save()</span><br><span class="line"></span><br><span class="line">       redis_cli.incr(<span class="string">"jobbole_count"</span>)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span></span><br></pre></td></tr></table></figure><h3 id="6-elasticsearch结合django搭建搜索引擎"><a href="#6-elasticsearch结合django搭建搜索引擎" class="headerlink" title="6. elasticsearch结合django搭建搜索引擎"></a>6. elasticsearch结合django搭建搜索引擎</h3><p>获取elasticsearch的查询接口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">body=&#123;</span><br><span class="line">                   <span class="string">"query"</span>:&#123;</span><br><span class="line">                       <span class="string">"multi_match"</span>:&#123;</span><br><span class="line">                           <span class="string">"query"</span>:key_words,</span><br><span class="line">                           <span class="string">"fields"</span>:[<span class="string">"tags"</span>, <span class="string">"title"</span>, <span class="string">"content"</span>]</span><br><span class="line">                       &#125;</span><br><span class="line">                   &#125;,</span><br><span class="line">                   <span class="string">"from"</span>:(page<span class="number">-1</span>)*<span class="number">10</span>,</span><br><span class="line">                   <span class="string">"size"</span>:<span class="number">10</span>,</span><br><span class="line">                   <span class="string">"highlight"</span>: &#123;</span><br><span class="line">                       <span class="string">"pre_tags"</span>: [<span class="string">'&lt;span class="keyWord"&gt;'</span>],</span><br><span class="line">                       <span class="string">"post_tags"</span>: [<span class="string">'&lt;/span&gt;'</span>],</span><br><span class="line">                       <span class="string">"fields"</span>: &#123;</span><br><span class="line">                           <span class="string">"title"</span>: &#123;&#125;,</span><br><span class="line">                           <span class="string">"content"</span>: &#123;&#125;,</span><br><span class="line">                       &#125;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br></pre></td></tr></table></figure><p>使django与其交互。</p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-1c550de187cbf9f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="搜索界面"></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-9608d37c848236d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="结果界面"></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/post/3c234171.html">Scrapy分布式爬虫打造搜索引擎- 系列完整版</a></p><p><span>文章作者:</span><a href="/" title="访问 mtianyan 的个人博客">mtianyan</a></p><p><span>发布时间:</span>2018年01月01日 - 20:01</p><p><span>最后更新:</span>2018年02月02日 - 20:02</p><p><span>原始链接:</span><a href="/post/3c234171.html" title="Scrapy分布式爬虫打造搜索引擎- 系列完整版">http://blog.mtianyan.cn/post/3c234171.html</a><span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://blog.mtianyan.cn/post/3c234171.html" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"",text:"复制成功",icon:"success",showConfirmButton:!0})})})</script></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>请博主吃包辣条</div> <button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'> <span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"> <img id="wechat_qr" src="/images/wechatpay.png" alt="mtianyan 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"> <img id="alipay_qr" src="/images/alipay.jpg" alt="mtianyan 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/爬虫/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a><a href="/tags/搜索引擎/" rel="tag"><i class="fa fa-tag"></i> 搜索引擎</a><a href="/tags/Scrapy/" rel="tag"><i class="fa fa-tag"></i> Scrapy</a><a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/post/723b0e71.html" rel="next" title="Scrapy分布式爬虫打造搜索引擎- (八)elasticsearch结合django搭建搜索引擎"><i class="fa fa-chevron-left"></i> Scrapy分布式爬虫打造搜索引擎- (八)elasticsearch结合django搭建搜索引擎</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/post/9237fc73.html" rel="prev" title="程序员装机必备爆款软件推荐与配置(windows版)">程序员装机必备爆款软件推荐与配置(windows版)<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div class="jiathis_style"> <span class="jiathis_txt">分享到：</span> <a class="jiathis_button_fav">收藏夹</a> <a class="jiathis_button_copy">复制网址</a> <a class="jiathis_button_email">邮件</a> <a class="jiathis_button_weixin">微信</a> <a class="jiathis_button_qzone">QQ空间</a> <a class="jiathis_button_tqq">腾讯微博</a> <a class="jiathis_button_douban">豆瓣</a> <a class="jiathis_button_share">一键分享</a> <a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a><a class="jiathis_counter_style"></a></div><script type="text/javascript">var jiathis_config={data_track_clickback:!0,summary:"",shortUrl:!1,hideMore:!1}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=2154292" charset="utf-8"></script></div></div></div><div class="comments" id="comments"><div id="SOHUCS"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> 站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="mtianyan"><p class="site-author-name" itemprop="name">mtianyan</p><p class="site-description motion-element" itemprop="description">爱分享，爱技术，爱生活。</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">37</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">5</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">23</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/mtianyan" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://www.jianshu.com/u/db9a7a0daa1f" target="_blank" title="简书"><i class="fa fa-fw fa-book"></i> 简书</a></span><span class="links-of-author-item"><a href="https://plus.google.com/u/0/114963812195952881148" target="_blank" title="Google"><i class="fa fa-fw fa-google"></i> Google</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="http://mtianyan.gitee.io/" title="本站孪生站(国内码云托管)" target="_blank">本站孪生站(国内码云托管)</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、基础知识学习"><span class="nav-number">1.</span> <span class="nav-text">一、基础知识学习:</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-爬取策略的深度优先和广度优先"><span class="nav-number">1.1.</span> <span class="nav-text">1. 爬取策略的深度优先和广度优先</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-爬虫网址去重策略"><span class="nav-number">1.2.</span> <span class="nav-text">2. 爬虫网址去重策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Python字符串编码问题解决："><span class="nav-number">1.3.</span> <span class="nav-text">3. Python字符串编码问题解决：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、伯乐在线爬取所有文章"><span class="nav-number">2.</span> <span class="nav-text">二、伯乐在线爬取所有文章</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-初始化文件目录"><span class="nav-number">2.1.</span> <span class="nav-text">1. 初始化文件目录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#scrapy项目初始化介绍"><span class="nav-number">2.1.1.</span> <span class="nav-text">scrapy项目初始化介绍</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-提取伯乐在线内容"><span class="nav-number">2.2.</span> <span class="nav-text">2. 提取伯乐在线内容</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#xpath的使用"><span class="nav-number">2.2.1.</span> <span class="nav-text">xpath的使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#css选择器的使用："><span class="nav-number">2.2.2.</span> <span class="nav-text">css选择器的使用：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-爬取所有文章"><span class="nav-number">2.3.</span> <span class="nav-text">3. 爬取所有文章</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-scrapy的items整合字段"><span class="nav-number">2.4.</span> <span class="nav-text">4. scrapy的items整合字段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-数据保存到本地文件以及mysql中"><span class="nav-number">2.5.</span> <span class="nav-text">5. 数据保存到本地文件以及mysql中</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#保存到本地json文件"><span class="nav-number">2.5.1.</span> <span class="nav-text">保存到本地json文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#保存到数据库-mysql"><span class="nav-number">2.5.2.</span> <span class="nav-text">保存到数据库(mysql)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、知乎网问题和答案爬取"><span class="nav-number">3.</span> <span class="nav-text">三、知乎网问题和答案爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-基础知识"><span class="nav-number">3.1.</span> <span class="nav-text">1. 基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#session和cookie机制"><span class="nav-number">3.1.1.</span> <span class="nav-text">session和cookie机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#request模拟知乎的登录"><span class="nav-number">3.1.2.</span> <span class="nav-text">request模拟知乎的登录</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-scrapy创建知乎爬虫登录"><span class="nav-number">3.2.</span> <span class="nav-text">2. scrapy创建知乎爬虫登录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#将item写入数据库"><span class="nav-number">3.2.1.</span> <span class="nav-text">将item写入数据库</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、通过CrawlSpider对招聘网站拉钩网进行整站爬取"><span class="nav-number">4.</span> <span class="nav-text">四、通过CrawlSpider对招聘网站拉钩网进行整站爬取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-设计拉勾网的数据表结构"><span class="nav-number">4.1.</span> <span class="nav-text">1. 设计拉勾网的数据表结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-初始化拉钩网项目并解读crawl源码"><span class="nav-number">4.2.</span> <span class="nav-text">2. 初始化拉钩网项目并解读crawl源码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#编写rule规则"><span class="nav-number">4.2.1.</span> <span class="nav-text">编写rule规则</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-设计lagou的items"><span class="nav-number">4.3.</span> <span class="nav-text">3. 设计lagou的items</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-提取字段值并存入数据库"><span class="nav-number">4.3.1.</span> <span class="nav-text">4. 提取字段值并存入数据库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-items中添加get-insert-sql实现存入数据库"><span class="nav-number">4.3.2.</span> <span class="nav-text">5. items中添加get_insert_sql实现存入数据库</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、爬虫与反爬虫"><span class="nav-number">5.</span> <span class="nav-text">五、爬虫与反爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-基础知识-1"><span class="nav-number">5.1.</span> <span class="nav-text">1. 基础知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-scrapy架构及源码介绍"><span class="nav-number">5.2.</span> <span class="nav-text">2. scrapy架构及源码介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-scrapy的两个重要类：request和response"><span class="nav-number">5.3.</span> <span class="nav-text">3. scrapy的两个重要类：request和response</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-自行编写随机更换useagent"><span class="nav-number">5.4.</span> <span class="nav-text">4. 自行编写随机更换useagent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-middlewire配置及编写fake-UseAgent代理池"><span class="nav-number">5.5.</span> <span class="nav-text">5. middlewire配置及编写fake UseAgent代理池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-使用西刺代理创建ip代理池保存到数据库"><span class="nav-number">5.6.</span> <span class="nav-text">6. 使用西刺代理创建ip代理池保存到数据库*</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-通过云打码实现验证码的识别"><span class="nav-number">5.7.</span> <span class="nav-text">7. 通过云打码实现验证码的识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-cookie的禁用。-amp-设置下载速度"><span class="nav-number">5.8.</span> <span class="nav-text">8. cookie的禁用。&amp; 设置下载速度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六、scrapy进阶开发"><span class="nav-number">6.</span> <span class="nav-text">六、scrapy进阶开发</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Selenium动态页面抓取"><span class="nav-number">6.1.</span> <span class="nav-text">1. Selenium动态页面抓取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#天猫价格获取"><span class="nav-number">6.1.1.</span> <span class="nav-text">天猫价格获取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#知乎模拟登录"><span class="nav-number">6.1.2.</span> <span class="nav-text">知乎模拟登录</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#微博模拟登录"><span class="nav-number">6.1.3.</span> <span class="nav-text">微博模拟登录</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模拟JavaScript鼠标下滑"><span class="nav-number">6.1.4.</span> <span class="nav-text">模拟JavaScript鼠标下滑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#页面不加载图片"><span class="nav-number">6.1.5.</span> <span class="nav-text">页面不加载图片</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#phantomjs无界面的浏览器获取天猫价格"><span class="nav-number">6.1.6.</span> <span class="nav-text">phantomjs无界面的浏览器获取天猫价格</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-selenium集成进scrapy"><span class="nav-number">6.2.</span> <span class="nav-text">2.selenium集成进scrapy</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建中间件。"><span class="nav-number">6.2.1.</span> <span class="nav-text">创建中间件。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信号量："><span class="nav-number">6.2.2.</span> <span class="nav-text">信号量：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#python下无界面浏览器"><span class="nav-number">6.2.3.</span> <span class="nav-text">python下无界面浏览器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#scrapy的暂停重启"><span class="nav-number">6.2.4.</span> <span class="nav-text">scrapy的暂停重启</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#七、scrapy-redis-分布式爬虫"><span class="nav-number">7.</span> <span class="nav-text">七、scrapy-redis 分布式爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-分布式爬虫设计及redis介绍"><span class="nav-number">7.1.</span> <span class="nav-text">1. 分布式爬虫设计及redis介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-redis命令"><span class="nav-number">7.2.</span> <span class="nav-text">2. redis命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-scrapy-redis搭建分布式爬虫"><span class="nav-number">7.3.</span> <span class="nav-text">3. scrapy-redis搭建分布式爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#push-urls-to-redis-放置初始url进入队列"><span class="nav-number">7.3.1.</span> <span class="nav-text">push urls to redis:放置初始url进入队列</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#八、elasticsearch搭建搜索引擎"><span class="nav-number">8.</span> <span class="nav-text">八、elasticsearch搭建搜索引擎</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-elasticsearch安装与配置"><span class="nav-number">8.1.</span> <span class="nav-text">1. elasticsearch安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-elasticsearch两个重要插件：head和kibana的安装"><span class="nav-number">8.2.</span> <span class="nav-text">2. elasticsearch两个重要插件：head和kibana的安装</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#配置elasticsearch与heade互通"><span class="nav-number">8.2.1.</span> <span class="nav-text">配置elasticsearch与heade互通</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kibana-bat"><span class="nav-number">8.2.2.</span> <span class="nav-text">kibana.bat</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-elasticsearch基础概念"><span class="nav-number">8.3.</span> <span class="nav-text">2. elasticsearch基础概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-倒排索引："><span class="nav-number">8.4.</span> <span class="nav-text">3. 倒排索引：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#倒排索引待解决的问题："><span class="nav-number">8.4.1.</span> <span class="nav-text">倒排索引待解决的问题：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-elasticsearch命令"><span class="nav-number">8.5.</span> <span class="nav-text">4. elasticsearch命令</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#elasticserach的mapping映射"><span class="nav-number">8.5.1.</span> <span class="nav-text">elasticserach的mapping映射</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#elasticserach查询："><span class="nav-number">8.5.2.</span> <span class="nav-text">elasticserach查询：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-把爬取的数据保存至elasticsearch"><span class="nav-number">8.6.</span> <span class="nav-text">5. 把爬取的数据保存至elasticsearch</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#elasticsearch-dsl-py"><span class="nav-number">8.6.1.</span> <span class="nav-text">elasticsearch-dsl-py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#items-py-中将数据保存至es"><span class="nav-number">8.6.2.</span> <span class="nav-text">items.py 中将数据保存至es</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-elasticsearch结合django搭建搜索引擎"><span class="nav-number">8.7.</span> <span class="nav-text">6. elasticsearch结合django搭建搜索引擎</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">mtianyan</span><div class="theme-info"><div class="powered-by"></div> <span class="post-count">博客全站共182.8k字</span></div></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script> <span class="site-uv">本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人次</span> <span class="site-pv">本站总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><script type="text/javascript">!function(){var t="1eb79150519fd9b791c77e8eef6f3632";if((window.innerWidth||document.documentElement.clientWidth)<960)window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=cyrJmJ3rL&conf='+t+'"><\/script>');else{!function(t,e){var n=document.getElementsByTagName("head")[0]||document.head||document.documentElement,a=document.createElement("script");a.setAttribute("type","text/javascript"),a.setAttribute("charset","UTF-8"),a.setAttribute("src",t),"function"==typeof e&&(window.attachEvent?a.onreadystatechange=function(){var t=a.readyState;"loaded"!==t&&"complete"!==t||(a.onreadystatechange=null,e())}:a.onload=e),n.appendChild(a)}("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:"cyrJmJ3rL",conf:t})})}}()</script><script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script><script type="text/javascript">var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")};function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){var r=!1,s=0,a=0,i=n.title.trim(),c=i.toLowerCase(),l=n.content.trim().replace(/<[^>]+>/g,""),h=l.toLowerCase(),p=decodeURIComponent(n.url),u=[],f=[];if(""!=i&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}u=u.concat(e(t,c,!1)),f=f.concat(e(t,h,!1))}),(u.length>0||f.length>0)&&(r=!0,s=u.length+f.length)),r){[u,f].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});function d(e,o,n,r){for(var s=r[r.length-1],i=s.position,c=s.word,l=[],h=0;i+c.length<=n&&0!=r.length;){c===t&&h++,l.push({position:i,length:c.length});var p=i+c.length;for(r.pop();0!=r.length&&(i=(s=r[r.length-1]).position,c=s.word,p>i);)r.pop()}return a+=h,{hits:l,start:o,end:n,searchTextCount:h}}var g=[];0!=u.length&&g.push(d(0,0,i.length,u));for(var v=[];0!=f.length;){var $=f[f.length-1],C=$.position,m=$.word,x=C-20,w=C+80;x<0&&(x=0),w<C+m.length&&(w=C+m.length),w>l.length&&(w=l.length),v.push(d(0,x,w,f))}v.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var y=parseInt("1");y>=0&&(v=v.slice(0,y));function T(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var b="";0!=g.length?b+="<li><a href='"+p+"' class='search-result-title'>"+T(i,g[0])+"</a>":b+="<li><a href='"+p+"' class='search-result-title'>"+i+"</a>",v.forEach(function(t){b+="<a href='"+p+'\'><p class="search-result">'+T(l,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:a,hitCount:s,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),!1===isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){27===t.which&&$(".search-popup").is(":visible")&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("2uqmIjYredIvFy6tEXbKG4Fj-gzGzoHsz","CWl1rE8cQlIseOg2Cq3hzxYi")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var i=0;i<e.length;i++){var s=e[i],r=s.get("url"),l=s.get("time"),c=document.getElementById(r);$(c).find(t).text(l)}for(i=0;i<n.length;i++){r=n[i],c=document.getElementById(r);var u=$(c).find(t);""==u.text()&&u.text(0)}}else o.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var s=new e,r=new AV.ACL;r.setPublicReadAccess(!0),r.setPublicWriteAccess(!0),s.setACL(r),s.set("title",o),s.set("url",n),s.set("time",1),s.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={},pbOptions.iconStyle="default",pbOptions.boxForm="horizontal",pbOptions.position="bottomCenter",pbOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-postbottom",pbOptions),flOptions={},flOptions.iconStyle="default",flOptions.boxForm="horizontal",flOptions.position="middleRight",flOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-float",flOptions)</script><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><div id="hexo-helper-live2d"><canvas id="live2dcanvas" width="150" height="300"></canvas></div><style>#live2dcanvas{position:fixed;width:150px;height:300px;opacity:.7;right:-30px;z-index:999;pointer-events:none;bottom:40px}</style><script type="text/javascript" src="/live2d/device.min.js"></script><script type="text/javascript">
const loadScript = function loadScript(c,b){var a=document.createElement("script");a.type="text/javascript";"undefined"!=typeof b&&(a.readyState?a.onreadystatechange=function(){if("loaded"==a.readyState||"complete"==a.readyState)a.onreadystatechange=null,b()}:a.onload=function(){b()});a.src=c;document.body.appendChild(a)};
(function(){
  if((typeof(device) != 'undefined') && (device.mobile())){
    var trElement = document.getElementById('hexo-helper-live2d');
    trElement.parentNode.removeChild(trElement);
    return;
  }else
    if (typeof(device) === 'undefined') console.error('Cannot find current-device script.');
  loadScript("/live2d/script.js", function(){loadlive2d("live2dcanvas", "/live2d/assets/hijiki.model.json", 0.5);});
})();
</script></body></html><script type="text/javascript" src="/js/src/love.js"></script>